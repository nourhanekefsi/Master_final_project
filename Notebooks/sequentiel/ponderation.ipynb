{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functional Similarity (GO SLIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 16:10:38,112 - INFO - Loaded 6726 UniProt to SGD mappings\n",
      "2025-04-12 16:10:38,147 - INFO - \n",
      "Processing STRING_levure network\n",
      "2025-04-12 16:10:38,653 - INFO - Loaded PPI network with 96810 interactions\n",
      "2025-04-12 16:10:38,960 - INFO - Converting SGD IDs to UniProt IDs in GO annotations\n",
      "2025-04-12 16:10:38,989 - INFO - Loaded 48478 GO annotations\n",
      "2025-04-12 16:10:39,322 - INFO - Calculating functional similarity for 5707 unique proteins\n",
      "2025-04-12 16:10:39,405 - INFO - Computing cosine similarity...\n",
      "2025-04-12 16:11:00,038 - INFO - Saved weighted network to C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\FS_STRING_levure.txt with 86142 interactions\n",
      "2025-04-12 16:11:00,054 - INFO - \n",
      "Processing BIOGRID_humain network\n",
      "2025-04-12 16:11:00,473 - INFO - Loaded PPI network with 88647 interactions\n",
      "2025-04-12 16:11:03,417 - INFO - Loaded 764609 GO annotations\n",
      "2025-04-12 16:11:04,120 - INFO - Calculating functional similarity for 11266 unique proteins\n",
      "2025-04-12 16:11:04,421 - INFO - Computing cosine similarity...\n",
      "2025-04-12 16:11:25,039 - INFO - Saved weighted network to C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\FS_BIOGRID_humain.txt with 80778 interactions\n",
      "2025-04-12 16:11:25,077 - INFO - \n",
      "Processing STRING_humain network\n",
      "2025-04-12 16:11:27,414 - INFO - Loaded PPI network with 349046 interactions\n",
      "2025-04-12 16:11:30,362 - INFO - Loaded 764609 GO annotations\n",
      "2025-04-12 16:11:32,029 - INFO - Calculating functional similarity for 17723 unique proteins\n",
      "2025-04-12 16:11:32,491 - INFO - Computing cosine similarity...\n",
      "2025-04-12 16:13:11,029 - INFO - Saved weighted network to C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\FS_STRING_humain.txt with 305683 interactions\n",
      "2025-04-12 16:13:11,067 - INFO - \n",
      "Processing BIOGRID_levure network\n",
      "2025-04-12 16:13:12,535 - INFO - Loaded PPI network with 300000 interactions\n",
      "2025-04-12 16:13:13,003 - INFO - Converting SGD IDs to UniProt IDs in GO annotations\n",
      "2025-04-12 16:13:13,047 - INFO - Loaded 48478 GO annotations\n",
      "2025-04-12 16:13:14,550 - INFO - Calculating functional similarity for 5805 unique proteins\n",
      "2025-04-12 16:13:14,723 - INFO - Computing cosine similarity...\n",
      "2025-04-12 16:14:05,116 - INFO - Saved weighted network to C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\FS_BIOGRID_levure.txt with 215304 interactions\n",
      "2025-04-12 16:14:05,118 - INFO - \n",
      "Processing DIP_levure network\n",
      "2025-04-12 16:14:05,300 - INFO - Loaded PPI network with 22614 interactions\n",
      "2025-04-12 16:14:05,550 - INFO - Converting SGD IDs to UniProt IDs in GO annotations\n",
      "2025-04-12 16:14:05,616 - INFO - Loaded 48478 GO annotations\n",
      "2025-04-12 16:14:05,797 - INFO - Calculating functional similarity for 5144 unique proteins\n",
      "2025-04-12 16:14:05,883 - INFO - Computing cosine similarity...\n",
      "2025-04-12 16:14:10,959 - INFO - Saved weighted network to C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\FS_DIP_levure.txt with 16393 interactions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from typing import Dict, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Configuration des chemins\n",
    "BASE_DIR = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\")\n",
    "RAW_DATA_DIR = BASE_DIR / \"raw data\" / \"autres\"\n",
    "CLEAN_DATA_DIR = BASE_DIR / \"clean data\"\n",
    "INTERACTIONS_DIR = CLEAN_DATA_DIR / \"interactions\"\n",
    "OUTPUT_DIR = CLEAN_DATA_DIR / \"autres\"\n",
    "GO_SLIM_YEAST = RAW_DATA_DIR / \"go_slim_mapping.tab\"\n",
    "GO_SLIM_HUMAN = RAW_DATA_DIR / \"uniprotkb_Homo_sapiens_Human_AND_model_2025_04_03.tsv\"\n",
    "MAPPING_FILE = RAW_DATA_DIR / \"YEAST_559292_idmapping.dat\"\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_mapping(mapping_file: Path) -> Tuple[Dict[str, str], Dict[str, str]]:\n",
    "    \"\"\"Charge le mapping UniProt vers SGD et inversement\"\"\"\n",
    "    try:\n",
    "        dtype = {'UniProt': 'string', 'DB': 'string', 'ID': 'string'}\n",
    "        df = pd.read_csv(mapping_file, sep='\\t', header=None,\n",
    "                        names=['UniProt', 'DB', 'ID'], dtype=dtype)\n",
    "        \n",
    "        sgd_mapping = df[df['DB'] == 'SGD']\n",
    "        uniprot_to_sgd = dict(zip(sgd_mapping['UniProt'], sgd_mapping['ID']))\n",
    "        sgd_to_uniprot = dict(zip(sgd_mapping['ID'], sgd_mapping['UniProt']))\n",
    "        \n",
    "        logger.info(f\"Loaded {len(uniprot_to_sgd)} UniProt to SGD mappings\")\n",
    "        return uniprot_to_sgd, sgd_to_uniprot\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors du chargement du mapping: {e}\")\n",
    "        return {}, {}\n",
    "\n",
    "def load_go_slim(go_file: Path, is_human: bool = True, \n",
    "                sgd_to_uniprot: Optional[Dict[str, str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Charge les annotations GO et convertit les identifiants en UniProt si nécessaire\"\"\"\n",
    "    try:\n",
    "        if is_human:\n",
    "            # Chargement des données humaines (déjà en UniProt)\n",
    "            df = pd.read_csv(go_file, sep='\\t', usecols=['Entry', 'Gene Ontology (GO)'], \n",
    "                            dtype={'Entry': 'string'})\n",
    "            \n",
    "            # Extraction des termes GO\n",
    "            go_data = []\n",
    "            for entry, go_str in df.dropna(subset=['Gene Ontology (GO)']).itertuples(index=False):\n",
    "                terms = re.findall(r'GO:\\d+', go_str)\n",
    "                go_data.extend([(entry, term) for term in terms])\n",
    "            \n",
    "            df_go = pd.DataFrame(go_data, columns=['protein', 'go_term'])\n",
    "        else:\n",
    "            # Chargement des données de levure (SGD -> besoin de conversion)\n",
    "            df = pd.read_csv(go_file, sep='\\t', header=None, \n",
    "                           usecols=[2, 5], names=['protein', 'go_term'],\n",
    "                           dtype={'protein': 'string', 'go_term': 'string'})\n",
    "            \n",
    "            # Filtrage des termes GO valides\n",
    "            df_go = df[df['go_term'].str.contains(r'^GO:\\d+$', na=False)].copy()\n",
    "            \n",
    "            # Conversion SGD -> UniProt si le mapping est fourni\n",
    "            if sgd_to_uniprot:\n",
    "                logger.info(\"Converting SGD IDs to UniProt IDs in GO annotations\")\n",
    "                df_go['protein'] = df_go['protein'].map(sgd_to_uniprot)\n",
    "                df_go = df_go.dropna(subset=['protein'])  # Enlever les non mappés\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df_go)} GO annotations\")\n",
    "        return df_go.drop_duplicates().reset_index(drop=True)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur lors du chargement des annotations GO: {e}\")\n",
    "        return pd.DataFrame(columns=['protein', 'go_term'])\n",
    "\n",
    "def calculate_functional_similarity(proteins: list, go_annotations: pd.DataFrame) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"Calcule la similarité fonctionnelle entre les protéines\"\"\"\n",
    "    try:\n",
    "        proteins_set = sorted(set(proteins))\n",
    "        logger.info(f\"Calculating functional similarity for {len(proteins_set)} unique proteins\")\n",
    "        \n",
    "        # Filtrer les annotations pour les protéines d'intérêt\n",
    "        go_filtered = go_annotations[go_annotations['protein'].isin(proteins_set)].copy()\n",
    "        \n",
    "        if len(go_filtered) == 0:\n",
    "            logger.warning(\"No matching GO annotations found for the proteins\")\n",
    "            return None\n",
    "        \n",
    "        # Créer une matrice protéine-terme GO\n",
    "        protein_cat = pd.CategoricalDtype(categories=proteins_set, ordered=True)\n",
    "        go_filtered['protein'] = go_filtered['protein'].astype(protein_cat)\n",
    "        \n",
    "        # Construction de la matrice creuse\n",
    "        row_ind = go_filtered['protein'].cat.codes\n",
    "        col_ind = pd.Categorical(go_filtered['go_term']).codes\n",
    "        data = np.ones(len(go_filtered), dtype=np.int8)\n",
    "        \n",
    "        sparse_matrix = sparse.csr_matrix(\n",
    "            (data, (row_ind, col_ind)),\n",
    "            shape=(len(proteins_set), len(go_filtered['go_term'].unique()))\n",
    "        )\n",
    "        # Calcul de similarité cosinus\n",
    "        logger.info(\"Computing cosine similarity...\")\n",
    "        similarity_matrix = cosine_similarity(sparse_matrix)\n",
    "        \n",
    "        return pd.DataFrame(similarity_matrix, \n",
    "                          index=proteins_set, \n",
    "                          columns=proteins_set)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating functional similarity: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_ppi_network(ppi_file: Path) -> pd.DataFrame:\n",
    "    \"\"\"Charge un réseau PPI (supposé utiliser des UniProt IDs)\"\"\"\n",
    "    try:\n",
    "        ppi = pd.read_csv(ppi_file, sep='\\t', header=None,\n",
    "                         names=['protein1', 'protein2'], dtype='string')\n",
    "        \n",
    "        # Nettoyage des identifiants\n",
    "        ppi['protein1'] = ppi['protein1'].str.upper().str.strip()\n",
    "        ppi['protein2'] = ppi['protein2'].str.upper().str.strip()\n",
    "        \n",
    "        # Suppression des doublons et des valeurs manquantes\n",
    "        ppi = ppi.dropna().drop_duplicates()\n",
    "        \n",
    "        logger.info(f\"Loaded PPI network with {len(ppi)} interactions\")\n",
    "        return ppi\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading PPI network: {e}\")\n",
    "        return pd.DataFrame(columns=['protein1', 'protein2'])\n",
    "\n",
    "def main():\n",
    "    # Chargement des mappings\n",
    "    uniprot_to_sgd, sgd_to_uniprot = load_mapping(MAPPING_FILE)\n",
    "    \n",
    "    # Configuration des réseaux à traiter\n",
    "    networks = {\n",
    "        \"STRING_levure\": (GO_SLIM_YEAST, False),\n",
    "        \"BIOGRID_humain\": (GO_SLIM_HUMAN, True),\n",
    "        \"STRING_humain\": (GO_SLIM_HUMAN, True),\n",
    "        \"BIOGRID_levure\": (GO_SLIM_YEAST, False),\n",
    "        \"DIP_levure\": (GO_SLIM_YEAST, False)   \n",
    "    }\n",
    "\n",
    "    for network_name, (go_file, is_human) in networks.items():\n",
    "        logger.info(f\"\\nProcessing {network_name} network\")\n",
    "        \n",
    "        # Chargement du réseau PPI\n",
    "        ppi_file = INTERACTIONS_DIR / f\"{network_name}.txt\"\n",
    "        ppi = load_ppi_network(ppi_file)\n",
    "        \n",
    "        if ppi.empty:\n",
    "            logger.warning(f\"No valid interactions found for {network_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Chargement des annotations GO avec conversion si nécessaire\n",
    "        go_annotations = load_go_slim(\n",
    "            go_file, \n",
    "            is_human=is_human,\n",
    "            sgd_to_uniprot=sgd_to_uniprot if not is_human else None\n",
    "        )\n",
    "        \n",
    "        if go_annotations.empty:\n",
    "            logger.warning(f\"No GO annotations found for {network_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Liste de toutes les protéines du réseau\n",
    "        all_proteins = list(set(ppi['protein1']).union(set(ppi['protein2'])))\n",
    "        \n",
    "        # Calcul de la similarité fonctionnelle\n",
    "        similarity_matrix = calculate_functional_similarity(all_proteins, go_annotations)\n",
    "        \n",
    "        if similarity_matrix is None:\n",
    "            logger.warning(f\"Could not compute similarity matrix for {network_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Ajout des poids aux interactions\n",
    "        ppi['weight'] = ppi.apply(\n",
    "            lambda row: similarity_matrix.loc[row['protein1'], row['protein2']]\n",
    "            if row['protein1'] in similarity_matrix.index and row['protein2'] in similarity_matrix.columns\n",
    "            else 0.0,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Filtrage des interactions avec poids > 0\n",
    "        ppi = ppi[ppi['weight'] > 0]\n",
    "        \n",
    "        # Sauvegarde\n",
    "        output_file = OUTPUT_DIR / f\"FS_{network_name}.txt\"\n",
    "        ppi.to_csv(output_file, sep='\\t', index=False)\n",
    "        logger.info(f\"Saved weighted network to {output_file} with {len(ppi)} interactions\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Co-Expression similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Traitement des données human ===\n",
      "\n",
      "Statistiques de mapping:\n",
      "- Uniprot IDs: 237779\n",
      "- Gene names: 26246\n",
      "- Gene IDs: 0\n",
      "- ORFs: 0\n",
      "- Ensembl IDs: 24400\n",
      "- STRING IDs: 19110\n",
      "\n",
      "Données d'expression chargées: 28869 protéines\n",
      "\n",
      "Traitement de BIOGRID_humain.txt\n",
      "Interactions chargées: 87806\n",
      "\n",
      "Recouvrement entre PPI et expression:\n",
      "- Protéines dans PPI: 11217\n",
      "- Protéines dans expression: 17721\n",
      "- Protéines communes: 10423\n",
      "\n",
      "Résultats sauvegardés dans C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data\\clean data/autres/coexpression_BIOGRID_humain.txt (72599 paires valides)\n",
      "\n",
      "Traitement de STRING_humain.txt\n",
      "Interactions chargées: 349046\n",
      "\n",
      "Recouvrement entre PPI et expression:\n",
      "- Protéines dans PPI: 17723\n",
      "- Protéines dans expression: 17721\n",
      "- Protéines communes: 15876\n",
      "\n",
      "Résultats sauvegardés dans C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data\\clean data/autres/coexpression_STRING_humain.txt (277922 paires valides)\n",
      "\n",
      "Traitement de STRING_humain_filtered_interactions.txt\n",
      "Avertissement: Aucune interaction valide après mapping!\n",
      "\n",
      "=== Traitement des données levure ===\n",
      "\n",
      "Statistiques de mapping:\n",
      "- Uniprot IDs: 6900\n",
      "- Gene names: 5270\n",
      "- Gene IDs: 6738\n",
      "- ORFs: 0\n",
      "- Ensembl IDs: 0\n",
      "- STRING IDs: 6156\n",
      "\n",
      "Données d'expression chargées: 10928 protéines\n",
      "\n",
      "Traitement de BIOGRID_levure.txt\n",
      "Interactions chargées: 300000\n",
      "\n",
      "Recouvrement entre PPI et expression:\n",
      "- Protéines dans PPI: 5805\n",
      "- Protéines dans expression: 5651\n",
      "- Protéines communes: 5589\n",
      "\n",
      "Résultats sauvegardés dans C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data\\clean data/autres/coexpression_BIOGRID_levure.txt (286549 paires valides)\n",
      "\n",
      "Traitement de DIP_levure.txt\n",
      "Interactions chargées: 21992\n",
      "\n",
      "Recouvrement entre PPI et expression:\n",
      "- Protéines dans PPI: 4933\n",
      "- Protéines dans expression: 5651\n",
      "- Protéines communes: 4587\n",
      "\n",
      "Résultats sauvegardés dans C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data\\clean data/autres/coexpression_DIP_levure.txt (20652 paires valides)\n",
      "\n",
      "Traitement de STRING_levure.temp\n",
      "Avertissement: Aucune interaction valide après mapping!\n",
      "\n",
      "Traitement de STRING_levure.txt\n",
      "Interactions chargées: 96810\n",
      "\n",
      "Recouvrement entre PPI et expression:\n",
      "- Protéines dans PPI: 5707\n",
      "- Protéines dans expression: 5651\n",
      "- Protéines communes: 5284\n",
      "\n",
      "Résultats sauvegardés dans C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data\\clean data/autres/coexpression_STRING_levure.txt (90224 paires valides)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "\n",
    "def load_mapping(file_path):\n",
    "    \"\"\"Charge les mappings d'identifiants avec tous les types pertinents\"\"\"\n",
    "    mapping = {\n",
    "        'uniprot_to_gene': defaultdict(list),\n",
    "        'uniprot_to_name': defaultdict(list),\n",
    "        'gene_to_uniprot': defaultdict(list),\n",
    "        'name_to_uniprot': defaultdict(list),\n",
    "        'orf_to_uniprot': defaultdict(list),\n",
    "        'uniprot_to_uniprot': defaultdict(list),\n",
    "        'ensembl_to_uniprot': defaultdict(list),\n",
    "        'string_to_uniprot': defaultdict(list)\n",
    "    }\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 3:\n",
    "                uniprot_id = parts[0]\n",
    "                db = parts[1]\n",
    "                db_id = parts[2]\n",
    "                \n",
    "                # Mapper Uniprot à lui-même\n",
    "                mapping['uniprot_to_uniprot'][uniprot_id].append(uniprot_id)\n",
    "                \n",
    "                if db == 'Gene_OrderedLocusName':\n",
    "                    mapping['uniprot_to_gene'][uniprot_id].append(db_id)\n",
    "                    mapping['gene_to_uniprot'][db_id].append(uniprot_id)\n",
    "                elif db == 'Gene_Name':\n",
    "                    mapping['uniprot_to_name'][uniprot_id].append(db_id)\n",
    "                    mapping['name_to_uniprot'][db_id.upper()].append(uniprot_id)\n",
    "                elif db == 'ORF':\n",
    "                    mapping['orf_to_uniprot'][db_id].append(uniprot_id)\n",
    "                elif db == 'Ensembl':\n",
    "                    mapping['ensembl_to_uniprot'][db_id].append(uniprot_id)\n",
    "                elif db == 'STRING':\n",
    "                    mapping['string_to_uniprot'][db_id].append(uniprot_id)\n",
    "    \n",
    "    # Simplifier les mappings en gardant le premier élément\n",
    "    simple_mappings = {}\n",
    "    for map_type in mapping:\n",
    "        simple_mappings[map_type] = {k: v[0] for k, v in mapping[map_type].items() if v}\n",
    "    \n",
    "    return simple_mappings\n",
    "\n",
    "def load_expression_data(file_path, mappings, species):\n",
    "    \"\"\"Charge les données d'expression en fonction de l'espèce\"\"\"\n",
    "    try:\n",
    "        # Trouver le début des données\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('!dataset_table_begin'):\n",
    "                    break\n",
    "            \n",
    "            # Lire les données avec toutes les colonnes\n",
    "            df = pd.read_csv(f, sep='\\t', dtype=str, low_memory=False)\n",
    "        \n",
    "        # Extraire les colonnes d'expression (commençant par GSM)\n",
    "        expression_cols = [col for col in df.columns if col.startswith('GSM')]\n",
    "        \n",
    "        # Vérifier que toutes les colonnes GSM ont le même nombre de valeurs\n",
    "        gsm_lengths = {col: len(df[col].dropna()) for col in expression_cols}\n",
    "        if len(set(gsm_lengths.values())) > 1:\n",
    "            print(\"Avertissement: Les colonnes GSM ont des longueurs différentes!\")\n",
    "            # Prendre le nombre minimal de valeurs\n",
    "            min_length = min(gsm_lengths.values())\n",
    "            for col in expression_cols:\n",
    "                df[col] = df[col].dropna().head(min_length)\n",
    "        \n",
    "        # Garder seulement les colonnes nécessaires\n",
    "        df = df[['IDENTIFIER'] + expression_cols].dropna()\n",
    "        df = df.set_index('IDENTIFIER')\n",
    "        \n",
    "        # Convertir en numérique en gérant les erreurs\n",
    "        for col in expression_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        df = df.dropna(how='all')\n",
    "        \n",
    "        # Mapping différent selon l'espèce\n",
    "        if species == 'human':\n",
    "            # Pour les données humaines, essayer plusieurs stratégies\n",
    "            def human_mapper(x):\n",
    "                try:\n",
    "                    x = str(x).upper()\n",
    "                    if x in mappings['name_to_uniprot']:\n",
    "                        return mappings['name_to_uniprot'][x]\n",
    "                    return None\n",
    "                except:\n",
    "                    return None\n",
    "            \n",
    "            df.index = df.index.map(human_mapper)\n",
    "        else:\n",
    "            # Pour la levure, utiliser les ORFs/noms de gènes\n",
    "            def yeast_mapper(x):\n",
    "                try:\n",
    "                    x = str(x)\n",
    "                    if x in mappings['gene_to_uniprot']:\n",
    "                        return mappings['gene_to_uniprot'][x]\n",
    "                    if x in mappings['orf_to_uniprot']:\n",
    "                        return mappings['orf_to_uniprot'][x]\n",
    "                    if x.upper() in mappings['name_to_uniprot']:\n",
    "                        return mappings['name_to_uniprot'][x.upper()]\n",
    "                    return None\n",
    "                except:\n",
    "                    return None\n",
    "            \n",
    "            df.index = df.index.map(yeast_mapper)\n",
    "        \n",
    "        return df.dropna()\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données d'expression: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_pcc(v, u):\n",
    "    \"\"\"Calcule le coefficient de corrélation de Pearson avec vérification de longueur\"\"\"\n",
    "    try:\n",
    "        # Convertir en arrays numpy et s'assurer qu'ils sont 1D\n",
    "        v = np.asarray(v, dtype=float).flatten()\n",
    "        u = np.asarray(u, dtype=float).flatten()\n",
    "        \n",
    "        # Trouver la longueur minimale\n",
    "        min_len = min(len(v), len(u))\n",
    "        if min_len < 2:\n",
    "            return np.nan\n",
    "            \n",
    "        # Tronquer les vecteurs à la même longueur\n",
    "        v = v[:min_len]\n",
    "        u = u[:min_len]\n",
    "        \n",
    "        # Masque pour les valeurs manquantes\n",
    "        mask = ~np.isnan(v) & ~np.isnan(u)\n",
    "        v_clean = v[mask]\n",
    "        u_clean = u[mask]\n",
    "        \n",
    "        if len(v_clean) < 2:\n",
    "            return np.nan\n",
    "        \n",
    "        # Vérifier si l'un des vecteurs est constant\n",
    "        if np.all(v_clean == v_clean[0]) or np.all(u_clean == u_clean[0]):\n",
    "            return np.nan\n",
    "            \n",
    "        pcc, _ = pearsonr(v_clean, u_clean)\n",
    "        return (pcc + 1) / 2  # Transformation vers [0,1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating PCC between shapes {np.shape(v)} and {np.shape(u)}: {str(e)}\")\n",
    "        return np.nan\n",
    "\n",
    "def calculate_coexpression(ppi, expr_data, output_file):\n",
    "    \"\"\"Calcule et sauvegarde les scores de co-expression\"\"\"\n",
    "    results = []\n",
    "    proteins_in_ppi = set(ppi['protein1']).union(set(ppi['protein2']))\n",
    "    proteins_in_expr = set(expr_data.index)\n",
    "    common_proteins = proteins_in_ppi.intersection(proteins_in_expr)\n",
    "    \n",
    "    print(f\"\\nRecouvrement entre PPI et expression:\")\n",
    "    print(f\"- Protéines dans PPI: {len(proteins_in_ppi)}\")\n",
    "    print(f\"- Protéines dans expression: {len(proteins_in_expr)}\")\n",
    "    print(f\"- Protéines communes: {len(common_proteins)}\")\n",
    "    \n",
    "    if len(common_proteins) < 2:\n",
    "        print(\"Avertissement: Pas assez de protéines communes pour calculer la co-expression!\")\n",
    "        return\n",
    "    \n",
    "    # Pré-traitement: s'assurer que toutes les protéines ont le même nombre de points de données\n",
    "    min_length = min([len(expr_data.loc[p].values) for p in common_proteins])\n",
    "    expr_data = expr_data.apply(lambda x: x[:min_length], axis=1)\n",
    "    \n",
    "    for _, row in ppi.iterrows():\n",
    "        p1, p2 = row['protein1'], row['protein2']\n",
    "        if p1 in expr_data.index and p2 in expr_data.index:\n",
    "            # Get expression values and ensure they're 1D arrays\n",
    "            v = expr_data.loc[p1].values\n",
    "            u = expr_data.loc[p2].values\n",
    "            \n",
    "            # Flatten arrays in case they're 2D\n",
    "            v = np.asarray(v).flatten()\n",
    "            u = np.asarray(u).flatten()\n",
    "            \n",
    "            # Skip if one array is empty\n",
    "            if len(v) == 0 or len(u) == 0:\n",
    "                continue\n",
    "                \n",
    "            pcc = calculate_pcc(v, u)\n",
    "            if not np.isnan(pcc):\n",
    "                results.append({'protein1': p1, 'protein2': p2, 'PCC': pcc})\n",
    "    \n",
    "    if results:\n",
    "        result_df = pd.DataFrame(results)\n",
    "        result_df.to_csv(output_file, sep='\\t', index=False)\n",
    "        print(f\"\\nRésultats sauvegardés dans {output_file} ({len(result_df)} paires valides)\")\n",
    "    else:\n",
    "        print(\"\\nAucun résultat valide à sauvegarder.\")\n",
    "\n",
    "def process_dataset(base_dir, species):\n",
    "    \"\"\"Traite un ensemble de données complet\"\"\"\n",
    "    print(f\"\\n=== Traitement des données {species} ===\")\n",
    "    \n",
    "    # Déterminer les noms de fichiers en fonction de l'espèce\n",
    "    if species == 'human':\n",
    "        mapping_file = os.path.join(base_dir, \"raw data/autres/HUMAN_9606_idmapping.dat\")\n",
    "        expr_file = os.path.join(base_dir, \"raw data/autres/human_co-expression.soft\")\n",
    "        ppi_pattern = os.path.join(base_dir, \"clean data/interactions/*humain*\")\n",
    "    else:  # levure\n",
    "        mapping_file = os.path.join(base_dir, \"raw data/autres/YEAST_559292_idmapping.dat\")\n",
    "        expr_file = os.path.join(base_dir, \"raw data/autres/levure_co-expression.soft\")\n",
    "        ppi_pattern = os.path.join(base_dir, \"clean data/interactions/*levure*\")\n",
    "    \n",
    "    # Charger les mappings\n",
    "    if not os.path.exists(mapping_file):\n",
    "        print(f\"Fichier de mapping introuvable: {mapping_file}\")\n",
    "        return\n",
    "    \n",
    "    mappings = load_mapping(mapping_file)\n",
    "    print(f\"\\nStatistiques de mapping:\")\n",
    "    print(f\"- Uniprot IDs: {len(mappings['uniprot_to_uniprot'])}\")\n",
    "    print(f\"- Gene names: {len(mappings['name_to_uniprot'])}\")\n",
    "    print(f\"- Gene IDs: {len(mappings['gene_to_uniprot'])}\")\n",
    "    print(f\"- ORFs: {len(mappings['orf_to_uniprot'])}\")\n",
    "    print(f\"- Ensembl IDs: {len(mappings['ensembl_to_uniprot'])}\")\n",
    "    print(f\"- STRING IDs: {len(mappings['string_to_uniprot'])}\")\n",
    "    \n",
    "    # Charger les données d'expression\n",
    "    if not os.path.exists(expr_file):\n",
    "        print(f\"Fichier d'expression introuvable: {expr_file}\")\n",
    "        return\n",
    "    \n",
    "    expr_data = load_expression_data(expr_file, mappings, species)\n",
    "    if expr_data.empty:\n",
    "        print(\"Avertissement: Aucune donnée d'expression valide après mapping!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nDonnées d'expression chargées: {len(expr_data)} protéines\")\n",
    "    \n",
    "    # Traiter les fichiers PPI\n",
    "    ppi_files = glob.glob(ppi_pattern)\n",
    "    if not ppi_files:\n",
    "        print(f\"Aucun fichier PPI trouvé avec le pattern: {ppi_pattern}\")\n",
    "    \n",
    "    for ppi_file in ppi_files:\n",
    "        print(f\"\\nTraitement de {os.path.basename(ppi_file)}\")\n",
    "        try:\n",
    "            ppi = pd.read_csv(ppi_file, sep='\\t', header=None, names=['protein1', 'protein2'], dtype=str)\n",
    "            \n",
    "            # Mapper les identifiants\n",
    "            ppi['protein1'] = ppi['protein1'].apply(lambda x: mappings['uniprot_to_uniprot'].get(x, None))\n",
    "            ppi['protein2'] = ppi['protein2'].apply(lambda x: mappings['uniprot_to_uniprot'].get(x, None))\n",
    "            ppi = ppi.dropna()\n",
    "            \n",
    "            if ppi.empty:\n",
    "                print(\"Avertissement: Aucune interaction valide après mapping!\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Interactions chargées: {len(ppi)}\")\n",
    "            \n",
    "            # Calculer la co-expression\n",
    "            output_name = os.path.splitext(os.path.basename(ppi_file))[0]\n",
    "            output_file = os.path.join(base_dir, f\"clean data/autres/coexpression_{output_name}.txt\")\n",
    "            \n",
    "            calculate_coexpression(ppi, expr_data, output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    base_dir = \"C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data\"\n",
    "    \n",
    "    # Créer le dossier de résultats s'il n'existe pas\n",
    "    os.makedirs(os.path.join(base_dir, \"clean data/autres\"), exist_ok=True)\n",
    "    \n",
    "    # Traiter les deux espèces\n",
    "    process_dataset(base_dir, 'human')\n",
    "    process_dataset(base_dir, 'levure')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Subcellular localization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 16:08:44,237 - INFO - \n",
      "==================================================\n",
      "2025-04-11 16:08:44,238 - INFO - Traitement des réseaux pour levure\n",
      "2025-04-11 16:08:44,238 - INFO - ==================================================\n",
      "2025-04-11 16:08:45,796 - INFO - 717525 entrées de compartiment valides chargées depuis C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\yeast_compartment_mapped.txt\n",
      "2025-04-11 16:08:45,797 - INFO - \n",
      "Traitement du réseau STRING...\n",
      "2025-04-11 16:08:45,879 - INFO - 48998 interactions chargées depuis C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_levure.txt\n",
      "2025-04-11 16:08:50,488 - WARNING - 175 protéines sans données de compartiment\n",
      "2025-04-11 16:08:50,693 - INFO - Réseau pondéré sauvegardé (46169 interactions)\n",
      "2025-04-11 16:08:50,693 - INFO - \n",
      "Traitement du réseau BIOGRID...\n",
      "2025-04-11 16:08:50,827 - INFO - 50000 interactions chargées depuis C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_levure.txt\n",
      "2025-04-11 16:08:55,514 - WARNING - 143 protéines sans données de compartiment\n",
      "2025-04-11 16:08:55,796 - INFO - Réseau pondéré sauvegardé (48757 interactions)\n",
      "2025-04-11 16:08:55,798 - INFO - \n",
      "Traitement du réseau DIP...\n",
      "2025-04-11 16:08:55,854 - INFO - 22614 interactions chargées depuis C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\DIP_levure.txt\n",
      "2025-04-11 16:08:58,448 - WARNING - 280 protéines sans données de compartiment\n",
      "2025-04-11 16:08:58,557 - INFO - Réseau pondéré sauvegardé (21582 interactions)\n",
      "2025-04-11 16:08:58,558 - INFO - \n",
      "==================================================\n",
      "2025-04-11 16:08:58,559 - INFO - Traitement des réseaux pour humain\n",
      "2025-04-11 16:08:58,560 - INFO - ==================================================\n",
      "2025-04-11 16:09:08,957 - INFO - 4472243 entrées de compartiment valides chargées depuis C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\human_compartment_mapped.txt\n",
      "2025-04-11 16:09:08,989 - INFO - \n",
      "Traitement du réseau STRING...\n",
      "2025-04-11 16:09:09,239 - INFO - 228171 interactions chargées depuis C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_humain.txt\n",
      "2025-04-11 16:09:31,018 - WARNING - 6 protéines sans données de compartiment\n",
      "2025-04-11 16:09:32,083 - INFO - Réseau pondéré sauvegardé (227814 interactions)\n",
      "2025-04-11 16:09:32,083 - INFO - \n",
      "Traitement du réseau BIOGRID...\n",
      "2025-04-11 16:09:32,200 - INFO - 88647 interactions chargées depuis C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_humain.txt\n",
      "2025-04-11 16:09:42,923 - WARNING - 119 protéines sans données de compartiment\n",
      "2025-04-11 16:09:43,406 - INFO - Réseau pondéré sauvegardé (86981 interactions)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler('ppi_weighting.log'),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "\n",
    "def read_interaction_file(file_path):\n",
    "    \"\"\"Lire un fichier d'interactions PPI\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t', header=None, usecols=[0, 1],\n",
    "                         dtype={0: str, 1: str})\n",
    "        df.columns = ['protein1', 'protein2']\n",
    "        df['protein1'] = df['protein1'].str.strip().str.upper()\n",
    "        df['protein2'] = df['protein2'].str.strip().str.upper()\n",
    "        if df.empty:\n",
    "            logging.warning(f\"Aucune interaction trouvée dans {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "        logging.info(f\"{len(df)} interactions chargées depuis {file_path}\")\n",
    "        return df.drop_duplicates()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lecture {file_path} : {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def read_compartment_file(file_path):\n",
    "    \"\"\"Lire et préparer les données de compartimentation (avec Mapped_UniProtKB_ID)\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep='\\t', \n",
    "                         usecols=['Mapped_UniProtKB_ID', 'GO_ID', 'Mapping_Status'],\n",
    "                         dtype=str)\n",
    "        df = df[df['Mapping_Status'] == 'Success'].copy()\n",
    "        df['Mapped_UniProtKB_ID'] = df['Mapped_UniProtKB_ID'].str.strip().str.upper()\n",
    "        df['GO_ID'] = df['GO_ID'].str.strip()\n",
    "        if df.empty:\n",
    "            logging.warning(f\"Aucune donnée valide dans {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "        logging.info(f\"{len(df)} entrées de compartiment valides chargées depuis {file_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lecture {file_path} : {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_sl_similarity(prot1_comps, prot2_comps):\n",
    "    \"\"\"Calculer la similarité de localisation subcellulaire\"\"\"\n",
    "    intersection = len(prot1_comps & prot2_comps)\n",
    "    if intersection == 0:\n",
    "        return 0.0\n",
    "    sl_v = len(prot1_comps)\n",
    "    sl_u = len(prot2_comps)\n",
    "    return (intersection ** 2) / (sl_v * sl_u)\n",
    "\n",
    "def calculate_weights(interaction_df, compartment_df):\n",
    "    \"\"\"Calculer les poids basés sur la similarité des compartiments\"\"\"\n",
    "    if interaction_df.empty or compartment_df.empty:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Dictionnaire des GO_IDs par protéine (ID = Mapped_UniProtKB_ID)\n",
    "    protein_compartments = compartment_df.groupby('Mapped_UniProtKB_ID')['GO_ID'].apply(set).to_dict()\n",
    "\n",
    "    results = []\n",
    "    missing_proteins = set()\n",
    "\n",
    "    for _, row in interaction_df.iterrows():\n",
    "        prot1, prot2 = row['protein1'], row['protein2']\n",
    "        if prot1 not in protein_compartments or prot2 not in protein_compartments:\n",
    "            if prot1 not in protein_compartments:\n",
    "                missing_proteins.add(prot1)\n",
    "            if prot2 not in protein_compartments:\n",
    "                missing_proteins.add(prot2)\n",
    "            continue\n",
    "        sim = calculate_sl_similarity(protein_compartments[prot1], protein_compartments[prot2])\n",
    "        if sim > 0:\n",
    "            results.append({'protein1': prot1, 'protein2': prot2, 'weight': sim})\n",
    "\n",
    "    if missing_proteins:\n",
    "        logging.warning(f\"{len(missing_proteins)} protéines sans données de compartiment\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def process_species_networks(species_files, compartment_files):\n",
    "    \"\"\"Traiter les réseaux PPI pour chaque espèce\"\"\"\n",
    "    for species, species_file in species_files.items():\n",
    "        logging.info(f\"\\n{'='*50}\")\n",
    "        logging.info(f\"Traitement des réseaux pour {species}\")\n",
    "        logging.info(f\"{'='*50}\")\n",
    "\n",
    "        # Charger les données de compartiment\n",
    "        compartment_df = read_compartment_file(compartment_files[species])\n",
    "        if compartment_df.empty:\n",
    "            logging.error(f\"Impossible de traiter {species} - données de compartiment vides\")\n",
    "            continue\n",
    "\n",
    "        for network_name, network_file in species_file.items():\n",
    "            logging.info(f\"\\nTraitement du réseau {network_name}...\")\n",
    "            interaction_df = read_interaction_file(network_file)\n",
    "            if interaction_df.empty:\n",
    "                logging.warning(\"Aucune interaction valide trouvée\")\n",
    "                continue\n",
    "\n",
    "            weighted_df = calculate_weights(interaction_df, compartment_df)\n",
    "            if weighted_df.empty:\n",
    "                logging.warning(f\"Aucune interaction valide avec poids pour {network_name}\")\n",
    "                continue\n",
    "\n",
    "            output_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\\\\"+f\"SL_{network_name}_{species}.txt\"\n",
    "            weighted_df.to_csv(output_file, sep='\\t', index=False, float_format='%.6f')\n",
    "            logging.info(f\"Réseau pondéré sauvegardé ({len(weighted_df)} interactions)\")\n",
    "\n",
    "def main():\n",
    "    BASE_DIR = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\")\n",
    "\n",
    "    species_files = {\n",
    "        \"levure\": {\n",
    "            \"STRING\": BASE_DIR / \"interactions/STRING_levure.txt\",\n",
    "            \"BIOGRID\": BASE_DIR / \"interactions/BIOGRID_levure.txt\",\n",
    "            \"DIP\": BASE_DIR / \"interactions/DIP_levure.txt\"\n",
    "        },\n",
    "        \"humain\": {\n",
    "            \"STRING\": BASE_DIR / \"interactions/STRING_humain.txt\",\n",
    "            \"BIOGRID\": BASE_DIR / \"interactions/BIOGRID_humain.txt\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    compartment_files = {\n",
    "        \"levure\": BASE_DIR / \"autres/yeast_compartment_mapped.txt\",\n",
    "        \"humain\": BASE_DIR / \"autres/human_compartment_mapped.txt\"\n",
    "    }\n",
    "\n",
    "    process_species_networks(species_files, compartment_files)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Common Neighbor similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traitement du fichier: BIOGRID_humain.txt\n",
      "Nombre d'interactions chargées: 88647\n",
      "Nombre de nœuds: 11266\n",
      "Nombre d'arêtes: 88647\n",
      "Résultats HCN sauvegardés dans C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data/clean data/autres\\HCN_scores_BIOGRID_humain.txt\n",
      "Score HCN moyen: 0.0011\n",
      "Nombre moyen de voisins communs: 9.14\n",
      "\n",
      "Traitement du fichier: BIOGRID_levure.txt\n",
      "Nombre d'interactions chargées: 50000\n",
      "Nombre de nœuds: 4938\n",
      "Nombre d'arêtes: 50000\n",
      "Résultats HCN sauvegardés dans C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data/clean data/autres\\HCN_scores_BIOGRID_levure.txt\n",
      "Score HCN moyen: 0.0007\n",
      "Nombre moyen de voisins communs: 7.66\n",
      "\n",
      "Traitement du fichier: DIP_levure.txt\n",
      "Nombre d'interactions chargées: 22615\n",
      "Nombre de nœuds: 5146\n",
      "Nombre d'arêtes: 22615\n",
      "Résultats HCN sauvegardés dans C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data/clean data/autres\\HCN_scores_DIP_levure.txt\n",
      "Score HCN moyen: 0.0011\n",
      "Nombre moyen de voisins communs: 2.07\n",
      "\n",
      "Traitement du fichier: STRING_humain.txt\n",
      "Nombre d'interactions chargées: 228171\n",
      "Nombre de nœuds: 15943\n",
      "Nombre d'arêtes: 228171\n",
      "Résultats HCN sauvegardés dans C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data/clean data/autres\\HCN_scores_STRING_humain.txt\n",
      "Score HCN moyen: 0.0024\n",
      "Nombre moyen de voisins communs: 32.87\n",
      "\n",
      "Traitement du fichier: STRING_levure.txt\n",
      "Nombre d'interactions chargées: 48998\n",
      "Nombre de nœuds: 4640\n",
      "Nombre d'arêtes: 48998\n",
      "Résultats HCN sauvegardés dans C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data/clean data/autres\\HCN_scores_STRING_levure.txt\n",
      "Score HCN moyen: 0.0073\n",
      "Nombre moyen de voisins communs: 46.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_ppi_network(file_path):\n",
    "    \"\"\"Charge un réseau PPI à partir d'un fichier\"\"\"\n",
    "    ppi = pd.read_csv(file_path, sep='\\t', header=None, names=['protein1', 'protein2'])\n",
    "    return ppi\n",
    "\n",
    "def build_graph(ppi_df):\n",
    "    \"\"\"Construit un graphe NetworkX à partir d'un dataframe PPI\"\"\"\n",
    "    G = nx.Graph()\n",
    "    for _, row in ppi_df.iterrows():\n",
    "        G.add_edge(row['protein1'], row['protein2'])\n",
    "    return G\n",
    "\n",
    "def calculate_hcn_similarity(G, output_file):\n",
    "    \"\"\"Calcule la similarité HCN pour toutes les paires de protéines connectées\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Pré-calculer les voisins pour tous les nœuds\n",
    "    neighbors = {node: set(G.neighbors(node)) for node in G.nodes()}\n",
    "    \n",
    "    for edge in G.edges():\n",
    "        v, u = edge\n",
    "        N_v = neighbors[v]\n",
    "        N_u = neighbors[u]\n",
    "        \n",
    "        # Calculer les composantes de la formule HCN\n",
    "        NCN = N_v & N_u  # Intersection des voisins (common neighbors)\n",
    "        N_union = N_v | N_u  # Union des voisins\n",
    "        \n",
    "        # Calculer chaque terme de la formule\n",
    "        numerator = len(NCN)**2\n",
    "        denominator = (len(N_v) * len(N_u) * len(N_union))\n",
    "        \n",
    "        # Éviter la division par zéro\n",
    "        if denominator == 0:\n",
    "            hcn = 0.0\n",
    "        else:\n",
    "            hcn = numerator / denominator\n",
    "        \n",
    "        results.append({\n",
    "            'protein1': v,\n",
    "            'protein2': u,\n",
    "            'HCN_score': hcn,\n",
    "        })\n",
    "    \n",
    "    # Sauvegarder les résultats avec seulement les 3 premières colonnes\n",
    "    result_df = pd.DataFrame(results)\n",
    "    result_df = result_df[['protein1', 'protein2', 'HCN_score']]  # Garder seulement les 3 premières colonnes\n",
    "    result_df.to_csv(output_file, sep='\\t', index=False)\n",
    "    print(f\"Résultats HCN sauvegardés dans {output_file}\")\n",
    "    return result_df\n",
    "\n",
    "def process_all_ppi_networks(base_dir, output_dir):\n",
    "    \"\"\"Traite tous les fichiers PPI dans le dossier spécifié\"\"\"\n",
    "    # Créer le dossier de sortie s'il n'existe pas\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Trouver tous les fichiers PPI\n",
    "    ppi_files = glob.glob(os.path.join(base_dir, \"*i*\"))[:5]  # Prendre les 5 premiers fichiers\n",
    "    \n",
    "    for ppi_file in ppi_files:\n",
    "        print(f\"\\nTraitement du fichier: {os.path.basename(ppi_file)}\")\n",
    "        \n",
    "        # Charger le réseau PPI\n",
    "        ppi_df = load_ppi_network(ppi_file)\n",
    "        print(f\"Nombre d'interactions chargées: {len(ppi_df)}\")\n",
    "        \n",
    "        # Construire le graphe\n",
    "        G = build_graph(ppi_df)\n",
    "        print(f\"Nombre de nœuds: {G.number_of_nodes()}\")\n",
    "        print(f\"Nombre d'arêtes: {G.number_of_edges()}\")\n",
    "        \n",
    "        # Calculer les scores HCN\n",
    "        output_name = os.path.splitext(os.path.basename(ppi_file))[0]\n",
    "        output_file = os.path.join(output_dir, f\"HCN_scores_{output_name}.txt\")\n",
    "        \n",
    "        hcn_results = calculate_hcn_similarity(G, output_file)\n",
    "        \n",
    "        # Afficher quelques statistiques\n",
    "        print(f\"Score HCN moyen: {hcn_results['HCN_score'].mean():.4f}\")\n",
    "        print(f\"Nombre moyen de voisins communs: {hcn_results['HCN_score'].count()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Configuration des chemins\n",
    "    base_dir = \"C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data/clean data/interactions\"\n",
    "    output_dir = \"C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data/clean data/autres\"\n",
    "    \n",
    "    # Exécuter le traitement pour tous les réseaux PPI\n",
    "    process_all_ppi_networks(base_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Weighted PPI Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 18:44:46,196 - INFO - \n",
      "========================================\n",
      "Traitement de BIOGRID_ (humain)\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 18:44:47,043 - INFO - FS: 80778 interactions\n",
      "2025-04-12 18:44:47,106 - INFO - SL: 86981 interactions\n",
      "2025-04-12 18:44:47,129 - INFO - CO: 72599 interactions\n",
      "2025-04-12 18:44:47,179 - INFO - HCN: 88647 interactions\n",
      "2025-04-12 18:44:47,241 - INFO - Interactions communes: 66373\n",
      "2025-04-12 18:44:47,791 - INFO - \n",
      "Réseau final sauvegardé dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\weighted_networks\\weighted_BIOGRID_humain.txt\n",
      "2025-04-12 18:44:47,808 - INFO - \n",
      "Résultats pour BIOGRID__humain:\n",
      "2025-04-12 18:44:47,809 - INFO - - Protéines uniques: 9171\n",
      "2025-04-12 18:44:47,809 - INFO - - Interactions: 66373\n",
      "2025-04-12 18:44:47,811 - INFO - - Poids moyen: 0.2936\n",
      "2025-04-12 18:44:47,830 - INFO - \n",
      "========================================\n",
      "Traitement de STRING_ (humain)\n",
      "========================================\n",
      "2025-04-12 18:44:50,343 - INFO - FS: 305683 interactions\n",
      "2025-04-12 18:44:50,628 - INFO - SL: 348452 interactions\n",
      "2025-04-12 18:44:50,810 - INFO - CO: 277922 interactions\n",
      "2025-04-12 18:44:50,989 - INFO - HCN: 349046 interactions\n",
      "2025-04-12 18:44:51,273 - INFO - Interactions communes: 246923\n",
      "2025-04-12 18:44:53,737 - INFO - \n",
      "Réseau final sauvegardé dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\weighted_networks\\weighted_STRING_humain.txt\n",
      "2025-04-12 18:44:53,820 - INFO - \n",
      "Résultats pour STRING__humain:\n",
      "2025-04-12 18:44:53,824 - INFO - - Protéines uniques: 14217\n",
      "2025-04-12 18:44:53,825 - INFO - - Interactions: 246923\n",
      "2025-04-12 18:44:53,827 - INFO - - Poids moyen: 0.2928\n",
      "2025-04-12 18:44:53,897 - INFO - \n",
      "========================================\n",
      "Traitement de BIOGRID_ (levure)\n",
      "========================================\n",
      "2025-04-12 18:44:55,455 - INFO - FS: 215304 interactions\n",
      "2025-04-12 18:44:55,594 - INFO - SL: 291326 interactions\n",
      "2025-04-12 18:44:55,722 - INFO - CO: 286549 interactions\n",
      "2025-04-12 18:44:55,868 - INFO - HCN: 300000 interactions\n",
      "2025-04-12 18:44:56,038 - INFO - Interactions communes: 200897\n",
      "2025-04-12 18:44:57,471 - INFO - \n",
      "Réseau final sauvegardé dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\weighted_networks\\weighted_BIOGRID_levure.txt\n",
      "2025-04-12 18:44:57,609 - INFO - \n",
      "Résultats pour BIOGRID__levure:\n",
      "2025-04-12 18:44:57,624 - INFO - - Protéines uniques: 5241\n",
      "2025-04-12 18:44:57,627 - INFO - - Interactions: 200897\n",
      "2025-04-12 18:44:57,627 - INFO - - Poids moyen: 0.2528\n",
      "2025-04-12 18:44:57,750 - INFO - \n",
      "========================================\n",
      "Traitement de DIP_ (levure)\n",
      "========================================\n",
      "2025-04-12 18:44:57,998 - INFO - FS: 16393 interactions\n",
      "2025-04-12 18:44:58,014 - INFO - SL: 21582 interactions\n",
      "2025-04-12 18:44:58,034 - INFO - CO: 20652 interactions\n",
      "2025-04-12 18:44:58,050 - INFO - HCN: 22614 interactions\n",
      "2025-04-12 18:44:58,056 - INFO - Interactions communes: 15376\n",
      "2025-04-12 18:44:58,152 - INFO - \n",
      "Réseau final sauvegardé dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\weighted_networks\\weighted_DIP_levure.txt\n",
      "2025-04-12 18:44:58,152 - INFO - \n",
      "Résultats pour DIP__levure:\n",
      "2025-04-12 18:44:58,152 - INFO - - Protéines uniques: 3973\n",
      "2025-04-12 18:44:58,167 - INFO - - Interactions: 15376\n",
      "2025-04-12 18:44:58,167 - INFO - - Poids moyen: 0.3245\n",
      "2025-04-12 18:44:58,167 - INFO - \n",
      "========================================\n",
      "Traitement de STRING_ (levure)\n",
      "========================================\n",
      "2025-04-12 18:44:59,244 - INFO - FS: 86142 interactions\n",
      "2025-04-12 18:44:59,315 - INFO - SL: 91378 interactions\n",
      "2025-04-12 18:44:59,387 - INFO - CO: 90224 interactions\n",
      "2025-04-12 18:44:59,412 - INFO - HCN: 48998 interactions\n",
      "2025-04-12 18:44:59,463 - INFO - Interactions communes: 41213\n",
      "2025-04-12 18:44:59,926 - INFO - \n",
      "Réseau final sauvegardé dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\weighted_networks\\weighted_STRING_levure.txt\n",
      "2025-04-12 18:44:59,947 - INFO - \n",
      "Résultats pour STRING__levure:\n",
      "2025-04-12 18:44:59,947 - INFO - - Protéines uniques: 4150\n",
      "2025-04-12 18:44:59,947 - INFO - - Interactions: 41213\n",
      "2025-04-12 18:44:59,947 - INFO - - Poids moyen: 0.3647\n",
      "2025-04-12 18:44:59,967 - INFO - \n",
      "\n",
      "Récapitulatif final:\n",
      "2025-04-12 18:44:59,984 - INFO - \n",
      "        network  unique_proteins  interactions  mean_weight\n",
      "BIOGRID__humain             9171         66373     0.293575\n",
      " STRING__humain            14217        246923     0.292783\n",
      "BIOGRID__levure             5241        200897     0.252786\n",
      "    DIP__levure             3973         15376     0.324516\n",
      " STRING__levure             4150         41213     0.364710\n",
      "2025-04-12 18:44:59,995 - INFO - \n",
      "Statistiques sauvegardées dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\network_statistics.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List, Set, Tuple\n",
    "import numpy as np\n",
    "\n",
    "# Configuration des chemins\n",
    "BASE_DIR = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\")\n",
    "CLEAN_DATA_DIR = BASE_DIR / \"clean data\"\n",
    "OUTPUT_DIR = CLEAN_DATA_DIR / \"autres\"\n",
    "WEIGHTED_DIR = CLEAN_DATA_DIR / \"weighted_networks\"\n",
    "\n",
    "# Création des dossiers s'ils n'existent pas\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "WEIGHTED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration du logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler(WEIGHTED_DIR / 'network_stats.log')\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_weighted_network(network_name: str, species: str, weight_type: str) -> pd.DataFrame:\n",
    "    \"\"\"Charge un réseau pondéré spécifique de manière optimisée\"\"\"\n",
    "    file_patterns = {\n",
    "        'FS': f\"FS_{network_name}{species}.txt\",\n",
    "        'SL': f\"SL_{network_name}{species}.txt\", \n",
    "        'CO': f\"coexpression_{network_name}{species}.txt\",\n",
    "        'HCN': f\"HCN_scores_{network_name}{species}.txt\"\n",
    "    }\n",
    "    \n",
    "    file_path = OUTPUT_DIR / file_patterns.get(weight_type, \"\")\n",
    "    try:\n",
    "        # Chargement optimisé avec dtype spécifié et utilisation de low_memory\n",
    "        df = pd.read_csv(file_path, sep='\\t', dtype={'protein1': 'str', 'protein2': 'str', 'weight': 'float64'}, \n",
    "                         low_memory=False)\n",
    "        df.columns = df.columns.str.lower()\n",
    "        \n",
    "        # Renommer la colonne weight si nécessaire\n",
    "        if 'weight' not in df.columns and len(df.columns) >= 3:\n",
    "            weight_col = df.columns[2]\n",
    "            df = df.rename(columns={weight_col: 'weight'})\n",
    "            df['weight'] = pd.to_numeric(df['weight'], errors='coerce')\n",
    "        \n",
    "        # Supprimer les doublons et les valeurs NaN\n",
    "        df = df.dropna(subset=['protein1', 'protein2', 'weight'])\n",
    "        df = df.drop_duplicates(subset=['protein1', 'protein2'])\n",
    "        \n",
    "        # Standardiser l'ordre des protéines pour éviter les doublons A-B et B-A\n",
    "        mask = df['protein1'] > df['protein2']\n",
    "        df.loc[mask, ['protein1', 'protein2']] = df.loc[mask, ['protein2', 'protein1']].values\n",
    "        \n",
    "        return df[['protein1', 'protein2', 'weight']]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Erreur de chargement {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def get_common_interactions(networks: Dict[str, pd.DataFrame]) -> Set[Tuple[str, str]]:\n",
    "    \"\"\"Trouve les interactions communes aux 4 réseaux de manière optimisée\"\"\"\n",
    "    if not networks:\n",
    "        return set()\n",
    "    \n",
    "    # Convertir chaque dataframe en un set de paires\n",
    "    pair_sets = []\n",
    "    for wt, df in networks.items():\n",
    "        # Utilisation de zip pour une création plus rapide des paires\n",
    "        pairs = set(zip(df['protein1'], df['protein2']))\n",
    "        pair_sets.append(pairs)\n",
    "        logger.info(f\"{wt}: {len(pairs)} interactions\")\n",
    "    \n",
    "    # Trouver l'intersection\n",
    "    common_pairs = set.intersection(*pair_sets)\n",
    "    logger.info(f\"Interactions communes: {len(common_pairs)}\")\n",
    "    \n",
    "    return common_pairs\n",
    "\n",
    "def calculate_final_network(common_pairs: Set[Tuple[str, str]], \n",
    "                          networks: Dict[str, pd.DataFrame]) -> pd.DataFrame:\n",
    "    \"\"\"Calcule le réseau final avec poids moyens de manière optimisée\"\"\"\n",
    "    # Convertir common_pairs en DataFrame pour une jointure efficace\n",
    "    common_df = pd.DataFrame(list(common_pairs), columns=['protein1', 'protein2'])\n",
    "    \n",
    "    # Préparer un dictionnaire pour stocker les poids moyens\n",
    "    weight_data = {\n",
    "        'protein1': common_df['protein1'],\n",
    "        'protein2': common_df['protein2'],\n",
    "        'weights': np.zeros(len(common_df))\n",
    "    }\n",
    "    \n",
    "    # Compter le nombre de réseaux pour chaque paire\n",
    "    count = np.zeros(len(common_df))\n",
    "    \n",
    "    for df in networks.values():\n",
    "        # Fusionner avec le réseau courant\n",
    "        merged = pd.merge(common_df, df, on=['protein1', 'protein2'], how='left')\n",
    "        # Ajouter les poids non nuls\n",
    "        valid_weights = merged['weight'].fillna(0).values\n",
    "        weight_data['weights'] += valid_weights\n",
    "        count += (valid_weights != 0)\n",
    "    \n",
    "    # Calculer la moyenne (en évitant la division par zéro)\n",
    "    mean_weights = np.divide(weight_data['weights'], count, where=(count != 0))\n",
    "    \n",
    "    # Créer le DataFrame final\n",
    "    final_network = pd.DataFrame({\n",
    "        'protein1': weight_data['protein1'],\n",
    "        'protein2': weight_data['protein2'],\n",
    "        'mean_weight': mean_weights\n",
    "    })\n",
    "    \n",
    "    # Supprimer les paires sans aucun poids\n",
    "    final_network = final_network[count > 0]\n",
    "    \n",
    "    return final_network\n",
    "\n",
    "def analyze_final_network(df: pd.DataFrame, network_name: str) -> Dict[str, int]:\n",
    "    \"\"\"Analyse le réseau final et retourne les statistiques\"\"\"\n",
    "    if df.empty:\n",
    "        return {}\n",
    "    \n",
    "    unique_proteins = pd.unique(np.concatenate([df['protein1'], df['protein2']])).size\n",
    "    num_interactions = len(df)\n",
    "    mean_w = df['mean_weight'].mean()\n",
    "    \n",
    "    logger.info(f\"\\nRésultats pour {network_name}:\")\n",
    "    logger.info(f\"- Protéines uniques: {unique_proteins}\")\n",
    "    logger.info(f\"- Interactions: {num_interactions}\")\n",
    "    logger.info(f\"- Poids moyen: {mean_w:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'network': network_name,\n",
    "        'unique_proteins': unique_proteins,\n",
    "        'interactions': num_interactions,\n",
    "        'mean_weight': mean_w\n",
    "    }\n",
    "\n",
    "def process_single_network(network_name: str, species: str) -> Dict[str, int]:\n",
    "    \"\"\"Traite un seul réseau PPI de manière optimisée\"\"\"\n",
    "    logger.info(f\"\\n{'='*40}\\nTraitement de {network_name} ({species})\\n{'='*40}\")\n",
    "    \n",
    "    # Charger les 4 versions pondérées\n",
    "    weighted_types = ['FS', 'SL', 'CO', 'HCN']\n",
    "    networks = {wt: load_weighted_network(network_name, species, wt) for wt in weighted_types}\n",
    "    \n",
    "    # Vérifier la présence des 4 réseaux\n",
    "    if any(df.empty for df in networks.values()):\n",
    "        missing = [wt for wt, df in networks.items() if df.empty]\n",
    "        logger.warning(f\"Réseaux manquants: {missing}\")\n",
    "        return {}\n",
    "    \n",
    "    # Trouver les interactions communes\n",
    "    common_pairs = get_common_interactions(networks)\n",
    "    if not common_pairs:\n",
    "        logger.warning(\"Aucune interaction commune trouvée\")\n",
    "        return {}\n",
    "    \n",
    "    # Calculer le réseau final\n",
    "    final_network = calculate_final_network(common_pairs, networks)\n",
    "    \n",
    "    if final_network.empty:\n",
    "        logger.warning(\"Aucune interaction valide après calcul des poids moyens\")\n",
    "        return {}\n",
    "    \n",
    "    # Sauvegarder le résultat\n",
    "    output_file = WEIGHTED_DIR / f\"weighted_{network_name}{species}.txt\"\n",
    "    final_network.to_csv(output_file, sep='\\t', index=False)\n",
    "    logger.info(f\"\\nRéseau final sauvegardé dans {output_file}\")\n",
    "    \n",
    "    # Analyser et retourner les statistiques\n",
    "    return analyze_final_network(final_network, f\"{network_name}_{species}\")\n",
    "\n",
    "def main():\n",
    "    # Liste des réseaux à traiter avec les espèces\n",
    "    network_species_pairs = [\n",
    "        (\"BIOGRID_\", \"humain\"),\n",
    "        (\"STRING_\", \"humain\"),\n",
    "        (\"BIOGRID_\", \"levure\"),\n",
    "        (\"DIP_\", \"levure\"),\n",
    "        (\"STRING_\", \"levure\")\n",
    "    ]\n",
    "    \n",
    "    all_stats = []\n",
    "    \n",
    "    # Traiter chaque réseau\n",
    "    for network_name, species in network_species_pairs:\n",
    "        stats = process_single_network(network_name, species)\n",
    "        if stats:\n",
    "            all_stats.append(stats)\n",
    "    \n",
    "    # Afficher le récapitulatif\n",
    "    logger.info(\"\\n\\nRécapitulatif final:\")\n",
    "    stats_df = pd.DataFrame(all_stats)\n",
    "    logger.info(\"\\n\" + stats_df.to_string(index=False))\n",
    "    \n",
    "    # Sauvegarder les statistiques\n",
    "    stats_file = OUTPUT_DIR / \"network_statistics.txt\"\n",
    "    stats_df.to_csv(stats_file, sep='\\t', index=False)\n",
    "    logger.info(f\"\\nStatistiques sauvegardées dans {stats_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
