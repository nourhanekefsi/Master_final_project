{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Filtrage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Base de donnees STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Traitement (filtrage) terminé avec succès.\n",
      "  - Nombre de protéines uniques : 4708\n",
      "  - Nombre d'interactions uniques : 53150\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemins des fichiers\n",
    "input_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\STRING_Interactions.txt\")\n",
    "output_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_filtered_interactions.txt\")\n",
    "\n",
    "# 1. Chargement et filtrage initial\n",
    "df = pd.read_csv(input_path, sep=\" \")\n",
    "\n",
    "# Créer un masque de filtrage\n",
    "mask = (df[\"combined_score\"] >= 900) & (\n",
    "    df[[\"experimental\", \"coexpression\", \"database\", \"textmining\"]].max(axis=1) > 90\n",
    ")\n",
    "\n",
    "# Appliquer le filtre et faire une copie explicite\n",
    "filtered_df = df.loc[mask].copy()\n",
    "\n",
    "# 2. Nettoyage des données\n",
    "# Supprimer \"4932.\" des noms de protéines\n",
    "for col in ['protein1', 'protein2']:\n",
    "    filtered_df.loc[:, col] = filtered_df[col].str.replace('4932.', '')\n",
    "\n",
    "# 3. Gestion des interactions uniques\n",
    "# Créer des identifiants d'interaction canoniques\n",
    "filtered_df.loc[:, 'interaction_key'] = filtered_df.apply(\n",
    "    lambda x: frozenset({x['protein1'], x['protein2']}), axis=1\n",
    ")\n",
    "\n",
    "# Supprimer les doublons en gardant la première occurrence\n",
    "unique_interactions_df = filtered_df.drop_duplicates(subset='interaction_key')\n",
    "\n",
    "# 4. Sauvegarde des résultats\n",
    "unique_interactions_df[['protein1', 'protein2']].to_csv(\n",
    "    output_path, sep=\"\\t\", index=False, header=False\n",
    ")\n",
    "\n",
    "# 5. Calcul des statistiques\n",
    "unique_proteins = pd.unique(\n",
    "    unique_interactions_df[['protein1', 'protein2']].values.ravel('K')\n",
    ")\n",
    "num_interactions = len(unique_interactions_df)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\n Traitement (filtrage) terminé avec succès.\")\n",
    "print(f\"  - Nombre de protéines uniques : {len(unique_proteins)}\")\n",
    "print(f\"  - Nombre d'interactions uniques : {num_interactions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Base de donnees DIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats du traitement (Saccharomyces cerevisiae uniquement):\n",
      "- Protéines uniques: 5012\n",
      "- Interactions uniques: 22436\n",
      "Fichier sauvegardé: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\DIP_filtered_interactions.txt\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "def process_dip_interactions():\n",
    "    # Configuration des chemins\n",
    "    input_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\DIP_Interactions.mif25\")\n",
    "    output_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\DIP_filtered_interactions.txt\")\n",
    "    \n",
    "    NS = {'mif': 'http://psi.hupo.org/mi/mif'}\n",
    "    TARGET_TAXID = \"4932\"  # TaxID de Saccharomyces cerevisiae\n",
    "    \n",
    "    tree = ET.parse(input_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # 1. Extraction des protéines avec vérification de l'organisme\n",
    "    id_to_protein = {}\n",
    "    for interactor in root.findall(\".//mif:interactor\", NS):\n",
    "        interactor_id = interactor.get(\"id\")\n",
    "        if not interactor_id:\n",
    "            continue\n",
    "            \n",
    "        # Vérification que la protéine appartient à la levure\n",
    "        organism = interactor.find(\".//mif:organism\", NS)\n",
    "        if organism is None or organism.get(\"ncbiTaxId\") != TARGET_TAXID:\n",
    "            continue\n",
    "            \n",
    "        uniprot_ref = interactor.find(\".//mif:xref/mif:secondaryRef[@db='uniprot knowledge base']\", NS)\n",
    "        refseq_ref = interactor.find(\".//mif:xref/mif:secondaryRef[@db='refseq']\", NS)\n",
    "        \n",
    "        protein_id = None\n",
    "        if uniprot_ref is not None:\n",
    "            protein_id = uniprot_ref.get(\"id\")\n",
    "        elif refseq_ref is not None:\n",
    "            protein_id = refseq_ref.get(\"id\")\n",
    "        else:\n",
    "            short_label = interactor.find(\".//mif:names/mif:shortLabel\", NS)\n",
    "            protein_id = short_label.text if short_label is not None else None\n",
    "        \n",
    "        if protein_id:\n",
    "            id_to_protein[interactor_id] = protein_id\n",
    "\n",
    "    # 2. Extraction des interactions avec contrôle qualité\n",
    "    unique_interactions = set()\n",
    "    protein_set = set()  # Pour stocker les protéines uniques\n",
    "    \n",
    "    for interaction in root.findall(\".//mif:interaction\", NS):\n",
    "        participants = interaction.findall(\".//mif:participant/mif:interactorRef\", NS)\n",
    "        \n",
    "        if len(participants) != 2:\n",
    "            continue\n",
    "            \n",
    "        id1, id2 = participants[0].text, participants[1].text\n",
    "        \n",
    "        # Vérification que les deux protéines existent, sont différentes et appartiennent à la levure\n",
    "        if (id1 not in id_to_protein or \n",
    "            id2 not in id_to_protein or \n",
    "            id_to_protein[id1] == id_to_protein[id2]):\n",
    "            continue\n",
    "            \n",
    "        prot1, prot2 = id_to_protein[id1], id_to_protein[id2]\n",
    "        \n",
    "        # Ajout aux protéines uniques\n",
    "        protein_set.add(prot1)\n",
    "        protein_set.add(prot2)\n",
    "        \n",
    "        # Vérification du score\n",
    "        score_element = interaction.find(\".//mif:confidence/mif:value\", NS)\n",
    "        if score_element is not None:\n",
    "            try:\n",
    "                score = float(score_element.text)\n",
    "                if score <= 0.8:\n",
    "                    continue\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "                \n",
    "        # Ajout sous forme triée pour éviter les doublons A-B vs B-A\n",
    "        sorted_interaction = tuple(sorted((prot1, prot2)))\n",
    "        unique_interactions.add(sorted_interaction)\n",
    "\n",
    "    # 3. Sauvegarde\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"Protein1\\tProtein2\\n\")\n",
    "        for prot1, prot2 in unique_interactions:\n",
    "            f.write(f\"{prot1}\\t{prot2}\\n\")\n",
    "\n",
    "    # 4. Calcul et affichage des statistiques\n",
    "    num_unique_proteins = len(protein_set)\n",
    "    num_unique_interactions = len(unique_interactions)\n",
    "    \n",
    "    print(\"\\nRésultats du traitement (Saccharomyces cerevisiae uniquement):\")\n",
    "    print(f\"- Protéines uniques: {num_unique_proteins}\")\n",
    "    print(f\"- Interactions uniques: {num_unique_interactions}\")\n",
    "    print(f\"Fichier sauvegardé: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dip_interactions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Base de donnes BIOGRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats sélection:\n",
      "- Interactions totales (levure): 855577\n",
      "- Interactions haute confiance: 224490\n",
      "- Interactions sélectionnées: 40000\n",
      "- Protéines uniques: 5519\n",
      "\n",
      "Top 40k interactions sauvegardées dans: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_top_40000.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def process_biogrid_high_confidence():\n",
    "    # Chemins des fichiers\n",
    "    input_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\BIOGRID-ORGANISM-Saccharomyces_cerevisiae.txt\")\n",
    "    output_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_top_40000.tsv\")\n",
    "    \n",
    "    # Charger les données\n",
    "    df = pd.read_csv(input_file, sep='\\t', comment='#', header=None)\n",
    "    df.columns = [\n",
    "        'ID_A', 'ID_B', 'Alt_IDs_A', 'Alt_IDs_B', \n",
    "        'Aliases_A', 'Aliases_B', 'Method', 'Author',\n",
    "        'PubIDs', 'TaxID_A', 'TaxID_B', 'IntType',\n",
    "        'SourceDB', 'IntIDs', 'Confidence'\n",
    "    ]\n",
    "    \n",
    "    # 1. Filtrer pour la levure S288c\n",
    "    yeast_df = df[(df['TaxID_A'] == 'taxid:559292') & \n",
    "                 (df['TaxID_B'] == 'taxid:559292')].copy()\n",
    "    \n",
    "    # 2. Critères de haute confiance\n",
    "    high_conf_methods = [\n",
    "        'affinity chromatography',\n",
    "        'two hybrid',\n",
    "        'pull down',\n",
    "        'coimmunoprecipitation'\n",
    "    ]\n",
    "    \n",
    "    high_conf_types = [\n",
    "        'direct interaction',\n",
    "        'physical association'\n",
    "    ]\n",
    "    \n",
    "    # 3. Filtrer avec critères combinés\n",
    "    method_mask = yeast_df['Method'].str.contains('|'.join(high_conf_methods), case=False, na=False)\n",
    "    type_mask = yeast_df['IntType'].str.contains('|'.join(high_conf_types), case=False, na=False)\n",
    "    \n",
    "    high_conf = yeast_df[method_mask & type_mask].copy()\n",
    "    \n",
    "    # 4. Extraire les UniProt IDs\n",
    "    def extract_uniprot(alt_ids):\n",
    "        if pd.isna(alt_ids): return None\n",
    "        match = re.search(r'uniprot/swiss[\\W-]?prot:([A-Z0-9]{6,10})', str(alt_ids))\n",
    "        return match.group(1) if match else None\n",
    "    \n",
    "    high_conf.loc[:, 'Protein1'] = high_conf['Alt_IDs_A'].apply(extract_uniprot)\n",
    "    high_conf.loc[:, 'Protein2'] = high_conf['Alt_IDs_B'].apply(extract_uniprot)\n",
    "    \n",
    "    # 5. Nettoyage final\n",
    "    clean_df = high_conf.dropna(subset=['Protein1', 'Protein2'])\n",
    "    clean_df = clean_df[clean_df['Protein1'] != clean_df['Protein2']]\n",
    "    \n",
    "    # 6. Sélection des 40 000 interactions les plus fréquentes\n",
    "    # Compter les occurrences de chaque interaction\n",
    "    interaction_counts = pd.concat([\n",
    "        clean_df.groupby('Protein1').size().rename('Count'),\n",
    "        clean_df.groupby('Protein2').size().rename('Count')\n",
    "    ], axis=1).fillna(0)\n",
    "    interaction_counts['Total'] = interaction_counts.sum(axis=1)\n",
    "    \n",
    "    # Ajouter les scores aux interactions\n",
    "    clean_df = clean_df.merge(\n",
    "        interaction_counts[['Total']].rename(columns={'Total': 'Score1'}),\n",
    "        left_on='Protein1', right_index=True\n",
    "    ).merge(\n",
    "        interaction_counts[['Total']].rename(columns={'Total': 'Score2'}),\n",
    "        left_on='Protein2', right_index=True\n",
    "    )\n",
    "    clean_df['InteractionScore'] = clean_df['Score1'] + clean_df['Score2']\n",
    "    \n",
    "    # Trier et sélectionner le top 40 000\n",
    "    top_interactions = clean_df.sort_values('InteractionScore', ascending=False)\\\n",
    "                             .drop_duplicates(subset=['Protein1', 'Protein2'])\\\n",
    "                             .head(40000)\n",
    "    \n",
    "    # 7. Statistiques\n",
    "    unique_proteins = pd.unique(top_interactions[['Protein1', 'Protein2']].values.ravel('K'))\n",
    "    \n",
    "    print(\"\\nRésultats sélection:\")\n",
    "    print(f\"- Interactions totales (levure): {len(yeast_df)}\")\n",
    "    print(f\"- Interactions haute confiance: {len(high_conf)}\")\n",
    "    print(f\"- Interactions sélectionnées: {len(top_interactions)}\")\n",
    "    print(f\"- Protéines uniques: {len(unique_proteins)}\")\n",
    "    \n",
    "    # 8. Sauvegarde\n",
    "    top_interactions[['Protein1', 'Protein2']].to_csv(\n",
    "        output_file, \n",
    "        sep='\\t', \n",
    "        index=False, \n",
    "        header=False\n",
    "    )\n",
    "    print(f\"\\nTop 40k interactions sauvegardées dans: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_biogrid_high_confidence()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
