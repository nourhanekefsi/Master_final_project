{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Filtrage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Base de donnees STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Traitement (filtrage) terminé avec succès.\n",
      "  - Nombre de protéines uniques : 4708\n",
      "  - Nombre d'interactions uniques : 53150\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemins des fichiers\n",
    "input_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\STRING_Interactions.txt\")\n",
    "output_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_filtered_interactions.txt\")\n",
    "\n",
    "# 1. Chargement et filtrage initial\n",
    "df = pd.read_csv(input_path, sep=\" \")\n",
    "\n",
    "# Créer un masque de filtrage\n",
    "mask = (df[\"combined_score\"] >= 900) & (\n",
    "    df[[\"experimental\", \"coexpression\", \"database\", \"textmining\"]].max(axis=1) > 90\n",
    ")\n",
    "\n",
    "# Appliquer le filtre et faire une copie explicite\n",
    "filtered_df = df.loc[mask].copy()\n",
    "\n",
    "# 2. Nettoyage des données\n",
    "# Supprimer \"4932.\" des noms de protéines\n",
    "for col in ['protein1', 'protein2']:\n",
    "    filtered_df.loc[:, col] = filtered_df[col].str.replace('4932.', '')\n",
    "\n",
    "# 3. Gestion des interactions uniques\n",
    "# Créer des identifiants d'interaction canoniques\n",
    "filtered_df.loc[:, 'interaction_key'] = filtered_df.apply(\n",
    "    lambda x: frozenset({x['protein1'], x['protein2']}), axis=1\n",
    ")\n",
    "\n",
    "# Supprimer les doublons en gardant la première occurrence\n",
    "unique_interactions_df = filtered_df.drop_duplicates(subset='interaction_key')\n",
    "\n",
    "# 4. Sauvegarde des résultats\n",
    "unique_interactions_df[['protein1', 'protein2']].to_csv(\n",
    "    output_path, sep=\"\\t\", index=False, header=False\n",
    ")\n",
    "\n",
    "# 5. Calcul des statistiques\n",
    "unique_proteins = pd.unique(\n",
    "    unique_interactions_df[['protein1', 'protein2']].values.ravel('K')\n",
    ")\n",
    "num_interactions = len(unique_interactions_df)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\n Traitement (filtrage) terminé avec succès.\")\n",
    "print(f\"  - Nombre de protéines uniques : {len(unique_proteins)}\")\n",
    "print(f\"  - Nombre d'interactions uniques : {num_interactions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Base de donnees DIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats du traitement (Saccharomyces cerevisiae uniquement):\n",
      "- Protéines uniques: 5012\n",
      "- Interactions uniques: 22436\n",
      "Fichier sauvegardé: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\DIP_filtered_interactions.txt\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "def process_dip_interactions():\n",
    "    # Configuration des chemins\n",
    "    input_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\DIP_Interactions.mif25\")\n",
    "    output_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\DIP_filtered_interactions.txt\")\n",
    "    \n",
    "    NS = {'mif': 'http://psi.hupo.org/mi/mif'}\n",
    "    TARGET_TAXID = \"4932\"  # TaxID de Saccharomyces cerevisiae\n",
    "    \n",
    "    tree = ET.parse(input_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # 1. Extraction des protéines avec vérification de l'organisme\n",
    "    id_to_protein = {}\n",
    "    for interactor in root.findall(\".//mif:interactor\", NS):\n",
    "        interactor_id = interactor.get(\"id\")\n",
    "        if not interactor_id:\n",
    "            continue\n",
    "            \n",
    "        # Vérification que la protéine appartient à la levure\n",
    "        organism = interactor.find(\".//mif:organism\", NS)\n",
    "        if organism is None or organism.get(\"ncbiTaxId\") != TARGET_TAXID:\n",
    "            continue\n",
    "            \n",
    "        uniprot_ref = interactor.find(\".//mif:xref/mif:secondaryRef[@db='uniprot knowledge base']\", NS)\n",
    "        refseq_ref = interactor.find(\".//mif:xref/mif:secondaryRef[@db='refseq']\", NS)\n",
    "        \n",
    "        protein_id = None\n",
    "        if uniprot_ref is not None:\n",
    "            protein_id = uniprot_ref.get(\"id\")\n",
    "        elif refseq_ref is not None:\n",
    "            protein_id = refseq_ref.get(\"id\")\n",
    "        else:\n",
    "            short_label = interactor.find(\".//mif:names/mif:shortLabel\", NS)\n",
    "            protein_id = short_label.text if short_label is not None else None\n",
    "        \n",
    "        if protein_id:\n",
    "            id_to_protein[interactor_id] = protein_id\n",
    "\n",
    "    # 2. Extraction des interactions avec contrôle qualité\n",
    "    unique_interactions = set()\n",
    "    protein_set = set()  # Pour stocker les protéines uniques\n",
    "    \n",
    "    for interaction in root.findall(\".//mif:interaction\", NS):\n",
    "        participants = interaction.findall(\".//mif:participant/mif:interactorRef\", NS)\n",
    "        \n",
    "        if len(participants) != 2:\n",
    "            continue\n",
    "            \n",
    "        id1, id2 = participants[0].text, participants[1].text\n",
    "        \n",
    "        # Vérification que les deux protéines existent, sont différentes et appartiennent à la levure\n",
    "        if (id1 not in id_to_protein or \n",
    "            id2 not in id_to_protein or \n",
    "            id_to_protein[id1] == id_to_protein[id2]):\n",
    "            continue\n",
    "            \n",
    "        prot1, prot2 = id_to_protein[id1], id_to_protein[id2]\n",
    "        \n",
    "        # Ajout aux protéines uniques\n",
    "        protein_set.add(prot1)\n",
    "        protein_set.add(prot2)\n",
    "        \n",
    "        # Vérification du score\n",
    "        score_element = interaction.find(\".//mif:confidence/mif:value\", NS)\n",
    "        if score_element is not None:\n",
    "            try:\n",
    "                score = float(score_element.text)\n",
    "                if score <= 0.8:\n",
    "                    continue\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "                \n",
    "        # Ajout sous forme triée pour éviter les doublons A-B vs B-A\n",
    "        sorted_interaction = tuple(sorted((prot1, prot2)))\n",
    "        unique_interactions.add(sorted_interaction)\n",
    "\n",
    "    # 3. Sauvegarde\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"Protein1\\tProtein2\\n\")\n",
    "        for prot1, prot2 in unique_interactions:\n",
    "            f.write(f\"{prot1}\\t{prot2}\\n\")\n",
    "\n",
    "    # 4. Calcul et affichage des statistiques\n",
    "    num_unique_proteins = len(protein_set)\n",
    "    num_unique_interactions = len(unique_interactions)\n",
    "    \n",
    "    print(\"\\nRésultats du traitement (Saccharomyces cerevisiae uniquement):\")\n",
    "    print(f\"- Protéines uniques: {num_unique_proteins}\")\n",
    "    print(f\"- Interactions uniques: {num_unique_interactions}\")\n",
    "    print(f\"Fichier sauvegardé: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dip_interactions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Base de donnes BIOGRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats sélection:\n",
      "- Interactions totales (levure): 855577\n",
      "- Interactions haute confiance: 224490\n",
      "- Interactions sélectionnées: 40000\n",
      "- Protéines uniques: 5519\n",
      "\n",
      "Top 40k interactions sauvegardées dans: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_top_40000.tsv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def process_biogrid_high_confidence():\n",
    "    # Chemins des fichiers\n",
    "    input_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\BIOGRID-ORGANISM-Saccharomyces_cerevisiae.txt\")\n",
    "    output_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_top_40000.tsv\")\n",
    "    \n",
    "    # Charger les données\n",
    "    df = pd.read_csv(input_file, sep='\\t', comment='#', header=None)\n",
    "    df.columns = [\n",
    "        'ID_A', 'ID_B', 'Alt_IDs_A', 'Alt_IDs_B', \n",
    "        'Aliases_A', 'Aliases_B', 'Method', 'Author',\n",
    "        'PubIDs', 'TaxID_A', 'TaxID_B', 'IntType',\n",
    "        'SourceDB', 'IntIDs', 'Confidence'\n",
    "    ]\n",
    "    \n",
    "    # 1. Filtrer pour la levure S288c\n",
    "    yeast_df = df[(df['TaxID_A'] == 'taxid:559292') & \n",
    "                 (df['TaxID_B'] == 'taxid:559292')].copy()\n",
    "    \n",
    "    # 2. Critères de haute confiance\n",
    "    high_conf_methods = [\n",
    "        'affinity chromatography',\n",
    "        'two hybrid',\n",
    "        'pull down',\n",
    "        'coimmunoprecipitation'\n",
    "    ]\n",
    "    \n",
    "    high_conf_types = [\n",
    "        'direct interaction',\n",
    "        'physical association'\n",
    "    ]\n",
    "    \n",
    "    # 3. Filtrer avec critères combinés\n",
    "    method_mask = yeast_df['Method'].str.contains('|'.join(high_conf_methods), case=False, na=False)\n",
    "    type_mask = yeast_df['IntType'].str.contains('|'.join(high_conf_types), case=False, na=False)\n",
    "    \n",
    "    high_conf = yeast_df[method_mask & type_mask].copy()\n",
    "    \n",
    "    # 4. Extraire les UniProt IDs\n",
    "    def extract_uniprot(alt_ids):\n",
    "        if pd.isna(alt_ids): return None\n",
    "        match = re.search(r'uniprot/swiss[\\W-]?prot:([A-Z0-9]{6,10})', str(alt_ids))\n",
    "        return match.group(1) if match else None\n",
    "    \n",
    "    high_conf.loc[:, 'Protein1'] = high_conf['Alt_IDs_A'].apply(extract_uniprot)\n",
    "    high_conf.loc[:, 'Protein2'] = high_conf['Alt_IDs_B'].apply(extract_uniprot)\n",
    "    \n",
    "    # 5. Nettoyage final\n",
    "    clean_df = high_conf.dropna(subset=['Protein1', 'Protein2'])\n",
    "    clean_df = clean_df[clean_df['Protein1'] != clean_df['Protein2']]\n",
    "    \n",
    "    # 6. Sélection des 40 000 interactions les plus fréquentes\n",
    "    # Compter les occurrences de chaque interaction\n",
    "    interaction_counts = pd.concat([\n",
    "        clean_df.groupby('Protein1').size().rename('Count'),\n",
    "        clean_df.groupby('Protein2').size().rename('Count')\n",
    "    ], axis=1).fillna(0)\n",
    "    interaction_counts['Total'] = interaction_counts.sum(axis=1)\n",
    "    \n",
    "    # Ajouter les scores aux interactions\n",
    "    clean_df = clean_df.merge(\n",
    "        interaction_counts[['Total']].rename(columns={'Total': 'Score1'}),\n",
    "        left_on='Protein1', right_index=True\n",
    "    ).merge(\n",
    "        interaction_counts[['Total']].rename(columns={'Total': 'Score2'}),\n",
    "        left_on='Protein2', right_index=True\n",
    "    )\n",
    "    clean_df['InteractionScore'] = clean_df['Score1'] + clean_df['Score2']\n",
    "    \n",
    "    # Trier et sélectionner le top 40 000\n",
    "    top_interactions = clean_df.sort_values('InteractionScore', ascending=False)\\\n",
    "                             .drop_duplicates(subset=['Protein1', 'Protein2'])\\\n",
    "                             .head(40000)\n",
    "    \n",
    "    # 7. Statistiques\n",
    "    unique_proteins = pd.unique(top_interactions[['Protein1', 'Protein2']].values.ravel('K'))\n",
    "    \n",
    "    print(\"\\nRésultats sélection:\")\n",
    "    print(f\"- Interactions totales (levure): {len(yeast_df)}\")\n",
    "    print(f\"- Interactions haute confiance: {len(high_conf)}\")\n",
    "    print(f\"- Interactions sélectionnées: {len(top_interactions)}\")\n",
    "    print(f\"- Protéines uniques: {len(unique_proteins)}\")\n",
    "    \n",
    "    # 8. Sauvegarde\n",
    "    top_interactions[['Protein1', 'Protein2']].to_csv(\n",
    "        output_file, \n",
    "        sep='\\t', \n",
    "        index=False, \n",
    "        header=False\n",
    "    )\n",
    "    print(f\"\\nTop 40k interactions sauvegardées dans: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_biogrid_high_confidence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Base de donnees (Complex Portal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_levure.txt\n",
      "Nombre total de complexes trouvés: 643\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# Chemins\n",
    "zip_path = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Complexes\\yeast.zip\"\n",
    "extract_folder = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Complexes\\yeast_extracted\"\n",
    "output_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_levure.txt\"\n",
    "\n",
    "# 1. Nettoyage et extraction\n",
    "if os.path.exists(extract_folder):\n",
    "    shutil.rmtree(extract_folder)\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "# 2. Traitement spécifique pour votre format XML\n",
    "complexes = []\n",
    "yeast_folder = os.path.join(extract_folder, \"yeast\")\n",
    "\n",
    "for filename in os.listdir(yeast_folder):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        filepath = os.path.join(yeast_folder, filename)\n",
    "        \n",
    "        try:\n",
    "            tree = ET.parse(filepath)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # Namespace spécifique à vos fichiers\n",
    "            ns = {'mif': 'http://psi.hupo.org/mi/mif300'}\n",
    "            \n",
    "            # Recherche des interactions complexes\n",
    "            for interaction in root.findall(\".//mif:abstractInteraction\", ns):\n",
    "                proteins = set()\n",
    "                \n",
    "                # Recherche des participants\n",
    "                for participant in interaction.findall(\".//mif:participant\", ns):\n",
    "                    # Référence à l'interacteur\n",
    "                    interactor_ref = participant.find(\".//mif:interactorRef\", ns)\n",
    "                    if interactor_ref is not None:\n",
    "                        # Trouver l'interacteur correspondant\n",
    "                        interactor = root.find(f\".//mif:interactor[@id='{interactor_ref.text}']\", ns)\n",
    "                        if interactor is not None:\n",
    "                            # Vérifier si c'est une protéine\n",
    "                            interactor_type = interactor.find(\".//mif:interactorType/mif:names/mif:shortLabel\", ns)\n",
    "                            if interactor_type is not None and interactor_type.text == \"protein\":\n",
    "                                # Récupérer l'identifiant UniProt\n",
    "                                uniprot = interactor.find(\".//mif:xref/mif:primaryRef[@db='uniprotkb']\", ns)\n",
    "                                if uniprot is not None:\n",
    "                                    proteins.add(uniprot.get(\"id\"))\n",
    "                \n",
    "                if proteins:\n",
    "                    complexes.append(sorted(proteins))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {filename}: {str(e)[:200]}\")\n",
    "\n",
    "# 3. Écriture du fichier final\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    # Format: ID [tab] Liste_de_protéines (séparées par des espaces)\n",
    "    for idx, proteins in enumerate(complexes, 1):\n",
    "        f_out.write(f\"{idx}\\t{' '.join(proteins)}\\n\")\n",
    "\n",
    "# 4. Rapport\n",
    "print(f\"Fichier généré: {output_file}\")\n",
    "print(f\"Nombre total de complexes trouvés: {len(complexes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. filtrage de complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de complexes chargés: 643\n",
      "\n",
      "Traitement de C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_filtered_interactions.txt...\n",
      "→ 21 complexes trouvés dans ce réseau.\n",
      "Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_STRING.txt\n",
      "\n",
      "Traitement de C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\DIP_filtered_interactions.txt...\n",
      "→ 278 complexes trouvés dans ce réseau.\n",
      "Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_DIP.txt\n",
      "\n",
      "Traitement de C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_top_40000.tsv...\n",
      "→ 22 complexes trouvés dans ce réseau.\n",
      "Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_BIOGRID.txt\n",
      "\n",
      "Terminé avec succès !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Chemins des fichiers\n",
    "complexes_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_levure.txt\"\n",
    "\n",
    "reseau_files = [\n",
    "    r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_filtered_interactions.txt\",\n",
    "    r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\DIP_filtered_interactions.txt\",\n",
    "    r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_top_40000.tsv\"\n",
    "]\n",
    "output_files = [\n",
    "    r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_STRING.txt\",\n",
    "    r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_DIP.txt\",\n",
    "    r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_BIOGRID.txt\"\n",
    "]\n",
    "\n",
    "# 1. Charger les complexes depuis complexes_levure.txt\n",
    "def load_complexes(file_path):\n",
    "    complexes = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                proteins = set(parts[1].split())\n",
    "                complexes.append(proteins)\n",
    "    return complexes\n",
    "\n",
    "complexes = load_complexes(complexes_file)\n",
    "print(f\"Nombre total de complexes chargés: {len(complexes)}\")\n",
    "\n",
    "# 2. Charger un réseau PPI depuis un fichier (gère les TSV/CSV et ignore les en-têtes)\n",
    "def load_ppi_network(file_path):\n",
    "    network = defaultdict(set)\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\") or line.startswith(\"protein1\"):  # Ignorer lignes vides/en-têtes\n",
    "                continue\n",
    "            parts = line.split('\\t') if file_path.endswith('.tsv') else line.split()\n",
    "            if len(parts) >= 2:\n",
    "                prot1, prot2 = parts[0], parts[1]  # Prendre les 2 premières colonnes\n",
    "                network[prot1].add(prot2)\n",
    "                network[prot2].add(prot1)\n",
    "    return network\n",
    "\n",
    "# 3. Vérifier si un complexe est entièrement couvert par le réseau\n",
    "def is_complex_covered(complexe, network):\n",
    "    proteins = list(complexe)\n",
    "    for i in range(len(proteins)):\n",
    "        for j in range(i + 1, len(proteins)):\n",
    "            prot1, prot2 = proteins[i], proteins[j]\n",
    "            if prot2 not in network.get(prot1, set()):\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "# 4. Traiter chaque réseau PPI et sauvegarder les résultats\n",
    "for reseau_file, output_file in zip(reseau_files, output_files):\n",
    "    print(f\"\\nTraitement de {reseau_file}...\")\n",
    "    try:\n",
    "        network = load_ppi_network(reseau_file)\n",
    "        \n",
    "        # Trouver les complexes couverts\n",
    "        covered_complexes = []\n",
    "        for complexe in complexes:\n",
    "            if is_complex_covered(complexe, network):\n",
    "                covered_complexes.append(complexe)\n",
    "        \n",
    "        # Sauvegarder dans le fichier de sortie\n",
    "        with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "            for idx, complexe in enumerate(covered_complexes, 1):\n",
    "                f_out.write(f\"{idx}\\t{' '.join(complexe)}\\n\")\n",
    "        \n",
    "        print(f\"→ {len(covered_complexes)} complexes trouvés dans ce réseau.\")\n",
    "        print(f\"Fichier généré: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur avec {reseau_file}: {str(e)}\")\n",
    "\n",
    "print(\"\\nTerminé avec succès !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
