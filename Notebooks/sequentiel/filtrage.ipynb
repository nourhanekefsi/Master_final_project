{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Filtrage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Base de donnees STRING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Levure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STRING ----- levure\n",
      "Traitement terminé avec succès.\n",
      "Nombre de protéines uniques : 5782\n",
      "Nombre d'interactions uniques : 103986\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemins des fichiers\n",
    "input_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\STRING_Interactions.txt\")\n",
    "output_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_levure.temp\")\n",
    "\n",
    "# 1. Chargement et filtrage initial\n",
    "df = pd.read_csv(input_path, sep=\" \")\n",
    "\n",
    "# Créer un masque de filtrage\n",
    "mask = (df[\"combined_score\"] >= 700) & (\n",
    "    df[[\"experimental\", \"coexpression\", \"database\", \"textmining\"]].max(axis=1) > 90\n",
    ")\n",
    "\n",
    "# Appliquer le filtre et faire une copie explicite\n",
    "filtered_df = df.loc[mask].copy()\n",
    "\n",
    "# 2. Nettoyage des données\n",
    "# Supprimer \"4932.\" des noms de protéines\n",
    "for col in ['protein1', 'protein2']:\n",
    "    filtered_df.loc[:, col] = filtered_df[col].str.replace('4932.', '')\n",
    "\n",
    "# 3. Gestion des interactions uniques\n",
    "# Créer des identifiants d'interaction canoniques\n",
    "filtered_df.loc[:, 'interaction_key'] = filtered_df.apply(\n",
    "    lambda x: frozenset({x['protein1'], x['protein2']}), axis=1\n",
    ")\n",
    "\n",
    "# Supprimer les doublons en gardant la première occurrence\n",
    "unique_interactions_df = filtered_df.drop_duplicates(subset='interaction_key')\n",
    "\n",
    "# 4. Sauvegarde des résultats\n",
    "unique_interactions_df[['protein1', 'protein2']].to_csv(\n",
    "    output_path, sep=\"\\t\", index=False, header=False\n",
    ")\n",
    "\n",
    "# 5. Calcul des statistiques\n",
    "unique_proteins = pd.unique(\n",
    "    unique_interactions_df[['protein1', 'protein2']].values.ravel('K')\n",
    ")\n",
    "num_interactions = len(unique_interactions_df)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\n STRING ----- levure\")\n",
    "print(\"Traitement terminé avec succès.\")\n",
    "print(f\"Nombre de protéines uniques : {len(unique_proteins)}\")\n",
    "print(f\"Nombre d'interactions uniques : {num_interactions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1.1. normalisation de STRING levure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traitement terminé avec succès.\n",
      "Nombre total d'interactions conservées : 96810\n",
      "Nombre total de protéines uniques : 5707\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Chemins des fichiers\n",
    "interactions_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_levure.temp\"\n",
    "mapping_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\YEAST_559292_idmapping.dat\"\n",
    "output_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_levure.txt\"\n",
    "\n",
    "def create_mapping_dict(mapping_file):\n",
    "    \"\"\"Crée un dictionnaire de mapping STRING -> UniProt et inversement\"\"\"\n",
    "    str_to_uniprot = defaultdict(list)\n",
    "    uniprot_to_str = defaultdict(list)\n",
    "    \n",
    "    with open(mapping_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 3:\n",
    "                uniprot_id = parts[0]\n",
    "                db_type = parts[1]\n",
    "                external_id = parts[2]\n",
    "\n",
    "                if db_type == \"STRING\":\n",
    "                    # Supprimer le préfixe '4932.' si présent\n",
    "                    if external_id.startswith(\"4932.\"):\n",
    "                        external_id = external_id.replace(\"4932.\", \"\")\n",
    "                    str_to_uniprot[external_id].append(uniprot_id)\n",
    "                    uniprot_to_str[uniprot_id].append(external_id)\n",
    "    \n",
    "    return str_to_uniprot\n",
    "\n",
    "def process_interactions(interactions_file, str_to_uniprot):\n",
    "    \"\"\"Traite le fichier d'interactions\"\"\"\n",
    "    interactions = []\n",
    "    missing_mappings = set()\n",
    "    \n",
    "    with open(interactions_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                prot1, prot2 = parts[0], parts[1]\n",
    "                \n",
    "                # Vérifier si les protéines existent dans le mapping\n",
    "                mapped1 = str_to_uniprot.get(prot1, [None])[0]\n",
    "                mapped2 = str_to_uniprot.get(prot2, [None])[0]\n",
    "                \n",
    "                if mapped1 and mapped2:\n",
    "                    interactions.append((mapped1, mapped2))\n",
    "                else:\n",
    "                    if not mapped1:\n",
    "                        missing_mappings.add(prot1)\n",
    "                    if not mapped2:\n",
    "                        missing_mappings.add(prot2)\n",
    "    \n",
    "    return interactions\n",
    "\n",
    "def main():\n",
    "    # 1. Créer le dictionnaire de mapping\n",
    "    str_to_uniprot = create_mapping_dict(mapping_file)\n",
    "    \n",
    "    # 2. Traiter les interactions\n",
    "    filtered_interactions = process_interactions(interactions_file, str_to_uniprot)\n",
    "    \n",
    "    # 3. Sauvegarder les résultats\n",
    "    with open(output_file, 'w') as f:\n",
    "        for prot1, prot2 in filtered_interactions:\n",
    "            f.write(f\"{prot1}\\t{prot2}\\n\")\n",
    "\n",
    "    # 4. Calcul des statistiques\n",
    "    unique_proteins = set()\n",
    "    for p1, p2 in filtered_interactions:\n",
    "        unique_proteins.update([p1, p2])\n",
    "\n",
    "    # 5. Affichage simplifié\n",
    "    print(\"\\nTraitement terminé avec succès.\")\n",
    "    print(f\"Nombre total d'interactions conservées : {len(filtered_interactions)}\")\n",
    "    print(f\"Nombre total de protéines uniques : {len(unique_proteins)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. humain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STRING ----- humain\n",
      "Traitement terminé avec succès.\n",
      "Nombre de protéines uniques : 18000\n",
      "Nombre d'interactions uniques : 359863\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemins des fichiers\n",
    "input_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\9606.protein.links.detailed.v12.0.txt\")\n",
    "output_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_humain_filtered_interactions.txt\")\n",
    "\n",
    "# 1. Chargement et filtrage initial\n",
    "df = pd.read_csv(input_path, sep=\" \")\n",
    "\n",
    "# Créer un masque de filtrage\n",
    "mask = (df[\"combined_score\"] >= 600) & (\n",
    "    df[[\"experimental\", \"coexpression\", \"database\", \"textmining\"]].max(axis=1) > 90\n",
    ")\n",
    "\n",
    "# Appliquer le filtre et faire une copie explicite\n",
    "filtered_df = df.loc[mask].copy()\n",
    "\n",
    "# 2. Nettoyage des données\n",
    "# Supprimer \"4932.\" des noms de protéines\n",
    "for col in ['protein1', 'protein2']:\n",
    "    filtered_df.loc[:, col] = filtered_df[col].str.replace('9606.', '')\n",
    "\n",
    "# 3. Gestion des interactions uniques\n",
    "# Créer des identifiants d'interaction canoniques\n",
    "filtered_df.loc[:, 'interaction_key'] = filtered_df.apply(\n",
    "    lambda x: frozenset({x['protein1'], x['protein2']}), axis=1\n",
    ")\n",
    "\n",
    "# Supprimer les doublons en gardant la première occurrence\n",
    "unique_interactions_df = filtered_df.drop_duplicates(subset='interaction_key')\n",
    "\n",
    "# 4. Sauvegarde des résultats\n",
    "unique_interactions_df[['protein1', 'protein2']].to_csv(\n",
    "    output_path, sep=\"\\t\", index=False, header=False\n",
    ")\n",
    "\n",
    "# 5. Calcul des statistiques\n",
    "unique_proteins = pd.unique(\n",
    "    unique_interactions_df[['protein1', 'protein2']].values.ravel('K')\n",
    ")\n",
    "num_interactions = len(unique_interactions_df)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\n STRING ----- humain\")\n",
    "print(\"Traitement terminé avec succès.\")\n",
    "print(f\"Nombre de protéines uniques : {len(unique_proteins)}\")\n",
    "print(f\"Nombre d'interactions uniques : {num_interactions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3. Normalisations des interactions STRING (humain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing mapping file...\n",
      "Found 19110 ENSP to Uniprot mappings\n",
      "Converting interactions...\n",
      "Converted interactions written to C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_humain.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_mapping_file(mapping_file):\n",
    "    \"\"\"\n",
    "    Parse the mapping file to create a dictionary from ENSP to Uniprot IDs.\n",
    "    The mapping file is assumed to have lines with format like:\n",
    "    P31946 STRING 9606.ENSP00000361930\n",
    "    \"\"\"\n",
    "    ensembl_to_uniprot = defaultdict(list)\n",
    "    \n",
    "    with open(mapping_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            uniprot_id, db, db_id = parts[:3]\n",
    "            if db == \"STRING\":\n",
    "                # Extract ENSP ID from STRING format (e.g., 9606.ENSP00000361930)\n",
    "                if '.' in db_id:\n",
    "                    ensembl_id = db_id.split('.')[1]\n",
    "                    ensembl_to_uniprot[ensembl_id].append(uniprot_id)\n",
    "    \n",
    "    # Remove duplicates and keep only the first Uniprot ID for each ENSP\n",
    "    return {ensp: uniprots[0] for ensp, uniprots in ensembl_to_uniprot.items() if uniprots}\n",
    "\n",
    "def convert_interactions(input_file, output_file, mapping_dict):\n",
    "    \"\"\"\n",
    "    Convert interaction file from ENSP IDs to Uniprot IDs using the mapping dictionary.\n",
    "    Only writes interactions where both proteins have corresponding Uniprot IDs.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            prot1, prot2 = parts\n",
    "            if prot1 in mapping_dict and prot2 in mapping_dict:\n",
    "                uniprot1 = mapping_dict[prot1]\n",
    "                uniprot2 = mapping_dict[prot2]\n",
    "                outfile.write(f\"{uniprot1}\\t{uniprot2}\\n\")\n",
    "\n",
    "def main():\n",
    "    interactions_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_humain_filtered_interactions.txt\"\n",
    "    mapping_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\HUMAN_9606_idmapping.dat\"\n",
    "    output_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_humain.txt\"\n",
    "    \n",
    "    print(\"Parsing mapping file...\")\n",
    "    mapping_dict = parse_mapping_file(mapping_file)\n",
    "    print(f\"Found {len(mapping_dict)} ENSP to Uniprot mappings\")\n",
    "    \n",
    "    print(\"Converting interactions...\")\n",
    "    convert_interactions(interactions_file, output_file, mapping_dict)\n",
    "    print(f\"Converted interactions written to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Base de donnees DIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats du traitement:\n",
      "- Protéines uniques: 5144\n",
      "- Interactions uniques: 22614\n",
      "Fichier sauvegardé: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\DIP_levure.txt\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "def process_dip_interactions():\n",
    "    # Configuration des chemins\n",
    "    input_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\DIP_Interactions.mif25\")\n",
    "    output_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\DIP_levure.txt\")\n",
    "    \n",
    "    NS = {'mif': 'http://psi.hupo.org/mi/mif'}\n",
    "    tree = ET.parse(input_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # 1. Extraction des protéines avec vérification\n",
    "    id_to_protein = {}\n",
    "    for interactor in root.findall(\".//mif:interactor\", NS):\n",
    "        interactor_id = interactor.get(\"id\")\n",
    "        if not interactor_id:\n",
    "            continue\n",
    "            \n",
    "        uniprot_ref = interactor.find(\".//mif:xref/mif:secondaryRef[@db='uniprot knowledge base']\", NS)\n",
    "        refseq_ref = interactor.find(\".//mif:xref/mif:secondaryRef[@db='refseq']\", NS)\n",
    "        \n",
    "        protein_id = None\n",
    "        if uniprot_ref is not None:\n",
    "            protein_id = uniprot_ref.get(\"id\")\n",
    "        elif refseq_ref is not None:\n",
    "            protein_id = refseq_ref.get(\"id\")\n",
    "        else:\n",
    "            short_label = interactor.find(\".//mif:names/mif:shortLabel\", NS)\n",
    "            protein_id = short_label.text if short_label is not None else None\n",
    "        \n",
    "        if protein_id:\n",
    "            id_to_protein[interactor_id] = protein_id\n",
    "\n",
    "    # 2. Extraction des interactions avec contrôle qualité\n",
    "    unique_interactions = set()\n",
    "    protein_set = set()  # Pour stocker les protéines uniques\n",
    "    \n",
    "    for interaction in root.findall(\".//mif:interaction\", NS):\n",
    "        participants = interaction.findall(\".//mif:participant/mif:interactorRef\", NS)\n",
    "        \n",
    "        if len(participants) != 2:\n",
    "            continue\n",
    "            \n",
    "        id1, id2 = participants[0].text, participants[1].text\n",
    "        \n",
    "        # Vérification que les deux protéines existent et sont différentes\n",
    "        if (id1 not in id_to_protein or \n",
    "            id2 not in id_to_protein or \n",
    "            id_to_protein[id1] == id_to_protein[id2]):\n",
    "            continue\n",
    "            \n",
    "        prot1, prot2 = id_to_protein[id1], id_to_protein[id2]\n",
    "        \n",
    "        # Ajout aux protéines uniques\n",
    "        protein_set.add(prot1)\n",
    "        protein_set.add(prot2)\n",
    "        \n",
    "        # Vérification du score\n",
    "        score_element = interaction.find(\".//mif:confidence/mif:value\", NS)\n",
    "        if score_element is not None:\n",
    "            try:\n",
    "                score = float(score_element.text)\n",
    "                if score <= 0.8:\n",
    "                    continue\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "                \n",
    "        # Ajout sous forme triée pour éviter les doublons A-B vs B-A\n",
    "        sorted_interaction = tuple(sorted((prot1, prot2)))\n",
    "        unique_interactions.add(sorted_interaction)\n",
    "\n",
    "    # 3. Sauvegarde\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"Protein1\\tProtein2\\n\")\n",
    "        for prot1, prot2 in unique_interactions:\n",
    "            f.write(f\"{prot1}\\t{prot2}\\n\")\n",
    "\n",
    "    # 4. Calcul et affichage des statistiques\n",
    "    num_unique_proteins = len(protein_set)\n",
    "    num_unique_interactions = len(unique_interactions)\n",
    "    \n",
    "    print(\"\\nRésultats du traitement:\")\n",
    "    print(f\"- Protéines uniques: {num_unique_proteins}\")\n",
    "    print(f\"- Interactions uniques: {num_unique_interactions}\")\n",
    "    print(f\"Fichier sauvegardé: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dip_interactions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Base de donnes BIOGRID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Levure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats finaux (haute confiance):\n",
      "- Interactions levures totales : 855,577\n",
      "- Interactions avec UniProt valides : 848,870\n",
      "- Interactions non-redondantes : 604,862\n",
      "- Interactions sélectionnées : 300,000\n",
      "- Protéines uniques : 5,805\n",
      "- Score minimum retenu : 6.0\n",
      "- Score maximum : 6.0\n",
      "- Score moyen : 6.0\n",
      "\n",
      "Données sauvegardées dans : C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_levure.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_uniprot(alt_ids):\n",
    "    \"\"\"Extract UniProt IDs from alternative IDs field\"\"\"\n",
    "    if pd.isna(alt_ids): \n",
    "        return None\n",
    "    alt_ids = str(alt_ids)\n",
    "    patterns = [\n",
    "        r'uniprot/swiss[\\W-]?prot:([A-Z0-9]{6,10})',\n",
    "        r'uniprot:([A-Z0-9]{6,10})',\n",
    "        r'swiss[\\W-]?prot:([A-Z0-9]{6,10})',\n",
    "        r'([A-Z0-9]{6,10})\\.\\d'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, alt_ids, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    return None\n",
    "\n",
    "def process_biogrid_high_confidence():\n",
    "    \"\"\"Process BioGRID data to get ~50,000 high-confidence interactions\"\"\"\n",
    "    # File paths\n",
    "    input_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\BIOGRID-ORGANISM-Saccharomyces_cerevisiae.txt\")\n",
    "    output_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_levure.txt\")\n",
    "    \n",
    "    # Load data\n",
    "    try:\n",
    "        df = pd.read_csv(input_file, sep='\\t', comment='#', header=None, dtype=str)\n",
    "        df.columns = [\n",
    "            'ID_A', 'ID_B', 'Alt_IDs_A', 'Alt_IDs_B', \n",
    "            'Aliases_A', 'Aliases_B', 'Method', 'Author',\n",
    "            'PubIDs', 'TaxID_A', 'TaxID_B', 'IntType',\n",
    "            'SourceDB', 'IntIDs', 'Confidence'\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 1. Filter for Saccharomyces cerevisiae\n",
    "    yeast_df = df[(df['TaxID_A'] == 'taxid:559292') & (df['TaxID_B'] == 'taxid:559292')].copy()\n",
    "    \n",
    "    # 2. Extract UniProt IDs\n",
    "    yeast_df['Protein1'] = yeast_df['Alt_IDs_A'].apply(extract_uniprot)\n",
    "    yeast_df['Protein2'] = yeast_df['Alt_IDs_B'].apply(extract_uniprot)\n",
    "    \n",
    "    # 3. Clean data\n",
    "    clean_df = yeast_df.dropna(subset=['Protein1', 'Protein2'])\n",
    "    clean_df = clean_df[clean_df['Protein1'] != clean_df['Protein2']].copy()\n",
    "    \n",
    "    # 4. High-confidence scoring system (less strict than ultra-strict version)\n",
    "    method_scores = {\n",
    "        'x-ray crystallography': 10,\n",
    "        'electron microscopy': 8,\n",
    "        'affinity chromatography': 6,\n",
    "        'coimmunoprecipitation': 4,\n",
    "        'two hybrid': 2,\n",
    "        'pull down': 3,\n",
    "        'mass spectrometry': 2\n",
    "    }\n",
    "    \n",
    "    type_scores = {\n",
    "        'direct interaction': 6,\n",
    "        'physical association': 3,\n",
    "        'complex': 4\n",
    "    }\n",
    "    \n",
    "    clean_df['Pub_Count'] = clean_df['PubIDs'].str.count(r'\\|').add(1).fillna(1)\n",
    "    \n",
    "    clean_df['Method_Score'] = clean_df['Method'].apply(\n",
    "        lambda x: max([method_scores.get(method.lower(), 1)\n",
    "                      for method in str(x).split('|') \n",
    "                      if 'psi-mi' not in method.lower()], default=1)\n",
    "    )\n",
    "    \n",
    "    clean_df['Type_Score'] = clean_df['IntType'].apply(\n",
    "        lambda x: max([type_scores.get(typ.lower(), 1)\n",
    "                      for typ in str(x).split('|') \n",
    "                      if 'psi-mi' not in typ.lower()], default=1)\n",
    "    )\n",
    "    \n",
    "    clean_df['Total_Score'] = (\n",
    "        clean_df['Method_Score'] * 3 +\n",
    "        clean_df['Type_Score'] * 2 +\n",
    "        clean_df['Pub_Count']\n",
    "    )\n",
    "    \n",
    "    # 5. Remove duplicates keeping highest scores\n",
    "    clean_df['SortedPair'] = clean_df.apply(\n",
    "        lambda x: tuple(sorted([x['Protein1'], x['Protein2']])), axis=1\n",
    "    )\n",
    "    \n",
    "    non_redundant = clean_df.sort_values('Total_Score', ascending=False).drop_duplicates(subset=['SortedPair'])\n",
    "    \n",
    "    # 6. Dynamic threshold to get ~50,000 interactions\n",
    "    target_count = 300000\n",
    "    if len(non_redundant) > target_count:\n",
    "        # Find score threshold that gives us ~50,000 interactions\n",
    "        thresholds = sorted(non_redundant['Total_Score'].unique(), reverse=True)\n",
    "        for threshold in thresholds:\n",
    "            filtered = non_redundant[non_redundant['Total_Score'] >= threshold]\n",
    "            if len(filtered) <= target_count:\n",
    "                break\n",
    "        \n",
    "        # If we're still too far from target, take top N\n",
    "        if len(filtered) < target_count * 0.8 or len(filtered) > target_count * 1.2:\n",
    "            filtered = non_redundant.head(target_count)\n",
    "    else:\n",
    "        filtered = non_redundant\n",
    "        print(f\"Warning: Only {len(non_redundant)} interactions available\")\n",
    "    \n",
    "    # 7. Statistics\n",
    "    unique_proteins = pd.unique(filtered[['Protein1', 'Protein2']].values.ravel('K'))\n",
    "    \n",
    "    print(f\"\\nRésultats finaux (haute confiance):\")\n",
    "    print(f\"- Interactions levures totales : {len(yeast_df):,}\")\n",
    "    print(f\"- Interactions avec UniProt valides : {len(clean_df):,}\")\n",
    "    print(f\"- Interactions non-redondantes : {len(non_redundant):,}\")\n",
    "    print(f\"- Interactions sélectionnées : {len(filtered):,}\")\n",
    "    print(f\"- Protéines uniques : {len(unique_proteins):,}\")\n",
    "    print(f\"- Score minimum retenu : {filtered['Total_Score'].min():.1f}\")\n",
    "    print(f\"- Score maximum : {filtered['Total_Score'].max():.1f}\")\n",
    "    print(f\"- Score moyen : {filtered['Total_Score'].mean():.1f}\")\n",
    "    \n",
    "    # 8. Save\n",
    "    try:\n",
    "        filtered[['Protein1', 'Protein2']].to_csv(\n",
    "            output_file, \n",
    "            sep='\\t', \n",
    "            index=False, \n",
    "            header=False\n",
    "        )\n",
    "        print(f\"\\nDonnées sauvegardées dans : {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sauvegarde : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_biogrid_high_confidence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. humain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats finaux :\n",
      "- Interactions humaines totales : 316235\n",
      "- Interactions avec UniProt valides : 309156\n",
      "- Interactions non-redondantes : 88647\n",
      "- Protéines uniques : 11266\n",
      "- Score moyen (pour information) : 6.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def process_biogrid_max_coverage():\n",
    "    # Chemins des fichiers\n",
    "    input_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\BIOGRID-MV-Physical.txt\")\n",
    "    output_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_humain.txt\")\n",
    "    \n",
    "    # Charger les données en forçant le type string\n",
    "    df = pd.read_csv(input_file, sep='\\t', comment='#', header=None, dtype=str)\n",
    "    df.columns = [\n",
    "        'ID_A', 'ID_B', 'Alt_IDs_A', 'Alt_IDs_B', \n",
    "        'Aliases_A', 'Aliases_B', 'Method', 'Author',\n",
    "        'PubIDs', 'TaxID_A', 'TaxID_B', 'IntType',\n",
    "        'SourceDB', 'IntIDs', 'Confidence'\n",
    "    ]\n",
    "    \n",
    "    # 1. Filtrer pour Homo sapiens uniquement\n",
    "    human_df = df[(df['TaxID_A'] == 'taxid:9606') & (df['TaxID_B'] == 'taxid:9606')].copy()\n",
    "    \n",
    "    # 2. Extraction robuste des UniProt IDs\n",
    "    def extract_uniprot(alt_ids):\n",
    "        if pd.isna(alt_ids): \n",
    "            return None\n",
    "        patterns = [\n",
    "            r'uniprot/swiss[\\W-]?prot:([A-Z0-9]{6,8})',\n",
    "            r'uniprot:([A-Z0-9]{6,8})',\n",
    "            r'([A-Z0-9]{6,8})\\.\\d'\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, str(alt_ids), re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    human_df['Protein1'] = human_df['Alt_IDs_A'].apply(extract_uniprot)\n",
    "    human_df['Protein2'] = human_df['Alt_IDs_B'].apply(extract_uniprot)\n",
    "    \n",
    "    # 3. Nettoyage de base\n",
    "    clean_df = human_df.dropna(subset=['Protein1', 'Protein2'])\n",
    "    clean_df = clean_df[clean_df['Protein1'] != clean_df['Protein2']].copy()\n",
    "    \n",
    "    # 4. Système de scoring des interactions (conservé pour information)\n",
    "    method_scores = {\n",
    "        'x-ray crystallography': 4,\n",
    "        'electron microscopy': 3,\n",
    "        'affinity chromatography': 3,\n",
    "        'coimmunoprecipitation': 2,\n",
    "        'two hybrid': 2,\n",
    "        'pull down': 2,\n",
    "        'mass spectrometry': 1\n",
    "    }\n",
    "    \n",
    "    type_scores = {\n",
    "        'direct interaction': 3,\n",
    "        'physical association': 1,\n",
    "        'complex': 2\n",
    "    }\n",
    "    \n",
    "    clean_df['Pub_Count'] = clean_df['PubIDs'].str.count(r'\\|') + 1\n",
    "    \n",
    "    clean_df['Method_Score'] = clean_df['Method'].map(\n",
    "        lambda x: max([method_scores.get(method.lower(), 1) \n",
    "                      for method in str(x).split('|')])\n",
    "    )\n",
    "    clean_df['Type_Score'] = clean_df['IntType'].map(\n",
    "        lambda x: max([type_scores.get(typ.lower(), 1) \n",
    "                      for typ in str(x).split('|')])\n",
    "    )\n",
    "    clean_df['Total_Score'] = (\n",
    "        clean_df['Method_Score'] * 3 + \n",
    "        clean_df['Type_Score'] * 2 + \n",
    "        clean_df['Pub_Count']\n",
    "    )\n",
    "    \n",
    "    # 5. Supprimer les doublons (même paire de protéines dans un ordre différent)\n",
    "    # Créer une colonne avec la paire triée pour identifier les doublons\n",
    "    clean_df['SortedPair'] = clean_df.apply(lambda x: tuple(sorted([x['Protein1'], x['Protein2']])), axis=1)\n",
    "    \n",
    "    # Garder toutes les interactions uniques (sans filtrage par score)\n",
    "    non_redundant = clean_df.drop_duplicates(subset=['SortedPair'])\n",
    "    \n",
    "    # 6. Statistiques finales\n",
    "    unique_proteins = pd.unique(\n",
    "        non_redundant[['Protein1', 'Protein2']].values.ravel('K')\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRésultats finaux :\")\n",
    "    print(f\"- Interactions humaines totales : {len(human_df)}\")\n",
    "    print(f\"- Interactions avec UniProt valides : {len(clean_df)}\")\n",
    "    print(f\"- Interactions non-redondantes : {len(non_redundant)}\")\n",
    "    print(f\"- Protéines uniques : {len(unique_proteins)}\")\n",
    "    print(f\"- Score moyen (pour information) : {non_redundant['Total_Score'].mean():.1f}\")\n",
    "    \n",
    "    # 7. Sauvegarde (seulement les deux colonnes Protein1 et Protein2)\n",
    "    non_redundant[['Protein1', 'Protein2']].to_csv(\n",
    "        output_file, \n",
    "        sep='\\t', \n",
    "        index=False, \n",
    "        header=False  # Pas d'en-tête dans le fichier de sortie\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_biogrid_max_coverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Base de donnees (Complex Portal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\Portal_complexes_levure.txt\n",
      "Nombre total de complexes trouvés: 643\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# Chemins\n",
    "zip_path = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Complexes\\yeast.zip\"\n",
    "extract_folder = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Complexes\\yeast_extracted\"\n",
    "output_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\Portal_complexes_levure.txt\"\n",
    "\n",
    "# 1. Nettoyage et extraction\n",
    "if os.path.exists(extract_folder):\n",
    "    shutil.rmtree(extract_folder)\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "# 2. Traitement spécifique pour votre format XML\n",
    "complexes = []\n",
    "yeast_folder = os.path.join(extract_folder, \"yeast\")\n",
    "\n",
    "for filename in os.listdir(yeast_folder):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        filepath = os.path.join(yeast_folder, filename)\n",
    "        \n",
    "        try:\n",
    "            tree = ET.parse(filepath)\n",
    "            root = tree.getroot()\n",
    "            \n",
    "            # Namespace spécifique à vos fichiers\n",
    "            ns = {'mif': 'http://psi.hupo.org/mi/mif300'}\n",
    "            \n",
    "            # Recherche des interactions complexes\n",
    "            for interaction in root.findall(\".//mif:abstractInteraction\", ns):\n",
    "                proteins = set()\n",
    "                \n",
    "                # Recherche des participants\n",
    "                for participant in interaction.findall(\".//mif:participant\", ns):\n",
    "                    # Référence à l'interacteur\n",
    "                    interactor_ref = participant.find(\".//mif:interactorRef\", ns)\n",
    "                    if interactor_ref is not None:\n",
    "                        # Trouver l'interacteur correspondant\n",
    "                        interactor = root.find(f\".//mif:interactor[@id='{interactor_ref.text}']\", ns)\n",
    "                        if interactor is not None:\n",
    "                            # Vérifier si c'est une protéine\n",
    "                            interactor_type = interactor.find(\".//mif:interactorType/mif:names/mif:shortLabel\", ns)\n",
    "                            if interactor_type is not None and interactor_type.text == \"protein\":\n",
    "                                # Récupérer l'identifiant UniProt\n",
    "                                uniprot = interactor.find(\".//mif:xref/mif:primaryRef[@db='uniprotkb']\", ns)\n",
    "                                if uniprot is not None:\n",
    "                                    proteins.add(uniprot.get(\"id\"))\n",
    "                \n",
    "                if proteins:\n",
    "                    complexes.append(sorted(proteins))\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {filename}: {str(e)[:200]}\")\n",
    "\n",
    "# 3. Écriture du fichier final\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    # Format: ID [tab] Liste_de_protéines (séparées par des espaces)\n",
    "    for idx, proteins in enumerate(complexes, 1):\n",
    "        f_out.write(f\"{idx}\\t{' '.join(proteins)}\\n\")\n",
    "\n",
    "# 4. Rapport\n",
    "print(f\"Fichier généré: {output_file}\")\n",
    "print(f\"Nombre total de complexes trouvés: {len(complexes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. filtrage de complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de complexes chargés: 643\n",
      "\n",
      "Traitement de weighted_STRING_levure.txt...\n",
      "- Protéines uniques dans PPI: 4,150\n",
      "- Interactions dans PPI: 41,213\n",
      "- Complexes analysés: 643\n",
      "- Complexes conservés: 512 (79.6%)\n",
      "  - Rejetés (protéines manquantes): 122\n",
      "  - Rejetés (non connectés): 9\n",
      "- Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_STRING_levure.txt\n",
      "\n",
      "Traitement de weighted_DIP_levure.txt...\n",
      "- Protéines uniques dans PPI: 3,973\n",
      "- Interactions dans PPI: 15,376\n",
      "- Complexes analysés: 643\n",
      "- Complexes conservés: 358 (55.7%)\n",
      "  - Rejetés (protéines manquantes): 188\n",
      "  - Rejetés (non connectés): 97\n",
      "- Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_DIP_levure.txt\n",
      "\n",
      "Traitement de weighted_BIOGRID_levure.txt...\n",
      "- Protéines uniques dans PPI: 5,241\n",
      "- Interactions dans PPI: 200,897\n",
      "- Complexes analysés: 643\n",
      "- Complexes conservés: 511 (79.5%)\n",
      "  - Rejetés (protéines manquantes): 110\n",
      "  - Rejetés (non connectés): 22\n",
      "- Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_BIOGRID_levure.txt\n",
      "\n",
      "Terminé avec succès !\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "\n",
    "def load_complexes(file_path):\n",
    "    \"\"\"Charge les complexes depuis le fichier et retourne une liste de tuples (id_complexe, set de protéines)\"\"\"\n",
    "    complexes = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                complex_id = parts[0]\n",
    "                proteins = set(p.strip() for p in parts[1].split() if p.strip())\n",
    "                complexes.append((complex_id, proteins))\n",
    "    return complexes\n",
    "\n",
    "def load_ppi_network(file_path):\n",
    "    \"\"\"Charge le réseau PPI et retourne un set de protéines et le graphe PPI\"\"\"\n",
    "    proteins = set()\n",
    "    graph = defaultdict(set)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\") or line.startswith(\"protein1\"):\n",
    "                continue\n",
    "            parts = line.split('\\t') if '\\t' in line else line.split()\n",
    "            if len(parts) >= 2:\n",
    "                p1, p2 = parts[0].strip(), parts[1].strip()\n",
    "                if p1 and p2:\n",
    "                    proteins.add(p1)\n",
    "                    proteins.add(p2)\n",
    "                    graph[p1].add(p2)\n",
    "                    graph[p2].add(p1)\n",
    "    return proteins, graph\n",
    "\n",
    "def is_single_connected_component(proteins, ppi_graph):\n",
    "    \"\"\"Vérifie si les protéines forment un seul composant connecté dans le réseau PPI\"\"\"\n",
    "    if not proteins:\n",
    "        return False\n",
    "    \n",
    "    visited = set()\n",
    "    queue = deque()\n",
    "    \n",
    "    start_protein = next(iter(proteins))\n",
    "    queue.append(start_protein)\n",
    "    visited.add(start_protein)\n",
    "    \n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        for neighbor in ppi_graph[current]:\n",
    "            if neighbor in proteins and neighbor not in visited:\n",
    "                visited.add(neighbor)\n",
    "                queue.append(neighbor)\n",
    "    \n",
    "    return visited == proteins\n",
    "\n",
    "def filter_complexes(complexes, ppi_proteins, ppi_graph, output_file):\n",
    "    \"\"\"Filtre les complexes et sauvegarde ceux valides\"\"\"\n",
    "    stats = {\n",
    "        'total': 0,\n",
    "        'kept': 0,\n",
    "        'missing_proteins': 0,\n",
    "        'disconnected': 0\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for complex_id, proteins in complexes:\n",
    "            stats['total'] += 1\n",
    "            \n",
    "            # Vérifier que toutes les protéines sont dans le PPI\n",
    "            if not all(p in ppi_proteins for p in proteins):\n",
    "                stats['missing_proteins'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Vérifier la connectivité\n",
    "            if not is_single_connected_component(proteins, ppi_graph):\n",
    "                stats['disconnected'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Écrire le complexe valide\n",
    "            f_out.write(f\"{complex_id}\\t{' '.join(proteins)}\\n\")\n",
    "            stats['kept'] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def main():\n",
    "    # Configuration des chemins\n",
    "    base_dir = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\")\n",
    "    \n",
    "    # Fichiers d'entrée/sortie\n",
    "    complexes_file = base_dir / \"complexes\" / \"complexes_levure.txt\"\n",
    "    reseau_files = [\n",
    "        base_dir / \"weighted_networks\" / \"weighted_STRING_levure.txt\",\n",
    "        base_dir / \"weighted_networks\" / \"weighted_DIP_levure.txt\",\n",
    "        base_dir / \"weighted_networks\" / \"weighted_BIOGRID_levure.txt\"\n",
    "    ]\n",
    "    output_files = [\n",
    "        base_dir / \"complexes\" / \"complexes_STRING_levure.txt\",\n",
    "        base_dir / \"complexes\" / \"complexes_DIP_levure.txt\",\n",
    "        base_dir / \"complexes\" / \"complexes_BIOGRID_levure.txt\"\n",
    "    ]\n",
    "\n",
    "    # Charger les complexes\n",
    "    complexes = load_complexes(complexes_file)\n",
    "    print(f\"Nombre total de complexes chargés: {len(complexes)}\")\n",
    "\n",
    "    # Traiter chaque réseau PPI\n",
    "    for ppi_file, out_file in zip(reseau_files, output_files):\n",
    "        print(f\"\\nTraitement de {ppi_file.name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Charger le réseau PPI\n",
    "            ppi_proteins, ppi_graph = load_ppi_network(ppi_file)\n",
    "            print(f\"- Protéines uniques dans PPI: {len(ppi_proteins):,}\")\n",
    "            print(f\"- Interactions dans PPI: {sum(len(v) for v in ppi_graph.values())//2:,}\")\n",
    "\n",
    "            # Filtrer les complexes\n",
    "            stats = filter_complexes(complexes, ppi_proteins, ppi_graph, out_file)\n",
    "            \n",
    "            # Afficher les statistiques\n",
    "            print(f\"- Complexes analysés: {stats['total']:,}\")\n",
    "            print(f\"- Complexes conservés: {stats['kept']:,} ({stats['kept']/stats['total']*100:.1f}%)\")\n",
    "            print(f\"  - Rejetés (protéines manquantes): {stats['missing_proteins']:,}\")\n",
    "            print(f\"  - Rejetés (non connectés): {stats['disconnected']:,}\")\n",
    "            print(f\"- Fichier généré: {out_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {ppi_file}: {str(e)}\")\n",
    "\n",
    "    print(\"\\nTerminé avec succès !\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.1. Corum ( filtrage de complexes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier de sortie a été généré : C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\CORUM_complexes_humain.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def process_complexes(input_file, output_file):\n",
    "    # Dictionnaire pour stocker les complexes (id -> set de protéines)\n",
    "    complexes = {}\n",
    "    \n",
    "    # Lire le fichier d'entrée\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        \n",
    "        for row in reader:\n",
    "            complex_id = row['complex_id']\n",
    "            proteins = row['subunits_uniprot_id']\n",
    "            \n",
    "            if proteins:\n",
    "                # Séparer les protéines et supprimer les doublons\n",
    "                protein_list = [p.strip() for p in proteins.split(';') if p.strip()]\n",
    "                unique_proteins = list(set(protein_list))\n",
    "                \n",
    "                # Stocker dans le dictionnaire\n",
    "                complexes[complex_id] = unique_proteins\n",
    "    \n",
    "    # Écrire le fichier de sortie\n",
    "    with open(output_file, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerow(['complex_id', 'proteins'])\n",
    "        \n",
    "        for complex_id, proteins in complexes.items():\n",
    "            # Joindre les protéines avec des points-virgules\n",
    "            protein_str = ';'.join(proteins)\n",
    "            writer.writerow([complex_id, protein_str])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_filename = r'C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Complexes\\corum_humanComplexes.txt'  # Remplacez par votre fichier d'entrée\n",
    "    output_filename = r'C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\CORUM_complexes_humain.txt'  # Fichier de sortie\n",
    "    \n",
    "    process_complexes(input_filename, output_filename)\n",
    "    print(f\"Le fichier de sortie a été généré : {output_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.2. filtrage des complexes CORUM -> BIOGRID & CORUM -> STRING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traitement du réseau BIOGRID...\n",
      "- Protéines uniques dans PPI: 9,173\n",
      "- Interactions dans PPI: 66,374\n",
      "- Complexes analysés: 5,367\n",
      "- Complexes conservés: 2,057 (38.3%)\n",
      "  - Rejetés (protéines manquantes): 2,060\n",
      "  - Rejetés (non connectés): 1,250\n",
      "- Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\BIOGRID_complexes_humain.txt\n",
      "\n",
      "Traitement du réseau STRING...\n",
      "- Protéines uniques dans PPI: 14,219\n",
      "- Interactions dans PPI: 246,924\n",
      "- Complexes analysés: 5,367\n",
      "- Complexes conservés: 2,957 (55.1%)\n",
      "  - Rejetés (protéines manquantes): 1,352\n",
      "  - Rejetés (non connectés): 1,058\n",
      "- Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\STRING_complexes_humain.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "def load_ppi_network(ppi_file):\n",
    "    \"\"\"Charge le réseau PPI et retourne un set de protéines uniques et le graphe PPI\"\"\"\n",
    "    proteins = set()\n",
    "    graph = defaultdict(set)\n",
    "    try:\n",
    "        with open(ppi_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                if len(row) >= 2:\n",
    "                    p1, p2 = row[0].strip(), row[1].strip()\n",
    "                    if p1 and p2:\n",
    "                        proteins.add(p1)\n",
    "                        proteins.add(p2)\n",
    "                        graph[p1].add(p2)\n",
    "                        graph[p2].add(p1)\n",
    "        return proteins, graph\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lecture {ppi_file}: {str(e)}\")\n",
    "        return set(), defaultdict(set)\n",
    "\n",
    "def is_single_connected_component(proteins, ppi_graph):\n",
    "    \"\"\"Vérifie si les protéines forment un seul composant connecté dans le réseau PPI\"\"\"\n",
    "    if not proteins:\n",
    "        return False\n",
    "    \n",
    "    visited = set()\n",
    "    queue = deque()\n",
    "    \n",
    "    # Prendre une protéine quelconque comme point de départ\n",
    "    start_protein = next(iter(proteins))\n",
    "    queue.append(start_protein)\n",
    "    visited.add(start_protein)\n",
    "    \n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        for neighbor in ppi_graph[current]:\n",
    "            if neighbor in proteins and neighbor not in visited:\n",
    "                visited.add(neighbor)\n",
    "                queue.append(neighbor)\n",
    "    \n",
    "    return visited == proteins\n",
    "\n",
    "def filter_complexes(complexes_file, ppi_proteins, ppi_graph, output_file):\n",
    "    \"\"\"Filtre les complexes conservant seulement ceux avec toutes les protéines dans le PPI et formant un seul composant connecté\"\"\"\n",
    "    stats = {'total': 0, 'kept': 0, 'missing_proteins': 0, 'disconnected': 0}\n",
    "    \n",
    "    try:\n",
    "        with open(complexes_file, 'r', encoding='utf-8') as f_in, \\\n",
    "             open(output_file, 'w', encoding='utf-8', newline='') as f_out:\n",
    "            \n",
    "            writer = csv.writer(f_out, delimiter='\\t')\n",
    "            writer.writerow(['complex_id', 'proteins'])\n",
    "            \n",
    "            for row in csv.reader(f_in, delimiter='\\t'):\n",
    "                if len(row) < 2:\n",
    "                    continue\n",
    "                \n",
    "                stats['total'] += 1\n",
    "                complex_id, proteins_str = row[0].strip(), row[1].strip()\n",
    "                proteins = set(p.strip() for p in proteins_str.split(';') if p.strip())\n",
    "                \n",
    "                # Vérifier que toutes les protéines sont dans le PPI\n",
    "                if not all(p in ppi_proteins for p in proteins):\n",
    "                    stats['missing_proteins'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Vérifier que les protéines forment un seul composant connecté\n",
    "                if not is_single_connected_component(proteins, ppi_graph):\n",
    "                    stats['disconnected'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Écrire le complexe valide\n",
    "                writer.writerow([complex_id, ';'.join(proteins)])\n",
    "                stats['kept'] += 1\n",
    "        \n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur traitement {complexes_file}: {str(e)}\")\n",
    "        return {'total': 0, 'kept': 0, 'missing_proteins': 0, 'disconnected': 0}\n",
    "\n",
    "def process_ppi_network(ppi_file, complexes_file, output_file, network_name):\n",
    "    \"\"\"Processus complet pour un réseau PPI\"\"\"\n",
    "    print(f\"\\nTraitement du réseau {network_name}...\")\n",
    "    \n",
    "    # Charger les protéines PPI et le graphe\n",
    "    ppi_proteins, ppi_graph = load_ppi_network(ppi_file)\n",
    "    if not ppi_proteins:\n",
    "        print(f\"Échec: Aucune protéine chargée depuis {ppi_file}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"- Protéines uniques dans PPI: {len(ppi_proteins):,}\")\n",
    "    print(f\"- Interactions dans PPI: {sum(len(v) for v in ppi_graph.values())//2:,}\")\n",
    "\n",
    "    # Filtrer les complexes\n",
    "    stats = filter_complexes(complexes_file, ppi_proteins, ppi_graph, output_file)\n",
    "    \n",
    "    if stats['total'] == 0:\n",
    "        print(\"Échec: Aucun complexe traité\")\n",
    "        return\n",
    "    \n",
    "    # Afficher les statistiques\n",
    "    print(f\"- Complexes analysés: {stats['total']:,}\")\n",
    "    print(f\"- Complexes conservés: {stats['kept']:,} ({stats['kept']/stats['total']*100:.1f}%)\")\n",
    "    print(f\"  - Rejetés (protéines manquantes): {stats['missing_proteins']:,}\")\n",
    "    print(f\"  - Rejetés (non connectés): {stats['disconnected']:,}\")\n",
    "    print(f\"- Fichier généré: {output_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration des chemins\n",
    "    DATA_DIR = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\")\n",
    "    \n",
    "    # Fichier de complexes source\n",
    "    CORUM_FILE = DATA_DIR / \"complexes\" / \"CORUM_complexes_humain.txt\"\n",
    "    \n",
    "    # Traitement BIOGRID\n",
    "    process_ppi_network(\n",
    "        ppi_file=DATA_DIR / \"weighted_networks\" / \"weighted_BIOGRID_humain.txt\",\n",
    "        complexes_file=CORUM_FILE,\n",
    "        output_file=DATA_DIR / \"complexes\" / \"BIOGRID_complexes_humain.txt\",\n",
    "        network_name=\"BIOGRID\"\n",
    "    )\n",
    "    \n",
    "    # Traitement STRING\n",
    "    process_ppi_network(\n",
    "        ppi_file=DATA_DIR / \"weighted_networks\" / \"weighted_STRING_humain.txt\",\n",
    "        complexes_file=CORUM_FILE,\n",
    "        output_file=DATA_DIR / \"complexes\" / \"STRING_complexes_humain.txt\",\n",
    "        network_name=\"STRING\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Normalisation de subcellular localization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vérification des fichiers...\n",
      "- Human mapping: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\HUMAN_9606_idmapping.dat (OK)\n",
      "- Yeast mapping: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\YEAST_559292_idmapping.dat (OK)\n",
      "- Human compartment: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\human_compartment_integrated_full.tsv (OK)\n",
      "- Yeast compartment: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\yeast_compartment_integrated_full.tsv (OK)\n",
      "\n",
      "Dossier de sortie: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\n",
      "\n",
      "==================================================\n",
      "CHARGEMENT DES FICHIERS DE MAPPING\n",
      "==================================================\n",
      "Chargement du fichier de mapping: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\HUMAN_9606_idmapping.dat\n",
      "Ligne 100000 traitée...\n",
      "Ligne 200000 traitée...\n",
      "Ligne 300000 traitée...\n",
      "Ligne 400000 traitée...\n",
      "Ligne 500000 traitée...\n",
      "Ligne 600000 traitée...\n",
      "Ligne 700000 traitée...\n",
      "Ligne 800000 traitée...\n",
      "Ligne 900000 traitée...\n",
      "Ligne 1000000 traitée...\n",
      "Ligne 1100000 traitée...\n",
      "Ligne 1200000 traitée...\n",
      "Ligne 1300000 traitée...\n",
      "Ligne 1400000 traitée...\n",
      "Ligne 1500000 traitée...\n",
      "Ligne 1600000 traitée...\n",
      "Ligne 1700000 traitée...\n",
      "Ligne 1800000 traitée...\n",
      "Ligne 1900000 traitée...\n",
      "Ligne 2000000 traitée...\n",
      "Ligne 2100000 traitée...\n",
      "Ligne 2200000 traitée...\n",
      "Ligne 2300000 traitée...\n",
      "Ligne 2400000 traitée...\n",
      "Ligne 2500000 traitée...\n",
      "Ligne 2600000 traitée...\n",
      "Ligne 2700000 traitée...\n",
      "Ligne 2800000 traitée...\n",
      "Ligne 2900000 traitée...\n",
      "Ligne 3000000 traitée...\n",
      "Ligne 3100000 traitée...\n",
      "Ligne 3200000 traitée...\n",
      "Ligne 3300000 traitée...\n",
      "Ligne 3400000 traitée...\n",
      "Ligne 3500000 traitée...\n",
      "Ligne 3600000 traitée...\n",
      "Ligne 3700000 traitée...\n",
      "Ligne 3800000 traitée...\n",
      "Ligne 3900000 traitée...\n",
      "Ligne 4000000 traitée...\n",
      "Ligne 4100000 traitée...\n",
      "Ligne 4200000 traitée...\n",
      "Ligne 4300000 traitée...\n",
      "Ligne 4400000 traitée...\n",
      "Ligne 4500000 traitée...\n",
      "Ligne 4600000 traitée...\n",
      "Ligne 4700000 traitée...\n",
      "Ligne 4800000 traitée...\n",
      "Ligne 4900000 traitée...\n",
      "Ligne 5000000 traitée...\n",
      "Ligne 5100000 traitée...\n",
      "Ligne 5200000 traitée...\n",
      "Chargement du fichier de mapping: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\YEAST_559292_idmapping.dat\n",
      "Ligne 100000 traitée...\n",
      "Ligne 200000 traitée...\n",
      "Ligne 300000 traitée...\n",
      "\n",
      "Temps de chargement: 25.88 secondes\n",
      "Protéines humaines chargées: 237779\n",
      "Protéines yeast chargées: 6900\n",
      "\n",
      "==================================================\n",
      "TRAITEMENT DES DONNÉES HUMAINES\n",
      "==================================================\n",
      "\n",
      "Début du traitement du fichier: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\human_compartment_integrated_full.tsv\n",
      "Chargement des données de compartiments...\n",
      "Nombre d'entrées à traiter: 4854519\n",
      "Préparation des structures de recherche...\n",
      "Application du mapping...\n",
      "Extraction des résultats...\n",
      "Sauvegarde des résultats dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\human_compartment_mapped.txt...\n",
      "382276 protéines non mappées sauvegardées dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\human_compartment_mapped_unmapped.txt\n",
      "Traitement terminé avec succès!\n",
      "\n",
      "==================================================\n",
      "TRAITEMENT DES DONNÉES YEAST\n",
      "==================================================\n",
      "\n",
      "Début du traitement du fichier: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\yeast_compartment_integrated_full.tsv\n",
      "Chargement des données de compartiments...\n",
      "Nombre d'entrées à traiter: 745850\n",
      "Préparation des structures de recherche...\n",
      "Application du mapping...\n",
      "Extraction des résultats...\n",
      "Sauvegarde des résultats dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\yeast_compartment_mapped.txt...\n",
      "28325 protéines non mappées sauvegardées dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\yeast_compartment_mapped_unmapped.txt\n",
      "Traitement terminé avec succès!\n",
      "\n",
      "==================================================\n",
      "RÉSULTATS FINAUX\n",
      "==================================================\n",
      "HUMAIN - Total: 4854519, Mappés: 4472243 (92.1%)\n",
      "YEAST  - Total: 745850, Mappés: 717525 (96.2%)\n",
      "\n",
      "Temps total d'exécution: 211.92 secondes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mapping_file(file_path):\n",
    "    \"\"\"Charge le fichier de mapping de manière optimisée\"\"\"\n",
    "    mapping = defaultdict(dict)\n",
    "    id_conversion = defaultdict(dict)\n",
    "    \n",
    "    # Pré-compiler les regex pour plus de performance\n",
    "    yeast_pattern = re.compile(r'^Y[A-Z]{2}\\d{3}[A-Z]?$')\n",
    "    \n",
    "    print(f\"Chargement du fichier de mapping: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            if i % 100000 == 0:\n",
    "                print(f\"Ligne {i} traitée...\")\n",
    "                \n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "                \n",
    "            uniprot_id, id_type, id_value = parts[0], parts[1], parts[2]\n",
    "            \n",
    "            # Stockage optimisé des données\n",
    "            if id_type not in mapping[uniprot_id]:\n",
    "                mapping[uniprot_id][id_type] = []\n",
    "            mapping[uniprot_id][id_type].append(id_value)\n",
    "            \n",
    "            # Conversion inversée optimisée\n",
    "            if id_type == 'STRING' and 'ENSP' in id_value:\n",
    "                ensembl_id = id_value.split('.')[-1]\n",
    "                id_conversion['Ensembl'][ensembl_id] = uniprot_id\n",
    "            elif id_type == 'Gene_OrderedLocusName' and yeast_pattern.match(id_value):\n",
    "                id_conversion['Yeast_Locus'][id_value] = uniprot_id\n",
    "            elif id_type == 'Ensembl_PRO':\n",
    "                id_conversion['Ensembl_PRO'][id_value] = uniprot_id\n",
    "    \n",
    "    return dict(mapping), dict(id_conversion)\n",
    "\n",
    "def map_compartment_data(compartment_file, mapping_data, id_conversion, output_file, species):\n",
    "    \"\"\"Mappe les données de compartiment de manière optimisée\"\"\"\n",
    "    print(f\"\\nDébut du traitement du fichier: {compartment_file}\")\n",
    "    \n",
    "    # Charger le fichier de compartiments\n",
    "    print(\"Chargement des données de compartiments...\")\n",
    "    compartment_df = pd.read_csv(\n",
    "        compartment_file, \n",
    "        sep='\\t', \n",
    "        header=None,\n",
    "        names=['Protein_ID', 'Gene_Name', 'GO_ID', 'GO_Term', 'Score'],\n",
    "        dtype={'Protein_ID': 'string', 'Gene_Name': 'string', \n",
    "               'GO_ID': 'string', 'GO_Term': 'string'},\n",
    "        on_bad_lines='warn'\n",
    "    )\n",
    "    \n",
    "    print(f\"Nombre d'entrées à traiter: {len(compartment_df)}\")\n",
    "    \n",
    "    # Préparer les structures pour une recherche rapide\n",
    "    print(\"Préparation des structures de recherche...\")\n",
    "    string_map = {}\n",
    "    if species == 'human':\n",
    "        string_map = {v.split('.')[-1]: k for k in mapping_data for v in mapping_data[k].get('STRING', [])}\n",
    "    \n",
    "    # Fonction de mapping optimisée\n",
    "    def map_id(protein_id):\n",
    "        # 1. Essayer le mapping direct\n",
    "        if protein_id in mapping_data:\n",
    "            return protein_id, mapping_data[protein_id]\n",
    "        \n",
    "        # 2. Essayer les conversions spécifiques\n",
    "        if species == 'human' and protein_id.startswith('ENSP'):\n",
    "            # Via STRING ID\n",
    "            if protein_id in string_map:\n",
    "                uniprot_id = string_map[protein_id]\n",
    "                return uniprot_id, mapping_data.get(uniprot_id, {})\n",
    "            # Via Ensembl_PRO direct\n",
    "            if protein_id in id_conversion.get('Ensembl_PRO', {}):\n",
    "                uniprot_id = id_conversion['Ensembl_PRO'][protein_id]\n",
    "                return uniprot_id, mapping_data.get(uniprot_id, {})\n",
    "                \n",
    "        elif species == 'yeast':\n",
    "            # Via Yeast Locus ID\n",
    "            if protein_id in id_conversion.get('Yeast_Locus', {}):\n",
    "                uniprot_id = id_conversion['Yeast_Locus'][protein_id]\n",
    "                return uniprot_id, mapping_data.get(uniprot_id, {})\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    # Appliquer le mapping\n",
    "    print(\"Application du mapping...\")\n",
    "    compartment_df['Mapped_Info'] = compartment_df['Protein_ID'].apply(map_id)\n",
    "    \n",
    "    # Extraire les résultats\n",
    "    print(\"Extraction des résultats...\")\n",
    "    compartment_df['Mapped_UniProtKB_ID'] = compartment_df['Mapped_Info'].apply(lambda x: x[0] if x and x[0] else '')\n",
    "    compartment_df['Mapping_Status'] = compartment_df['Mapped_UniProtKB_ID'].apply(lambda x: 'Success' if x else 'Failed')\n",
    "    \n",
    "    # Remplir les colonnes mappées\n",
    "    for field in ['GeneID', 'Ensembl', 'RefSeq', 'Entrez_Gene', 'STRING_ID']:\n",
    "        compartment_df[field] = compartment_df['Mapped_Info'].apply(\n",
    "            lambda x: x[1].get(field, [''])[0] if x and x[1] else ''\n",
    "        )\n",
    "    \n",
    "    # Sélectionner les colonnes finales\n",
    "    result_cols = {\n",
    "        'Protein_ID': 'Original_Protein_ID',\n",
    "        'Gene_Name': 'Gene_Name',\n",
    "        'GO_ID': 'GO_ID',\n",
    "        'GO_Term': 'GO_Term',\n",
    "        'Score': 'Score',\n",
    "        'Mapped_UniProtKB_ID': 'Mapped_UniProtKB_ID',\n",
    "        'GeneID': 'GeneID',\n",
    "        'Ensembl': 'Ensembl',\n",
    "        'RefSeq': 'RefSeq',\n",
    "        'Entrez_Gene': 'Entrez_Gene',\n",
    "        'STRING_ID': 'STRING_ID',\n",
    "        'Mapping_Status': 'Mapping_Status'\n",
    "    }\n",
    "    \n",
    "    result_df = compartment_df[list(result_cols.keys())].rename(columns=result_cols)\n",
    "    \n",
    "    # Sauvegarder les résultats\n",
    "    print(f\"Sauvegarde des résultats dans {output_file}...\")\n",
    "    result_df.to_csv(output_file, sep='\\t', index=False)\n",
    "    \n",
    "    # Calculer les statistiques\n",
    "    total = len(result_df)\n",
    "    mapped = (result_df['Mapping_Status'] == 'Success').sum()\n",
    "    stats = {'total': total, 'mapped': mapped, 'unmapped': total - mapped}\n",
    "    \n",
    "    # Sauvegarder les stats\n",
    "    stats_file = output_file.with_name(output_file.stem + \"_stats.txt\")\n",
    "    with open(stats_file, 'w') as f:\n",
    "        f.write(f\"Total proteins: {stats['total']}\\n\")\n",
    "        f.write(f\"Mapped proteins: {stats['mapped']} ({stats['mapped']/stats['total']:.1%})\\n\")\n",
    "        f.write(f\"Unmapped proteins: {stats['unmapped']} ({stats['unmapped']/stats['total']:.1%})\\n\")\n",
    "    \n",
    "    # Sauvegarder les non-mappés si nécessaire\n",
    "    if stats['unmapped'] > 0:\n",
    "        unmapped_file = output_file.with_name(output_file.stem + \"_unmapped.txt\")\n",
    "        unmapped_proteins = result_df[result_df['Mapping_Status'] == 'Failed']['Original_Protein_ID']\n",
    "        unmapped_proteins.to_csv(unmapped_file, index=False, header=False)\n",
    "        print(f\"{stats['unmapped']} protéines non mappées sauvegardées dans {unmapped_file}\")\n",
    "    \n",
    "    print(\"Traitement terminé avec succès!\")\n",
    "    return stats\n",
    "\n",
    "# ==============================================\n",
    "# CONFIGURATION PRINCIPALE\n",
    "# ==============================================\n",
    "\n",
    "# Chemins absolus des fichiers\n",
    "BASE_DIR = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\")\n",
    "RAW_DATA_DIR = BASE_DIR / \"raw data\" / \"autres\"\n",
    "CLEAN_DATA_DIR = BASE_DIR / \"clean data\"\n",
    "\n",
    "human_mapping_path = RAW_DATA_DIR / \"HUMAN_9606_idmapping.dat\"\n",
    "yeast_mapping_path = RAW_DATA_DIR / \"YEAST_559292_idmapping.dat\"\n",
    "human_compartment_path = RAW_DATA_DIR / \"human_compartment_integrated_full.tsv\"\n",
    "yeast_compartment_path = RAW_DATA_DIR / \"yeast_compartment_integrated_full.tsv\"\n",
    "output_dir = CLEAN_DATA_DIR / \"autres\"\n",
    "\n",
    "# Vérification des fichiers\n",
    "print(\"\\nVérification des fichiers...\")\n",
    "required_files = {\n",
    "    \"Human mapping\": human_mapping_path,\n",
    "    \"Yeast mapping\": yeast_mapping_path,\n",
    "    \"Human compartment\": human_compartment_path,\n",
    "    \"Yeast compartment\": yeast_compartment_path\n",
    "}\n",
    "\n",
    "for name, path in required_files.items():\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Fichier {name} introuvable à l'emplacement: {path}\")\n",
    "    print(f\"- {name}: {path} (OK)\")\n",
    "\n",
    "# Créer le dossier de sortie\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"\\nDossier de sortie: {output_dir}\")\n",
    "\n",
    "# ==============================================\n",
    "# EXÉCUTION PRINCIPALE\n",
    "# ==============================================\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # 1. Charger les fichiers de mapping\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CHARGEMENT DES FICHIERS DE MAPPING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    human_mapping, human_conversion = load_mapping_file(human_mapping_path)\n",
    "    yeast_mapping, yeast_conversion = load_mapping_file(yeast_mapping_path)\n",
    "    \n",
    "    print(f\"\\nTemps de chargement: {time.time() - start_time:.2f} secondes\")\n",
    "    print(f\"Protéines humaines chargées: {len(human_mapping)}\")\n",
    "    print(f\"Protéines yeast chargées: {len(yeast_mapping)}\")\n",
    "    \n",
    "    # 2. Traitement des données humaines\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAITEMENT DES DONNÉES HUMAINES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    human_stats = map_compartment_data(\n",
    "        human_compartment_path,\n",
    "        human_mapping,\n",
    "        human_conversion,\n",
    "        output_dir / \"human_compartment_mapped.txt\",\n",
    "        'human'\n",
    "    )\n",
    "    \n",
    "    # 3. Traitement des données yeast\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAITEMENT DES DONNÉES YEAST\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    yeast_stats = map_compartment_data(\n",
    "        yeast_compartment_path,\n",
    "        yeast_mapping,\n",
    "        yeast_conversion,\n",
    "        output_dir / \"yeast_compartment_mapped.txt\",\n",
    "        'yeast'\n",
    "    )\n",
    "    \n",
    "    # 4. Affichage des résultats finaux\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RÉSULTATS FINAUX\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"HUMAIN - Total: {human_stats['total']}, Mappés: {human_stats['mapped']} ({human_stats['mapped']/human_stats['total']:.1%})\")\n",
    "    print(f\"YEAST  - Total: {yeast_stats['total']}, Mappés: {yeast_stats['mapped']} ({yeast_stats['mapped']/yeast_stats['total']:.1%})\")\n",
    "    print(f\"\\nTemps total d'exécution: {time.time() - start_time:.2f} secondes\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERREUR: {str(e)}\")\n",
    "    raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
