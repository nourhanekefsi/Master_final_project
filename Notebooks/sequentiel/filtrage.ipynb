{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Filtrage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Base de donnees STRING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Levure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STRING ----- levure\n",
      "Traitement terminé avec succès.\n",
      "Nombre de protéines uniques : 5782\n",
      "Nombre d'interactions uniques : 103986\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemins des fichiers\n",
    "input_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\STRING_Interactions.txt\")\n",
    "output_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_levure.temp\")\n",
    "\n",
    "# 1. Chargement et filtrage initial\n",
    "df = pd.read_csv(input_path, sep=\" \")\n",
    "\n",
    "# Créer un masque de filtrage\n",
    "mask = (df[\"combined_score\"] >= 700) & (\n",
    "    df[[\"experimental\", \"coexpression\", \"database\", \"textmining\"]].max(axis=1) > 90\n",
    ")\n",
    "\n",
    "# Appliquer le filtre et faire une copie explicite\n",
    "filtered_df = df.loc[mask].copy()\n",
    "\n",
    "# 2. Nettoyage des données\n",
    "# Supprimer \"4932.\" des noms de protéines\n",
    "for col in ['protein1', 'protein2']:\n",
    "    filtered_df.loc[:, col] = filtered_df[col].str.replace('4932.', '')\n",
    "\n",
    "# 3. Gestion des interactions uniques\n",
    "# Créer des identifiants d'interaction canoniques\n",
    "filtered_df.loc[:, 'interaction_key'] = filtered_df.apply(\n",
    "    lambda x: frozenset({x['protein1'], x['protein2']}), axis=1\n",
    ")\n",
    "\n",
    "# Supprimer les doublons en gardant la première occurrence\n",
    "unique_interactions_df = filtered_df.drop_duplicates(subset='interaction_key')\n",
    "\n",
    "# 4. Sauvegarde des résultats\n",
    "unique_interactions_df[['protein1', 'protein2']].to_csv(\n",
    "    output_path, sep=\"\\t\", index=False, header=False\n",
    ")\n",
    "\n",
    "# 5. Calcul des statistiques\n",
    "unique_proteins = pd.unique(\n",
    "    unique_interactions_df[['protein1', 'protein2']].values.ravel('K')\n",
    ")\n",
    "num_interactions = len(unique_interactions_df)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\n STRING ----- levure\")\n",
    "print(\"Traitement terminé avec succès.\")\n",
    "print(f\"Nombre de protéines uniques : {len(unique_proteins)}\")\n",
    "print(f\"Nombre d'interactions uniques : {num_interactions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1.1. normalisation de STRING levure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traitement terminé avec succès.\n",
      "Nombre total d'interactions conservées : 96810\n",
      "Nombre total de protéines uniques : 5707\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Chemins des fichiers\n",
    "interactions_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_levure.temp\"\n",
    "mapping_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\YEAST_559292_idmapping.dat\"\n",
    "output_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_levure.txt\"\n",
    "\n",
    "def create_mapping_dict(mapping_file):\n",
    "    \"\"\"Crée un dictionnaire de mapping STRING -> UniProt et inversement\"\"\"\n",
    "    str_to_uniprot = defaultdict(list)\n",
    "    uniprot_to_str = defaultdict(list)\n",
    "    \n",
    "    with open(mapping_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 3:\n",
    "                uniprot_id = parts[0]\n",
    "                db_type = parts[1]\n",
    "                external_id = parts[2]\n",
    "\n",
    "                if db_type == \"STRING\":\n",
    "                    # Supprimer le préfixe '4932.' si présent\n",
    "                    if external_id.startswith(\"4932.\"):\n",
    "                        external_id = external_id.replace(\"4932.\", \"\")\n",
    "                    str_to_uniprot[external_id].append(uniprot_id)\n",
    "                    uniprot_to_str[uniprot_id].append(external_id)\n",
    "    \n",
    "    return str_to_uniprot\n",
    "\n",
    "def process_interactions(interactions_file, str_to_uniprot):\n",
    "    \"\"\"Traite le fichier d'interactions\"\"\"\n",
    "    interactions = []\n",
    "    missing_mappings = set()\n",
    "    \n",
    "    with open(interactions_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) >= 2:\n",
    "                prot1, prot2 = parts[0], parts[1]\n",
    "                \n",
    "                # Vérifier si les protéines existent dans le mapping\n",
    "                mapped1 = str_to_uniprot.get(prot1, [None])[0]\n",
    "                mapped2 = str_to_uniprot.get(prot2, [None])[0]\n",
    "                \n",
    "                if mapped1 and mapped2:\n",
    "                    interactions.append((mapped1, mapped2))\n",
    "                else:\n",
    "                    if not mapped1:\n",
    "                        missing_mappings.add(prot1)\n",
    "                    if not mapped2:\n",
    "                        missing_mappings.add(prot2)\n",
    "    \n",
    "    return interactions\n",
    "\n",
    "def main():\n",
    "    # 1. Créer le dictionnaire de mapping\n",
    "    str_to_uniprot = create_mapping_dict(mapping_file)\n",
    "    \n",
    "    # 2. Traiter les interactions\n",
    "    filtered_interactions = process_interactions(interactions_file, str_to_uniprot)\n",
    "    \n",
    "    # 3. Sauvegarder les résultats\n",
    "    with open(output_file, 'w') as f:\n",
    "        for prot1, prot2 in filtered_interactions:\n",
    "            f.write(f\"{prot1}\\t{prot2}\\n\")\n",
    "\n",
    "    # 4. Calcul des statistiques\n",
    "    unique_proteins = set()\n",
    "    for p1, p2 in filtered_interactions:\n",
    "        unique_proteins.update([p1, p2])\n",
    "\n",
    "    # 5. Affichage simplifié\n",
    "    print(\"\\nTraitement terminé avec succès.\")\n",
    "    print(f\"Nombre total d'interactions conservées : {len(filtered_interactions)}\")\n",
    "    print(f\"Nombre total de protéines uniques : {len(unique_proteins)}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2. humain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " STRING ----- humain\n",
      "Traitement terminé avec succès.\n",
      "Nombre de protéines uniques : 16187\n",
      "Nombre d'interactions uniques : 236743\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Chemins des fichiers\n",
    "input_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\9606.protein.links.detailed.v12.0.txt\")\n",
    "output_path = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_humain.temp\")\n",
    "\n",
    "# 1. Chargement et filtrage initial\n",
    "df = pd.read_csv(input_path, sep=\" \")\n",
    "\n",
    "# Créer un masque de filtrage\n",
    "mask = (df[\"combined_score\"] >= 700) & (\n",
    "    df[[\"experimental\", \"coexpression\", \"database\", \"textmining\"]].max(axis=1) > 90\n",
    ")\n",
    "\n",
    "# Appliquer le filtre et faire une copie explicite\n",
    "filtered_df = df.loc[mask].copy()\n",
    "\n",
    "# 2. Nettoyage des données\n",
    "# Supprimer \"4932.\" des noms de protéines\n",
    "for col in ['protein1', 'protein2']:\n",
    "    filtered_df.loc[:, col] = filtered_df[col].str.replace('9606.', '')\n",
    "\n",
    "# 3. Gestion des interactions uniques\n",
    "# Créer des identifiants d'interaction canoniques\n",
    "filtered_df.loc[:, 'interaction_key'] = filtered_df.apply(\n",
    "    lambda x: frozenset({x['protein1'], x['protein2']}), axis=1\n",
    ")\n",
    "\n",
    "# Supprimer les doublons en gardant la première occurrence\n",
    "unique_interactions_df = filtered_df.drop_duplicates(subset='interaction_key')\n",
    "\n",
    "# 4. Sauvegarde des résultats\n",
    "unique_interactions_df[['protein1', 'protein2']].to_csv(\n",
    "    output_path, sep=\"\\t\", index=False, header=False\n",
    ")\n",
    "\n",
    "# 5. Calcul des statistiques\n",
    "unique_proteins = pd.unique(\n",
    "    unique_interactions_df[['protein1', 'protein2']].values.ravel('K')\n",
    ")\n",
    "num_interactions = len(unique_interactions_df)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(\"\\n STRING ----- humain\")\n",
    "print(\"Traitement terminé avec succès.\")\n",
    "print(f\"Nombre de protéines uniques : {len(unique_proteins)}\")\n",
    "print(f\"Nombre d'interactions uniques : {num_interactions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.3. Normalisations des interactions STRING (humain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing mapping file...\n",
      "Found 19110 ENSP to Uniprot mappings\n",
      "Converting interactions...\n",
      "Converted interactions written to C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_humain.txt\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_mapping_file(mapping_file):\n",
    "    \"\"\"\n",
    "    Parse the mapping file to create a dictionary from ENSP to Uniprot IDs.\n",
    "    The mapping file is assumed to have lines with format like:\n",
    "    P31946 STRING 9606.ENSP00000361930\n",
    "    \"\"\"\n",
    "    ensembl_to_uniprot = defaultdict(list)\n",
    "    \n",
    "    with open(mapping_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "            uniprot_id, db, db_id = parts[:3]\n",
    "            if db == \"STRING\":\n",
    "                # Extract ENSP ID from STRING format (e.g., 9606.ENSP00000361930)\n",
    "                if '.' in db_id:\n",
    "                    ensembl_id = db_id.split('.')[1]\n",
    "                    ensembl_to_uniprot[ensembl_id].append(uniprot_id)\n",
    "    \n",
    "    # Remove duplicates and keep only the first Uniprot ID for each ENSP\n",
    "    return {ensp: uniprots[0] for ensp, uniprots in ensembl_to_uniprot.items() if uniprots}\n",
    "\n",
    "def convert_interactions(input_file, output_file, mapping_dict):\n",
    "    \"\"\"\n",
    "    Convert interaction file from ENSP IDs to Uniprot IDs using the mapping dictionary.\n",
    "    Only writes interactions where both proteins have corresponding Uniprot IDs.\n",
    "    \"\"\"\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            prot1, prot2 = parts\n",
    "            if prot1 in mapping_dict and prot2 in mapping_dict:\n",
    "                uniprot1 = mapping_dict[prot1]\n",
    "                uniprot2 = mapping_dict[prot2]\n",
    "                outfile.write(f\"{uniprot1}\\t{uniprot2}\\n\")\n",
    "\n",
    "def main():\n",
    "    interactions_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_humain_filtered_interactions.txt\"\n",
    "    mapping_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\HUMAN_9606_idmapping.dat\"\n",
    "    output_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\STRING_humain.txt\"\n",
    "    \n",
    "    print(\"Parsing mapping file...\")\n",
    "    mapping_dict = parse_mapping_file(mapping_file)\n",
    "    print(f\"Found {len(mapping_dict)} ENSP to Uniprot mappings\")\n",
    "    \n",
    "    print(\"Converting interactions...\")\n",
    "    convert_interactions(interactions_file, output_file, mapping_dict)\n",
    "    print(f\"Converted interactions written to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Base de donnees DIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats du traitement:\n",
      "- Protéines uniques: 5144\n",
      "- Interactions uniques: 22614\n",
      "Fichier sauvegardé: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\DIP_levure.txt\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "def process_dip_interactions():\n",
    "    # Configuration des chemins\n",
    "    input_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\DIP_Interactions.mif25\")\n",
    "    output_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\DIP_levure.txt\")\n",
    "    \n",
    "    NS = {'mif': 'http://psi.hupo.org/mi/mif'}\n",
    "    tree = ET.parse(input_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # 1. Extraction des protéines avec vérification\n",
    "    id_to_protein = {}\n",
    "    for interactor in root.findall(\".//mif:interactor\", NS):\n",
    "        interactor_id = interactor.get(\"id\")\n",
    "        if not interactor_id:\n",
    "            continue\n",
    "            \n",
    "        uniprot_ref = interactor.find(\".//mif:xref/mif:secondaryRef[@db='uniprot knowledge base']\", NS)\n",
    "        refseq_ref = interactor.find(\".//mif:xref/mif:secondaryRef[@db='refseq']\", NS)\n",
    "        \n",
    "        protein_id = None\n",
    "        if uniprot_ref is not None:\n",
    "            protein_id = uniprot_ref.get(\"id\")\n",
    "        elif refseq_ref is not None:\n",
    "            protein_id = refseq_ref.get(\"id\")\n",
    "        else:\n",
    "            short_label = interactor.find(\".//mif:names/mif:shortLabel\", NS)\n",
    "            protein_id = short_label.text if short_label is not None else None\n",
    "        \n",
    "        if protein_id:\n",
    "            id_to_protein[interactor_id] = protein_id\n",
    "\n",
    "    # 2. Extraction des interactions avec contrôle qualité\n",
    "    unique_interactions = set()\n",
    "    protein_set = set()  # Pour stocker les protéines uniques\n",
    "    \n",
    "    for interaction in root.findall(\".//mif:interaction\", NS):\n",
    "        participants = interaction.findall(\".//mif:participant/mif:interactorRef\", NS)\n",
    "        \n",
    "        if len(participants) != 2:\n",
    "            continue\n",
    "            \n",
    "        id1, id2 = participants[0].text, participants[1].text\n",
    "        \n",
    "        # Vérification que les deux protéines existent et sont différentes\n",
    "        if (id1 not in id_to_protein or \n",
    "            id2 not in id_to_protein or \n",
    "            id_to_protein[id1] == id_to_protein[id2]):\n",
    "            continue\n",
    "            \n",
    "        prot1, prot2 = id_to_protein[id1], id_to_protein[id2]\n",
    "        \n",
    "        # Ajout aux protéines uniques\n",
    "        protein_set.add(prot1)\n",
    "        protein_set.add(prot2)\n",
    "        \n",
    "        # Vérification du score\n",
    "        score_element = interaction.find(\".//mif:confidence/mif:value\", NS)\n",
    "        if score_element is not None:\n",
    "            try:\n",
    "                score = float(score_element.text)\n",
    "                if score <= 0.8:\n",
    "                    continue\n",
    "            except (ValueError, TypeError):\n",
    "                continue\n",
    "                \n",
    "        # Ajout sous forme triée pour éviter les doublons A-B vs B-A\n",
    "        sorted_interaction = tuple(sorted((prot1, prot2)))\n",
    "        unique_interactions.add(sorted_interaction)\n",
    "\n",
    "    # 3. Sauvegarde\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"Protein1\\tProtein2\\n\")\n",
    "        for prot1, prot2 in unique_interactions:\n",
    "            f.write(f\"{prot1}\\t{prot2}\\n\")\n",
    "\n",
    "    # 4. Calcul et affichage des statistiques\n",
    "    num_unique_proteins = len(protein_set)\n",
    "    num_unique_interactions = len(unique_interactions)\n",
    "    \n",
    "    print(\"\\nRésultats du traitement:\")\n",
    "    print(f\"- Protéines uniques: {num_unique_proteins}\")\n",
    "    print(f\"- Interactions uniques: {num_unique_interactions}\")\n",
    "    print(f\"Fichier sauvegardé: {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_dip_interactions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Base de donnes BIOGRID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Levure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats finaux (haute confiance):\n",
      "- Interactions levures totales : 855,577\n",
      "- Interactions avec UniProt valides : 848,870\n",
      "- Interactions non-redondantes : 604,862\n",
      "- Interactions sélectionnées : 300,000\n",
      "- Protéines uniques : 5,805\n",
      "- Score minimum retenu : 6.0\n",
      "- Score maximum : 6.0\n",
      "- Score moyen : 6.0\n",
      "\n",
      "Données sauvegardées dans : C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_levure.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_uniprot(alt_ids):\n",
    "    \"\"\"Extract UniProt IDs from alternative IDs field\"\"\"\n",
    "    if pd.isna(alt_ids): \n",
    "        return None\n",
    "    alt_ids = str(alt_ids)\n",
    "    patterns = [\n",
    "        r'uniprot/swiss[\\W-]?prot:([A-Z0-9]{6,10})',\n",
    "        r'uniprot:([A-Z0-9]{6,10})',\n",
    "        r'swiss[\\W-]?prot:([A-Z0-9]{6,10})',\n",
    "        r'([A-Z0-9]{6,10})\\.\\d'\n",
    "    ]\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, alt_ids, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1).upper()\n",
    "    return None\n",
    "\n",
    "def process_biogrid_high_confidence():\n",
    "    \"\"\"Process BioGRID data to get ~50,000 high-confidence interactions\"\"\"\n",
    "    # File paths\n",
    "    input_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\BIOGRID-ORGANISM-Saccharomyces_cerevisiae.txt\")\n",
    "    output_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_levure.txt\")\n",
    "    \n",
    "    # Load data\n",
    "    try:\n",
    "        df = pd.read_csv(input_file, sep='\\t', comment='#', header=None, dtype=str)\n",
    "        df.columns = [\n",
    "            'ID_A', 'ID_B', 'Alt_IDs_A', 'Alt_IDs_B', \n",
    "            'Aliases_A', 'Aliases_B', 'Method', 'Author',\n",
    "            'PubIDs', 'TaxID_A', 'TaxID_B', 'IntType',\n",
    "            'SourceDB', 'IntIDs', 'Confidence'\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 1. Filter for Saccharomyces cerevisiae\n",
    "    yeast_df = df[(df['TaxID_A'] == 'taxid:559292') & (df['TaxID_B'] == 'taxid:559292')].copy()\n",
    "    \n",
    "    # 2. Extract UniProt IDs\n",
    "    yeast_df['Protein1'] = yeast_df['Alt_IDs_A'].apply(extract_uniprot)\n",
    "    yeast_df['Protein2'] = yeast_df['Alt_IDs_B'].apply(extract_uniprot)\n",
    "    \n",
    "    # 3. Clean data\n",
    "    clean_df = yeast_df.dropna(subset=['Protein1', 'Protein2'])\n",
    "    clean_df = clean_df[clean_df['Protein1'] != clean_df['Protein2']].copy()\n",
    "    \n",
    "    # 4. High-confidence scoring system (less strict than ultra-strict version)\n",
    "    method_scores = {\n",
    "        'x-ray crystallography': 10,\n",
    "        'electron microscopy': 8,\n",
    "        'affinity chromatography': 6,\n",
    "        'coimmunoprecipitation': 4,\n",
    "        'two hybrid': 2,\n",
    "        'pull down': 3,\n",
    "        'mass spectrometry': 2\n",
    "    }\n",
    "    \n",
    "    type_scores = {\n",
    "        'direct interaction': 6,\n",
    "        'physical association': 3,\n",
    "        'complex': 4\n",
    "    }\n",
    "    \n",
    "    clean_df['Pub_Count'] = clean_df['PubIDs'].str.count(r'\\|').add(1).fillna(1)\n",
    "    \n",
    "    clean_df['Method_Score'] = clean_df['Method'].apply(\n",
    "        lambda x: max([method_scores.get(method.lower(), 1)\n",
    "                      for method in str(x).split('|') \n",
    "                      if 'psi-mi' not in method.lower()], default=1)\n",
    "    )\n",
    "    \n",
    "    clean_df['Type_Score'] = clean_df['IntType'].apply(\n",
    "        lambda x: max([type_scores.get(typ.lower(), 1)\n",
    "                      for typ in str(x).split('|') \n",
    "                      if 'psi-mi' not in typ.lower()], default=1)\n",
    "    )\n",
    "    \n",
    "    clean_df['Total_Score'] = (\n",
    "        clean_df['Method_Score'] * 3 +\n",
    "        clean_df['Type_Score'] * 2 +\n",
    "        clean_df['Pub_Count']\n",
    "    )\n",
    "    \n",
    "    # 5. Remove duplicates keeping highest scores\n",
    "    clean_df['SortedPair'] = clean_df.apply(\n",
    "        lambda x: tuple(sorted([x['Protein1'], x['Protein2']])), axis=1\n",
    "    )\n",
    "    \n",
    "    non_redundant = clean_df.sort_values('Total_Score', ascending=False).drop_duplicates(subset=['SortedPair'])\n",
    "    \n",
    "    # 6. Dynamic threshold to get ~50,000 interactions\n",
    "    target_count = 300000\n",
    "    if len(non_redundant) > target_count:\n",
    "        # Find score threshold that gives us ~50,000 interactions\n",
    "        thresholds = sorted(non_redundant['Total_Score'].unique(), reverse=True)\n",
    "        for threshold in thresholds:\n",
    "            filtered = non_redundant[non_redundant['Total_Score'] >= threshold]\n",
    "            if len(filtered) <= target_count:\n",
    "                break\n",
    "        \n",
    "        # If we're still too far from target, take top N\n",
    "        if len(filtered) < target_count * 0.8 or len(filtered) > target_count * 1.2:\n",
    "            filtered = non_redundant.head(target_count)\n",
    "    else:\n",
    "        filtered = non_redundant\n",
    "        print(f\"Warning: Only {len(non_redundant)} interactions available\")\n",
    "    \n",
    "    # 7. Statistics\n",
    "    unique_proteins = pd.unique(filtered[['Protein1', 'Protein2']].values.ravel('K'))\n",
    "    \n",
    "    print(f\"\\nRésultats finaux (haute confiance):\")\n",
    "    print(f\"- Interactions levures totales : {len(yeast_df):,}\")\n",
    "    print(f\"- Interactions avec UniProt valides : {len(clean_df):,}\")\n",
    "    print(f\"- Interactions non-redondantes : {len(non_redundant):,}\")\n",
    "    print(f\"- Interactions sélectionnées : {len(filtered):,}\")\n",
    "    print(f\"- Protéines uniques : {len(unique_proteins):,}\")\n",
    "    print(f\"- Score minimum retenu : {filtered['Total_Score'].min():.1f}\")\n",
    "    print(f\"- Score maximum : {filtered['Total_Score'].max():.1f}\")\n",
    "    print(f\"- Score moyen : {filtered['Total_Score'].mean():.1f}\")\n",
    "    \n",
    "    # 8. Save\n",
    "    try:\n",
    "        filtered[['Protein1', 'Protein2']].to_csv(\n",
    "            output_file, \n",
    "            sep='\\t', \n",
    "            index=False, \n",
    "            header=False\n",
    "        )\n",
    "        print(f\"\\nDonnées sauvegardées dans : {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la sauvegarde : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_biogrid_high_confidence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2. humain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats finaux :\n",
      "- Interactions humaines totales : 316235\n",
      "- Interactions avec UniProt valides : 309156\n",
      "- Interactions non-redondantes : 88647\n",
      "- Protéines uniques : 11266\n",
      "- Score moyen (pour information) : 6.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def process_biogrid_max_coverage():\n",
    "    # Chemins des fichiers\n",
    "    input_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Protein_Interactions\\BIOGRID-MV-Physical.txt\")\n",
    "    output_file = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\interactions\\BIOGRID_humain.txt\")\n",
    "    \n",
    "    # Charger les données en forçant le type string\n",
    "    df = pd.read_csv(input_file, sep='\\t', comment='#', header=None, dtype=str)\n",
    "    df.columns = [\n",
    "        'ID_A', 'ID_B', 'Alt_IDs_A', 'Alt_IDs_B', \n",
    "        'Aliases_A', 'Aliases_B', 'Method', 'Author',\n",
    "        'PubIDs', 'TaxID_A', 'TaxID_B', 'IntType',\n",
    "        'SourceDB', 'IntIDs', 'Confidence'\n",
    "    ]\n",
    "    \n",
    "    # 1. Filtrer pour Homo sapiens uniquement\n",
    "    human_df = df[(df['TaxID_A'] == 'taxid:9606') & (df['TaxID_B'] == 'taxid:9606')].copy()\n",
    "    \n",
    "    # 2. Extraction robuste des UniProt IDs\n",
    "    def extract_uniprot(alt_ids):\n",
    "        if pd.isna(alt_ids): \n",
    "            return None\n",
    "        patterns = [\n",
    "            r'uniprot/swiss[\\W-]?prot:([A-Z0-9]{6,8})',\n",
    "            r'uniprot:([A-Z0-9]{6,8})',\n",
    "            r'([A-Z0-9]{6,8})\\.\\d'\n",
    "        ]\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, str(alt_ids), re.IGNORECASE)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    human_df['Protein1'] = human_df['Alt_IDs_A'].apply(extract_uniprot)\n",
    "    human_df['Protein2'] = human_df['Alt_IDs_B'].apply(extract_uniprot)\n",
    "    \n",
    "    # 3. Nettoyage de base\n",
    "    clean_df = human_df.dropna(subset=['Protein1', 'Protein2'])\n",
    "    clean_df = clean_df[clean_df['Protein1'] != clean_df['Protein2']].copy()\n",
    "    \n",
    "    # 4. Système de scoring des interactions (conservé pour information)\n",
    "    method_scores = {\n",
    "        'x-ray crystallography': 4,\n",
    "        'electron microscopy': 3,\n",
    "        'affinity chromatography': 3,\n",
    "        'coimmunoprecipitation': 2,\n",
    "        'two hybrid': 2,\n",
    "        'pull down': 2,\n",
    "        'mass spectrometry': 1\n",
    "    }\n",
    "    \n",
    "    type_scores = {\n",
    "        'direct interaction': 3,\n",
    "        'physical association': 1,\n",
    "        'complex': 2\n",
    "    }\n",
    "    \n",
    "    clean_df['Pub_Count'] = clean_df['PubIDs'].str.count(r'\\|') + 1\n",
    "    \n",
    "    clean_df['Method_Score'] = clean_df['Method'].map(\n",
    "        lambda x: max([method_scores.get(method.lower(), 1) \n",
    "                      for method in str(x).split('|')])\n",
    "    )\n",
    "    clean_df['Type_Score'] = clean_df['IntType'].map(\n",
    "        lambda x: max([type_scores.get(typ.lower(), 1) \n",
    "                      for typ in str(x).split('|')])\n",
    "    )\n",
    "    clean_df['Total_Score'] = (\n",
    "        clean_df['Method_Score'] * 3 + \n",
    "        clean_df['Type_Score'] * 2 + \n",
    "        clean_df['Pub_Count']\n",
    "    )\n",
    "    \n",
    "    # 5. Supprimer les doublons (même paire de protéines dans un ordre différent)\n",
    "    # Créer une colonne avec la paire triée pour identifier les doublons\n",
    "    clean_df['SortedPair'] = clean_df.apply(lambda x: tuple(sorted([x['Protein1'], x['Protein2']])), axis=1)\n",
    "    \n",
    "    # Garder toutes les interactions uniques (sans filtrage par score)\n",
    "    non_redundant = clean_df.drop_duplicates(subset=['SortedPair'])\n",
    "    \n",
    "    # 6. Statistiques finales\n",
    "    unique_proteins = pd.unique(\n",
    "        non_redundant[['Protein1', 'Protein2']].values.ravel('K')\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nRésultats finaux :\")\n",
    "    print(f\"- Interactions humaines totales : {len(human_df)}\")\n",
    "    print(f\"- Interactions avec UniProt valides : {len(clean_df)}\")\n",
    "    print(f\"- Interactions non-redondantes : {len(non_redundant)}\")\n",
    "    print(f\"- Protéines uniques : {len(unique_proteins)}\")\n",
    "    print(f\"- Score moyen (pour information) : {non_redundant['Total_Score'].mean():.1f}\")\n",
    "    \n",
    "    # 7. Sauvegarde (seulement les deux colonnes Protein1 et Protein2)\n",
    "    non_redundant[['Protein1', 'Protein2']].to_csv(\n",
    "        output_file, \n",
    "        sep='\\t', \n",
    "        index=False, \n",
    "        header=False  # Pas d'en-tête dans le fichier de sortie\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_biogrid_max_coverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Base de donnees (Complex Portal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\Portal_complexes_levure.txt\n",
      "==================================================\n",
      "\n",
      "STATISTIQUES GLOBALES:\n",
      "- Nombre total de complexes: 643\n",
      "- Nombre de protéines uniques: 1862\n",
      "- Nombre moyen de protéines par complexe: 4.28\n",
      "\n",
      "DISTRIBUTION DES TAILLES DE COMPLEXES:\n",
      "- Complexes de taille 1: 21 (3.3%)\n",
      "- Complexes de taille 2: 244 (37.9%)\n",
      "- Complexes de taille 3: 130 (20.2%)\n",
      "- Complexes de taille 4: 83 (12.9%)\n",
      "- Complexes de taille 5: 48 (7.5%)\n",
      "- Complexes de taille 6: 34 (5.3%)\n",
      "- Complexes de taille 7: 19 (3.0%)\n",
      "- Complexes de taille 8: 13 (2.0%)\n",
      "- Complexes de taille 9: 4 (0.6%)\n",
      "- Complexes de taille 10: 9 (1.4%)\n",
      "- Complexes de taille 11: 4 (0.6%)\n",
      "- Complexes de taille 12: 2 (0.3%)\n",
      "- Complexes de taille 13: 2 (0.3%)\n",
      "- Complexes de taille 14: 6 (0.9%)\n",
      "- Complexes de taille 15: 3 (0.5%)\n",
      "- Complexes de taille 16: 5 (0.8%)\n",
      "- Complexes de taille 17: 4 (0.6%)\n",
      "- Complexes de taille 18: 1 (0.2%)\n",
      "- Complexes de taille 19: 1 (0.2%)\n",
      "- Complexes de taille 20: 1 (0.2%)\n",
      "- Complexes de taille 21: 1 (0.2%)\n",
      "- Complexes de taille 22: 1 (0.2%)\n",
      "- Complexes de taille 27: 1 (0.2%)\n",
      "- Complexes de taille 29: 1 (0.2%)\n",
      "- Complexes de taille 33: 1 (0.2%)\n",
      "- Complexes de taille 34: 2 (0.3%)\n",
      "- Complexes de taille 43: 1 (0.2%)\n",
      "- Complexes de taille 46: 1 (0.2%)\n",
      "\n",
      "TOP 10 DES PROTÉINES LES PLUS FRÉQUENTES:\n",
      "- Q08273: présente dans 16 complexes (2.5%)\n",
      "- P52286: présente dans 13 complexes (2.0%)\n",
      "- Q12018: présente dans 11 complexes (1.7%)\n",
      "- P14682: présente dans 11 complexes (1.7%)\n",
      "- P54999: présente dans 10 complexes (1.6%)\n",
      "- P40018: présente dans 10 complexes (1.6%)\n",
      "- Q02260: présente dans 10 complexes (1.6%)\n",
      "- P43321: présente dans 10 complexes (1.6%)\n",
      "- Q06217: présente dans 10 complexes (1.6%)\n",
      "- P17157: présente dans 10 complexes (1.6%)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "from xml.etree import ElementTree as ET\n",
    "from collections import defaultdict\n",
    "\n",
    "# Chemins\n",
    "zip_path = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Complexes\\yeast.zip\"\n",
    "extract_folder = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Complexes\\yeast_extracted\"\n",
    "output_file = r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\Portal_complexes_levure.txt\"\n",
    "\n",
    "# 1. Nettoyage et extraction\n",
    "if os.path.exists(extract_folder):\n",
    "    shutil.rmtree(extract_folder)\n",
    "os.makedirs(extract_folder, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "# 2. Traitement des fichiers XML\n",
    "complexes = []\n",
    "all_proteins = set()\n",
    "protein_to_complexes = defaultdict(list)\n",
    "\n",
    "yeast_folder = os.path.join(extract_folder, \"yeast\")\n",
    "\n",
    "for filename in os.listdir(yeast_folder):\n",
    "    if filename.endswith(\".xml\"):\n",
    "        filepath = os.path.join(yeast_folder, filename)\n",
    "        \n",
    "        try:\n",
    "            tree = ET.parse(filepath)\n",
    "            root = tree.getroot()\n",
    "            ns = {'mif': 'http://psi.hupo.org/mi/mif300'}\n",
    "            \n",
    "            for interaction in root.findall(\".//mif:abstractInteraction\", ns):\n",
    "                proteins = set()\n",
    "                \n",
    "                for participant in interaction.findall(\".//mif:participant\", ns):\n",
    "                    interactor_ref = participant.find(\".//mif:interactorRef\", ns)\n",
    "                    if interactor_ref is not None:\n",
    "                        interactor = root.find(f\".//mif:interactor[@id='{interactor_ref.text}']\", ns)\n",
    "                        if interactor is not None:\n",
    "                            interactor_type = interactor.find(\".//mif:interactorType/mif:names/mif:shortLabel\", ns)\n",
    "                            if interactor_type is not None and interactor_type.text == \"protein\":\n",
    "                                uniprot = interactor.find(\".//mif:xref/mif:primaryRef[@db='uniprotkb']\", ns)\n",
    "                                if uniprot is not None:\n",
    "                                    protein_id = uniprot.get(\"id\")\n",
    "                                    proteins.add(protein_id)\n",
    "                                    all_proteins.add(protein_id)\n",
    "                \n",
    "                if proteins:\n",
    "                    complex_id = f\"complex_{len(complexes)+1}\"\n",
    "                    complexes.append((complex_id, proteins))\n",
    "                    for protein in proteins:\n",
    "                        protein_to_complexes[protein].append(complex_id)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {filename}: {str(e)[:200]}\")\n",
    "\n",
    "# 3. Écriture du fichier final\n",
    "with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    for complex_id, proteins in complexes:\n",
    "        f_out.write(f\"{complex_id}\\t{' '.join(sorted(proteins))}\\n\")\n",
    "\n",
    "# 4. Calcul des statistiques\n",
    "num_complexes = len(complexes)\n",
    "num_unique_proteins = len(all_proteins)\n",
    "\n",
    "# Distribution des tailles de complexes\n",
    "size_distribution = defaultdict(int)\n",
    "for _, proteins in complexes:\n",
    "    size_distribution[len(proteins)] += 1\n",
    "\n",
    "# Protéines les plus fréquentes\n",
    "protein_frequency = {protein: len(complexes) for protein, complexes in protein_to_complexes.items()}\n",
    "top_proteins = sorted(protein_frequency.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# 5. Affichage des résultats\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Fichier généré: {output_file}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nSTATISTIQUES GLOBALES:\")\n",
    "print(f\"- Nombre total de complexes: {num_complexes}\")\n",
    "print(f\"- Nombre de protéines uniques: {num_unique_proteins}\")\n",
    "print(f\"- Nombre moyen de protéines par complexe: {sum(len(p) for _, p in complexes)/num_complexes:.2f}\")\n",
    "\n",
    "print(\"\\nDISTRIBUTION DES TAILLES DE COMPLEXES:\")\n",
    "for size, count in sorted(size_distribution.items()):\n",
    "    print(f\"- Complexes de taille {size}: {count} ({(count/num_complexes)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nTOP 10 DES PROTÉINES LES PLUS FRÉQUENTES:\")\n",
    "for protein, freq in top_proteins:\n",
    "    print(f\"- {protein}: présente dans {freq} complexes ({(freq/num_complexes)*100:.1f}%)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. filtrage de complexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de complexes chargés: 643\n",
      "\n",
      "Traitement de weighted_STRING_levure.txt...\n",
      "- Protéines uniques dans PPI: 4,935\n",
      "- Interactions dans PPI: 76,901\n",
      "- Complexes analysés: 643\n",
      "- Complexes conservés: 520 (80.9%)\n",
      "  - Rejetés (protéines manquantes): 110\n",
      "  - Rejetés (non connectés): 13\n",
      "- Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_STRING_levure.txt\n",
      "\n",
      "Traitement de weighted_DIP_levure.txt...\n",
      "- Protéines uniques dans PPI: 3,973\n",
      "- Interactions dans PPI: 15,376\n",
      "- Complexes analysés: 643\n",
      "- Complexes conservés: 358 (55.7%)\n",
      "  - Rejetés (protéines manquantes): 188\n",
      "  - Rejetés (non connectés): 97\n",
      "- Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_DIP_levure.txt\n",
      "\n",
      "Traitement de weighted_BIOGRID_levure.txt...\n",
      "- Protéines uniques dans PPI: 5,241\n",
      "- Interactions dans PPI: 200,897\n",
      "- Complexes analysés: 643\n",
      "- Complexes conservés: 511 (79.5%)\n",
      "  - Rejetés (protéines manquantes): 110\n",
      "  - Rejetés (non connectés): 22\n",
      "- Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\complexes_BIOGRID_levure.txt\n",
      "\n",
      "Terminé avec succès !\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "\n",
    "def load_complexes(file_path):\n",
    "    \"\"\"Charge les complexes depuis le fichier et retourne une liste de tuples (id_complexe, set de protéines)\"\"\"\n",
    "    complexes = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) >= 2:\n",
    "                complex_id = parts[0]\n",
    "                proteins = set(p.strip() for p in parts[1].split() if p.strip())\n",
    "                complexes.append((complex_id, proteins))\n",
    "    return complexes\n",
    "\n",
    "def load_ppi_network(file_path):\n",
    "    \"\"\"Charge le réseau PPI et retourne un set de protéines et le graphe PPI\"\"\"\n",
    "    proteins = set()\n",
    "    graph = defaultdict(set)\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\") or line.startswith(\"protein1\"):\n",
    "                continue\n",
    "            parts = line.split('\\t') if '\\t' in line else line.split()\n",
    "            if len(parts) >= 2:\n",
    "                p1, p2 = parts[0].strip(), parts[1].strip()\n",
    "                if p1 and p2:\n",
    "                    proteins.add(p1)\n",
    "                    proteins.add(p2)\n",
    "                    graph[p1].add(p2)\n",
    "                    graph[p2].add(p1)\n",
    "    return proteins, graph\n",
    "\n",
    "def is_single_connected_component(proteins, ppi_graph):\n",
    "    \"\"\"Vérifie si les protéines forment un seul composant connecté dans le réseau PPI\"\"\"\n",
    "    if not proteins:\n",
    "        return False\n",
    "    \n",
    "    visited = set()\n",
    "    queue = deque()\n",
    "    \n",
    "    start_protein = next(iter(proteins))\n",
    "    queue.append(start_protein)\n",
    "    visited.add(start_protein)\n",
    "    \n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        for neighbor in ppi_graph[current]:\n",
    "            if neighbor in proteins and neighbor not in visited:\n",
    "                visited.add(neighbor)\n",
    "                queue.append(neighbor)\n",
    "    \n",
    "    return visited == proteins\n",
    "\n",
    "def filter_complexes(complexes, ppi_proteins, ppi_graph, output_file):\n",
    "    \"\"\"Filtre les complexes et sauvegarde ceux valides\"\"\"\n",
    "    stats = {\n",
    "        'total': 0,\n",
    "        'kept': 0,\n",
    "        'missing_proteins': 0,\n",
    "        'disconnected': 0\n",
    "    }\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "        for complex_id, proteins in complexes:\n",
    "            stats['total'] += 1\n",
    "            \n",
    "            # Vérifier que toutes les protéines sont dans le PPI\n",
    "            if not all(p in ppi_proteins for p in proteins):\n",
    "                stats['missing_proteins'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Vérifier la connectivité\n",
    "            if not is_single_connected_component(proteins, ppi_graph):\n",
    "                stats['disconnected'] += 1\n",
    "                continue\n",
    "            \n",
    "            # Écrire le complexe valide\n",
    "            f_out.write(f\"{complex_id}\\t{' '.join(proteins)}\\n\")\n",
    "            stats['kept'] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "def main():\n",
    "    # Configuration des chemins\n",
    "    base_dir = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\")\n",
    "    \n",
    "    # Fichiers d'entrée/sortie\n",
    "    complexes_file = base_dir / \"complexes\" / \"complexes_levure.txt\"\n",
    "    reseau_files = [\n",
    "        base_dir / \"weighted_networks\" / \"weighted_STRING_levure.txt\",\n",
    "        base_dir / \"weighted_networks\" / \"weighted_DIP_levure.txt\",\n",
    "        base_dir / \"weighted_networks\" / \"weighted_BIOGRID_levure.txt\"\n",
    "    ]\n",
    "    output_files = [\n",
    "        base_dir / \"complexes\" / \"complexes_STRING_levure.txt\",\n",
    "        base_dir / \"complexes\" / \"complexes_DIP_levure.txt\",\n",
    "        base_dir / \"complexes\" / \"complexes_BIOGRID_levure.txt\"\n",
    "    ]\n",
    "\n",
    "    # Charger les complexes\n",
    "    complexes = load_complexes(complexes_file)\n",
    "    print(f\"Nombre total de complexes chargés: {len(complexes)}\")\n",
    "\n",
    "    # Traiter chaque réseau PPI\n",
    "    for ppi_file, out_file in zip(reseau_files, output_files):\n",
    "        print(f\"\\nTraitement de {ppi_file.name}...\")\n",
    "        \n",
    "        try:\n",
    "            # Charger le réseau PPI\n",
    "            ppi_proteins, ppi_graph = load_ppi_network(ppi_file)\n",
    "            print(f\"- Protéines uniques dans PPI: {len(ppi_proteins):,}\")\n",
    "            print(f\"- Interactions dans PPI: {sum(len(v) for v in ppi_graph.values())//2:,}\")\n",
    "\n",
    "            # Filtrer les complexes\n",
    "            stats = filter_complexes(complexes, ppi_proteins, ppi_graph, out_file)\n",
    "            \n",
    "            # Afficher les statistiques\n",
    "            print(f\"- Complexes analysés: {stats['total']:,}\")\n",
    "            print(f\"- Complexes conservés: {stats['kept']:,} ({stats['kept']/stats['total']*100:.1f}%)\")\n",
    "            print(f\"  - Rejetés (protéines manquantes): {stats['missing_proteins']:,}\")\n",
    "            print(f\"  - Rejetés (non connectés): {stats['disconnected']:,}\")\n",
    "            print(f\"- Fichier généré: {out_file}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur avec {ppi_file}: {str(e)}\")\n",
    "\n",
    "    print(\"\\nTerminé avec succès !\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.1. Corum + hu.MAP( filtrage de complexes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CORUM] Complexes: 5366 | Protéines uniques: 5205\n",
      "[Hu.MAP] Complexes: 6965 | Protéines uniques: 9962\n",
      "\n",
      "[Union] Complexes non redondants : 12154\n",
      "[Union] Protéines uniques totales : 11433\n",
      "Fichier sauvegardé : C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\merged_unique_complexes.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def process_corum(file_path):\n",
    "    complexes = {}\n",
    "    all_proteins = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for row in reader:\n",
    "            complex_id = \"CORUM_\" + row['complex_id']\n",
    "            proteins = row['subunits_uniprot_id']\n",
    "            if proteins:\n",
    "                protein_list = sorted(set(p.strip() for p in proteins.split(';') if p.strip()))\n",
    "                complexes[complex_id] = protein_list\n",
    "                all_proteins.update(protein_list)\n",
    "    print(f\"[CORUM] Complexes: {len(complexes)} | Protéines uniques: {len(all_proteins)}\")\n",
    "    return complexes, all_proteins\n",
    "\n",
    "def process_humap(file_path):\n",
    "    complexes = {}\n",
    "    all_proteins = set()\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            complex_id = \"HuMAP_\" + row['HuMAP2_ID']\n",
    "            proteins = row['Uniprot_ACCs']\n",
    "            if proteins:\n",
    "                protein_list = sorted(set(p.strip() for p in proteins.split(' ') if p.strip()))\n",
    "                complexes[complex_id] = protein_list\n",
    "                all_proteins.update(protein_list)\n",
    "    print(f\"[Hu.MAP] Complexes: {len(complexes)} | Protéines uniques: {len(all_proteins)}\")\n",
    "    return complexes, all_proteins\n",
    "\n",
    "def export_unique_complexes(complexes_dict, output_path):\n",
    "    seen = set()\n",
    "    unique_complexes = []\n",
    "\n",
    "    for complex_id, proteins in complexes_dict.items():\n",
    "        protein_tuple = tuple(sorted(proteins))\n",
    "        if protein_tuple not in seen:\n",
    "            seen.add(protein_tuple)\n",
    "            unique_complexes.append((complex_id, protein_tuple))\n",
    "\n",
    "    with open(output_path, 'w', encoding='utf-8', newline='') as f:\n",
    "        writer = csv.writer(f, delimiter='\\t')\n",
    "        writer.writerow(['complex_id', 'proteins'])\n",
    "        for complex_id, protein_tuple in unique_complexes:\n",
    "            writer.writerow([complex_id, ';'.join(protein_tuple)])\n",
    "\n",
    "    all_proteins = set(p for _, proteins in unique_complexes for p in proteins)\n",
    "    print(f\"\\n[Union] Complexes non redondants : {len(unique_complexes)}\")\n",
    "    print(f\"[Union] Protéines uniques totales : {len(all_proteins)}\")\n",
    "    print(f\"Fichier sauvegardé : {output_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    corum_file = r'C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Complexes\\corum_humanComplexes.txt'\n",
    "    humap_file = r'C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\Complexes\\hu.MAP_complexes.txt'\n",
    "    output_file = r'C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\merged_unique_complexes.txt'\n",
    "\n",
    "    corum_complexes, corum_proteins = process_corum(corum_file)\n",
    "    humap_complexes, humap_proteins = process_humap(humap_file)\n",
    "\n",
    "    all_complexes = {**corum_complexes, **humap_complexes}\n",
    "    export_unique_complexes(all_complexes, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6.2. filtrage des complexes (CORUM + hu.MAP -> BIOGRID) & (CORUM + hu.MAP-> STRING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Traitement du réseau BIOGRID...\n",
      "- Protéines uniques dans PPI: 9,173\n",
      "- Interactions dans PPI: 66,374\n",
      "- Complexes analysés: 12,155\n",
      "- Complexes conservés: 1,480 (12.2%)\n",
      "  - Rejetés (taille < 3): 4,889\n",
      "  - Rejetés (protéines manquantes): 4,435\n",
      "  - Rejetés (non connectés): 1,351\n",
      "- Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\BIOGRID_complexes_humain.txt\n",
      "\n",
      "Traitement du réseau STRING...\n",
      "- Protéines uniques dans PPI: 13,018\n",
      "- Interactions dans PPI: 166,002\n",
      "- Complexes analysés: 12,155\n",
      "- Complexes conservés: 1,859 (15.3%)\n",
      "  - Rejetés (taille < 3): 4,889\n",
      "  - Rejetés (protéines manquantes): 3,677\n",
      "  - Rejetés (non connectés): 1,730\n",
      "- Fichier généré: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\complexes\\STRING_complexes_humain.txt\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, deque\n",
    "\n",
    "def load_ppi_network(ppi_file):\n",
    "    \"\"\"Charge le réseau PPI et retourne un set de protéines uniques et le graphe PPI\"\"\"\n",
    "    proteins = set()\n",
    "    graph = defaultdict(set)\n",
    "    try:\n",
    "        with open(ppi_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            for row in reader:\n",
    "                if len(row) >= 2:\n",
    "                    p1, p2 = row[0].strip(), row[1].strip()\n",
    "                    if p1 and p2:\n",
    "                        proteins.add(p1)\n",
    "                        proteins.add(p2)\n",
    "                        graph[p1].add(p2)\n",
    "                        graph[p2].add(p1)\n",
    "        return proteins, graph\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lecture {ppi_file}: {str(e)}\")\n",
    "        return set(), defaultdict(set)\n",
    "\n",
    "def is_single_connected_component(proteins, ppi_graph):\n",
    "    \"\"\"Vérifie si les protéines forment un seul composant connecté dans le réseau PPI\"\"\"\n",
    "    if len(proteins) < 2:  # Un complexe d'une seule protéine est toujours connecté\n",
    "        return True\n",
    "    \n",
    "    visited = set()\n",
    "    queue = deque()\n",
    "    \n",
    "    start_protein = next(iter(proteins))\n",
    "    queue.append(start_protein)\n",
    "    visited.add(start_protein)\n",
    "    \n",
    "    while queue:\n",
    "        current = queue.popleft()\n",
    "        for neighbor in ppi_graph[current]:\n",
    "            if neighbor in proteins and neighbor not in visited:\n",
    "                visited.add(neighbor)\n",
    "                queue.append(neighbor)\n",
    "    \n",
    "    return visited == proteins\n",
    "\n",
    "def filter_complexes(complexes_file, ppi_proteins, ppi_graph, output_file):\n",
    "    \"\"\"Filtre les complexes selon les critères spécifiés\"\"\"\n",
    "    stats = {\n",
    "        'total': 0,\n",
    "        'kept': 0,\n",
    "        'small': 0,\n",
    "        'missing_proteins': 0,\n",
    "        'disconnected': 0\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(complexes_file, 'r', encoding='utf-8') as f_in, \\\n",
    "             open(output_file, 'w', encoding='utf-8', newline='') as f_out:\n",
    "            \n",
    "            writer = csv.writer(f_out, delimiter='\\t')\n",
    "            writer.writerow(['complex_id', 'proteins'])\n",
    "            \n",
    "            for row in csv.reader(f_in, delimiter='\\t'):\n",
    "                if len(row) < 2:\n",
    "                    continue\n",
    "                \n",
    "                stats['total'] += 1\n",
    "                complex_id, proteins_str = row[0].strip(), row[1].strip()\n",
    "                proteins = [p.strip() for p in proteins_str.split(';') if p.strip()]\n",
    "                proteins_set = set(proteins)\n",
    "                \n",
    "                # Vérifier la taille du complexe\n",
    "                if len(proteins_set) < 3:\n",
    "                    stats['small'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Vérifier que toutes les protéines sont dans le PPI\n",
    "                if not all(p in ppi_proteins for p in proteins_set):\n",
    "                    stats['missing_proteins'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Vérifier la connectivité\n",
    "                if not is_single_connected_component(proteins_set, ppi_graph):\n",
    "                    stats['disconnected'] += 1\n",
    "                    continue\n",
    "                \n",
    "                # Écrire le complexe valide\n",
    "                writer.writerow([complex_id, ';'.join(proteins)])\n",
    "                stats['kept'] += 1\n",
    "        \n",
    "        return stats\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur traitement {complexes_file}: {str(e)}\")\n",
    "        return {'total': 0, 'kept': 0, 'small': 0, 'missing_proteins': 0, 'disconnected': 0}\n",
    "\n",
    "def process_ppi_network(ppi_file, complexes_file, output_file, network_name):\n",
    "    \"\"\"Processus complet pour un réseau PPI\"\"\"\n",
    "    print(f\"\\nTraitement du réseau {network_name}...\")\n",
    "    \n",
    "    # Charger les protéines PPI et le graphe\n",
    "    ppi_proteins, ppi_graph = load_ppi_network(ppi_file)\n",
    "    if not ppi_proteins:\n",
    "        print(f\"Échec: Aucune protéine chargée depuis {ppi_file}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"- Protéines uniques dans PPI: {len(ppi_proteins):,}\")\n",
    "    print(f\"- Interactions dans PPI: {sum(len(v) for v in ppi_graph.values())//2:,}\")\n",
    "\n",
    "    # Filtrer les complexes\n",
    "    stats = filter_complexes(complexes_file, ppi_proteins, ppi_graph, output_file)\n",
    "    \n",
    "    if stats['total'] == 0:\n",
    "        print(\"Échec: Aucun complexe traité\")\n",
    "        return\n",
    "    \n",
    "    # Afficher les statistiques\n",
    "    print(f\"- Complexes analysés: {stats['total']:,}\")\n",
    "    print(f\"- Complexes conservés: {stats['kept']:,} ({stats['kept']/stats['total']*100:.1f}%)\")\n",
    "    print(f\"  - Rejetés (taille < 3): {stats['small']:,}\")\n",
    "    print(f\"  - Rejetés (protéines manquantes): {stats['missing_proteins']:,}\")\n",
    "    print(f\"  - Rejetés (non connectés): {stats['disconnected']:,}\")\n",
    "    print(f\"- Fichier généré: {output_file}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configuration des chemins\n",
    "    DATA_DIR = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\")\n",
    "    \n",
    "    # Fichier de complexes source\n",
    "    CORUM_FILE = DATA_DIR / \"complexes\" / \"merged_unique_complexes.txt\"\n",
    "    \n",
    "    # Traitement BIOGRID\n",
    "    process_ppi_network(\n",
    "        ppi_file=DATA_DIR / \"weighted_networks\" / \"weighted_BIOGRID_humain.txt\",\n",
    "        complexes_file=CORUM_FILE,\n",
    "        output_file=DATA_DIR / \"complexes\" / \"BIOGRID_complexes_humain.txt\",\n",
    "        network_name=\"BIOGRID\"\n",
    "    )\n",
    "    \n",
    "    # Traitement STRING\n",
    "    process_ppi_network(\n",
    "        ppi_file=DATA_DIR / \"weighted_networks\" / \"weighted_STRING_humain.txt\",\n",
    "        complexes_file=CORUM_FILE,\n",
    "        output_file=DATA_DIR / \"complexes\" / \"STRING_complexes_humain.txt\",\n",
    "        network_name=\"STRING\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Normalisation de subcellular localization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vérification des fichiers...\n",
      "- Human mapping: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\HUMAN_9606_idmapping.dat (OK)\n",
      "- Yeast mapping: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\YEAST_559292_idmapping.dat (OK)\n",
      "- Human compartment: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\human_compartment_integrated_full.tsv (OK)\n",
      "- Yeast compartment: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\yeast_compartment_integrated_full.tsv (OK)\n",
      "\n",
      "Dossier de sortie: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\n",
      "\n",
      "==================================================\n",
      "CHARGEMENT DES FICHIERS DE MAPPING\n",
      "==================================================\n",
      "Chargement du fichier de mapping: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\HUMAN_9606_idmapping.dat\n",
      "Ligne 100000 traitée...\n",
      "Ligne 200000 traitée...\n",
      "Ligne 300000 traitée...\n",
      "Ligne 400000 traitée...\n",
      "Ligne 500000 traitée...\n",
      "Ligne 600000 traitée...\n",
      "Ligne 700000 traitée...\n",
      "Ligne 800000 traitée...\n",
      "Ligne 900000 traitée...\n",
      "Ligne 1000000 traitée...\n",
      "Ligne 1100000 traitée...\n",
      "Ligne 1200000 traitée...\n",
      "Ligne 1300000 traitée...\n",
      "Ligne 1400000 traitée...\n",
      "Ligne 1500000 traitée...\n",
      "Ligne 1600000 traitée...\n",
      "Ligne 1700000 traitée...\n",
      "Ligne 1800000 traitée...\n",
      "Ligne 1900000 traitée...\n",
      "Ligne 2000000 traitée...\n",
      "Ligne 2100000 traitée...\n",
      "Ligne 2200000 traitée...\n",
      "Ligne 2300000 traitée...\n",
      "Ligne 2400000 traitée...\n",
      "Ligne 2500000 traitée...\n",
      "Ligne 2600000 traitée...\n",
      "Ligne 2700000 traitée...\n",
      "Ligne 2800000 traitée...\n",
      "Ligne 2900000 traitée...\n",
      "Ligne 3000000 traitée...\n",
      "Ligne 3100000 traitée...\n",
      "Ligne 3200000 traitée...\n",
      "Ligne 3300000 traitée...\n",
      "Ligne 3400000 traitée...\n",
      "Ligne 3500000 traitée...\n",
      "Ligne 3600000 traitée...\n",
      "Ligne 3700000 traitée...\n",
      "Ligne 3800000 traitée...\n",
      "Ligne 3900000 traitée...\n",
      "Ligne 4000000 traitée...\n",
      "Ligne 4100000 traitée...\n",
      "Ligne 4200000 traitée...\n",
      "Ligne 4300000 traitée...\n",
      "Ligne 4400000 traitée...\n",
      "Ligne 4500000 traitée...\n",
      "Ligne 4600000 traitée...\n",
      "Ligne 4700000 traitée...\n",
      "Ligne 4800000 traitée...\n",
      "Ligne 4900000 traitée...\n",
      "Ligne 5000000 traitée...\n",
      "Ligne 5100000 traitée...\n",
      "Ligne 5200000 traitée...\n",
      "Chargement du fichier de mapping: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\YEAST_559292_idmapping.dat\n",
      "Ligne 100000 traitée...\n",
      "Ligne 200000 traitée...\n",
      "Ligne 300000 traitée...\n",
      "\n",
      "Temps de chargement: 15.36 secondes\n",
      "Protéines humaines chargées: 237779\n",
      "Protéines yeast chargées: 6900\n",
      "\n",
      "==================================================\n",
      "TRAITEMENT DES DONNÉES HUMAINES\n",
      "==================================================\n",
      "\n",
      "Début du traitement du fichier: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\human_compartment_integrated_full.tsv\n",
      "Chargement des données de compartiments...\n",
      "Nombre d'entrées à traiter: 4854519\n",
      "Préparation des structures de recherche...\n",
      "Application du mapping...\n",
      "Extraction des résultats...\n",
      "Sauvegarde des résultats dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\human_compartment_mapped.txt...\n",
      "382276 protéines non mappées sauvegardées dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\human_compartment_mapped_unmapped.txt\n",
      "Traitement terminé avec succès!\n",
      "\n",
      "==================================================\n",
      "TRAITEMENT DES DONNÉES YEAST\n",
      "==================================================\n",
      "\n",
      "Début du traitement du fichier: C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\raw data\\autres\\yeast_compartment_integrated_full.tsv\n",
      "Chargement des données de compartiments...\n",
      "Nombre d'entrées à traiter: 745850\n",
      "Préparation des structures de recherche...\n",
      "Application du mapping...\n",
      "Extraction des résultats...\n",
      "Sauvegarde des résultats dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\yeast_compartment_mapped.txt...\n",
      "28325 protéines non mappées sauvegardées dans C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\\clean data\\autres\\yeast_compartment_mapped_unmapped.txt\n",
      "Traitement terminé avec succès!\n",
      "\n",
      "==================================================\n",
      "RÉSULTATS FINAUX\n",
      "==================================================\n",
      "HUMAIN - Total: 4854519, Mappés: 4472243 (92.1%)\n",
      "YEAST  - Total: 745850, Mappés: 717525 (96.2%)\n",
      "\n",
      "Temps total d'exécution: 109.31 secondes\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "def load_mapping_file(file_path):\n",
    "    \"\"\"Charge le fichier de mapping de manière optimisée\"\"\"\n",
    "    mapping = defaultdict(dict)\n",
    "    id_conversion = defaultdict(dict)\n",
    "    \n",
    "    # Pré-compiler les regex pour plus de performance\n",
    "    yeast_pattern = re.compile(r'^Y[A-Z]{2}\\d{3}[A-Z]?$')\n",
    "    \n",
    "    print(f\"Chargement du fichier de mapping: {file_path}\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in enumerate(f, 1):\n",
    "            if i % 100000 == 0:\n",
    "                print(f\"Ligne {i} traitée...\")\n",
    "                \n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 3:\n",
    "                continue\n",
    "                \n",
    "            uniprot_id, id_type, id_value = parts[0], parts[1], parts[2]\n",
    "            \n",
    "            # Stockage optimisé des données\n",
    "            if id_type not in mapping[uniprot_id]:\n",
    "                mapping[uniprot_id][id_type] = []\n",
    "            mapping[uniprot_id][id_type].append(id_value)\n",
    "            \n",
    "            # Conversion inversée optimisée\n",
    "            if id_type == 'STRING' and 'ENSP' in id_value:\n",
    "                ensembl_id = id_value.split('.')[-1]\n",
    "                id_conversion['Ensembl'][ensembl_id] = uniprot_id\n",
    "            elif id_type == 'Gene_OrderedLocusName' and yeast_pattern.match(id_value):\n",
    "                id_conversion['Yeast_Locus'][id_value] = uniprot_id\n",
    "            elif id_type == 'Ensembl_PRO':\n",
    "                id_conversion['Ensembl_PRO'][id_value] = uniprot_id\n",
    "    \n",
    "    return dict(mapping), dict(id_conversion)\n",
    "\n",
    "def map_compartment_data(compartment_file, mapping_data, id_conversion, output_file, species):\n",
    "    \"\"\"Mappe les données de compartiment de manière optimisée\"\"\"\n",
    "    print(f\"\\nDébut du traitement du fichier: {compartment_file}\")\n",
    "    \n",
    "    # Charger le fichier de compartiments\n",
    "    print(\"Chargement des données de compartiments...\")\n",
    "    compartment_df = pd.read_csv(\n",
    "        compartment_file, \n",
    "        sep='\\t', \n",
    "        header=None,\n",
    "        names=['Protein_ID', 'Gene_Name', 'GO_ID', 'GO_Term', 'Score'],\n",
    "        dtype={'Protein_ID': 'string', 'Gene_Name': 'string', \n",
    "               'GO_ID': 'string', 'GO_Term': 'string'},\n",
    "        on_bad_lines='warn'\n",
    "    )\n",
    "    \n",
    "    print(f\"Nombre d'entrées à traiter: {len(compartment_df)}\")\n",
    "    \n",
    "    # Préparer les structures pour une recherche rapide\n",
    "    print(\"Préparation des structures de recherche...\")\n",
    "    string_map = {}\n",
    "    if species == 'human':\n",
    "        string_map = {v.split('.')[-1]: k for k in mapping_data for v in mapping_data[k].get('STRING', [])}\n",
    "    \n",
    "    # Fonction de mapping optimisée\n",
    "    def map_id(protein_id):\n",
    "        # 1. Essayer le mapping direct\n",
    "        if protein_id in mapping_data:\n",
    "            return protein_id, mapping_data[protein_id]\n",
    "        \n",
    "        # 2. Essayer les conversions spécifiques\n",
    "        if species == 'human' and protein_id.startswith('ENSP'):\n",
    "            # Via STRING ID\n",
    "            if protein_id in string_map:\n",
    "                uniprot_id = string_map[protein_id]\n",
    "                return uniprot_id, mapping_data.get(uniprot_id, {})\n",
    "            # Via Ensembl_PRO direct\n",
    "            if protein_id in id_conversion.get('Ensembl_PRO', {}):\n",
    "                uniprot_id = id_conversion['Ensembl_PRO'][protein_id]\n",
    "                return uniprot_id, mapping_data.get(uniprot_id, {})\n",
    "                \n",
    "        elif species == 'yeast':\n",
    "            # Via Yeast Locus ID\n",
    "            if protein_id in id_conversion.get('Yeast_Locus', {}):\n",
    "                uniprot_id = id_conversion['Yeast_Locus'][protein_id]\n",
    "                return uniprot_id, mapping_data.get(uniprot_id, {})\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    # Appliquer le mapping\n",
    "    print(\"Application du mapping...\")\n",
    "    compartment_df['Mapped_Info'] = compartment_df['Protein_ID'].apply(map_id)\n",
    "    \n",
    "    # Extraire les résultats\n",
    "    print(\"Extraction des résultats...\")\n",
    "    compartment_df['Mapped_UniProtKB_ID'] = compartment_df['Mapped_Info'].apply(lambda x: x[0] if x and x[0] else '')\n",
    "    compartment_df['Mapping_Status'] = compartment_df['Mapped_UniProtKB_ID'].apply(lambda x: 'Success' if x else 'Failed')\n",
    "    \n",
    "    # Remplir les colonnes mappées\n",
    "    for field in ['GeneID', 'Ensembl', 'RefSeq', 'Entrez_Gene', 'STRING_ID']:\n",
    "        compartment_df[field] = compartment_df['Mapped_Info'].apply(\n",
    "            lambda x: x[1].get(field, [''])[0] if x and x[1] else ''\n",
    "        )\n",
    "    \n",
    "    # Sélectionner les colonnes finales\n",
    "    result_cols = {\n",
    "        'Protein_ID': 'Original_Protein_ID',\n",
    "        'Gene_Name': 'Gene_Name',\n",
    "        'GO_ID': 'GO_ID',\n",
    "        'GO_Term': 'GO_Term',\n",
    "        'Score': 'Score',\n",
    "        'Mapped_UniProtKB_ID': 'Mapped_UniProtKB_ID',\n",
    "        'GeneID': 'GeneID',\n",
    "        'Ensembl': 'Ensembl',\n",
    "        'RefSeq': 'RefSeq',\n",
    "        'Entrez_Gene': 'Entrez_Gene',\n",
    "        'STRING_ID': 'STRING_ID',\n",
    "        'Mapping_Status': 'Mapping_Status'\n",
    "    }\n",
    "    \n",
    "    result_df = compartment_df[list(result_cols.keys())].rename(columns=result_cols)\n",
    "    \n",
    "    # Sauvegarder les résultats\n",
    "    print(f\"Sauvegarde des résultats dans {output_file}...\")\n",
    "    result_df.to_csv(output_file, sep='\\t', index=False)\n",
    "    \n",
    "    # Calculer les statistiques\n",
    "    total = len(result_df)\n",
    "    mapped = (result_df['Mapping_Status'] == 'Success').sum()\n",
    "    stats = {'total': total, 'mapped': mapped, 'unmapped': total - mapped}\n",
    "    \n",
    "    # Sauvegarder les stats\n",
    "    stats_file = output_file.with_name(output_file.stem + \"_stats.txt\")\n",
    "    with open(stats_file, 'w') as f:\n",
    "        f.write(f\"Total proteins: {stats['total']}\\n\")\n",
    "        f.write(f\"Mapped proteins: {stats['mapped']} ({stats['mapped']/stats['total']:.1%})\\n\")\n",
    "        f.write(f\"Unmapped proteins: {stats['unmapped']} ({stats['unmapped']/stats['total']:.1%})\\n\")\n",
    "    \n",
    "    # Sauvegarder les non-mappés si nécessaire\n",
    "    if stats['unmapped'] > 0:\n",
    "        unmapped_file = output_file.with_name(output_file.stem + \"_unmapped.txt\")\n",
    "        unmapped_proteins = result_df[result_df['Mapping_Status'] == 'Failed']['Original_Protein_ID']\n",
    "        unmapped_proteins.to_csv(unmapped_file, index=False, header=False)\n",
    "        print(f\"{stats['unmapped']} protéines non mappées sauvegardées dans {unmapped_file}\")\n",
    "    \n",
    "    print(\"Traitement terminé avec succès!\")\n",
    "    return stats\n",
    "\n",
    "# ==============================================\n",
    "# CONFIGURATION PRINCIPALE\n",
    "# ==============================================\n",
    "\n",
    "# Chemins absolus des fichiers\n",
    "BASE_DIR = Path(r\"C:\\Users\\PC\\Documents\\M2 HPC\\PFE\\PFE_CODE\\Data\")\n",
    "RAW_DATA_DIR = BASE_DIR / \"raw data\" / \"autres\"\n",
    "CLEAN_DATA_DIR = BASE_DIR / \"clean data\"\n",
    "\n",
    "human_mapping_path = RAW_DATA_DIR / \"HUMAN_9606_idmapping.dat\"\n",
    "yeast_mapping_path = RAW_DATA_DIR / \"YEAST_559292_idmapping.dat\"\n",
    "human_compartment_path = RAW_DATA_DIR / \"human_compartment_integrated_full.tsv\"\n",
    "yeast_compartment_path = RAW_DATA_DIR / \"yeast_compartment_integrated_full.tsv\"\n",
    "output_dir = CLEAN_DATA_DIR / \"autres\"\n",
    "\n",
    "# Vérification des fichiers\n",
    "print(\"\\nVérification des fichiers...\")\n",
    "required_files = {\n",
    "    \"Human mapping\": human_mapping_path,\n",
    "    \"Yeast mapping\": yeast_mapping_path,\n",
    "    \"Human compartment\": human_compartment_path,\n",
    "    \"Yeast compartment\": yeast_compartment_path\n",
    "}\n",
    "\n",
    "for name, path in required_files.items():\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Fichier {name} introuvable à l'emplacement: {path}\")\n",
    "    print(f\"- {name}: {path} (OK)\")\n",
    "\n",
    "# Créer le dossier de sortie\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "print(f\"\\nDossier de sortie: {output_dir}\")\n",
    "\n",
    "# ==============================================\n",
    "# EXÉCUTION PRINCIPALE\n",
    "# ==============================================\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # 1. Charger les fichiers de mapping\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"CHARGEMENT DES FICHIERS DE MAPPING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    human_mapping, human_conversion = load_mapping_file(human_mapping_path)\n",
    "    yeast_mapping, yeast_conversion = load_mapping_file(yeast_mapping_path)\n",
    "    \n",
    "    print(f\"\\nTemps de chargement: {time.time() - start_time:.2f} secondes\")\n",
    "    print(f\"Protéines humaines chargées: {len(human_mapping)}\")\n",
    "    print(f\"Protéines yeast chargées: {len(yeast_mapping)}\")\n",
    "    \n",
    "    # 2. Traitement des données humaines\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAITEMENT DES DONNÉES HUMAINES\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    human_stats = map_compartment_data(\n",
    "        human_compartment_path,\n",
    "        human_mapping,\n",
    "        human_conversion,\n",
    "        output_dir / \"human_compartment_mapped.txt\",\n",
    "        'human'\n",
    "    )\n",
    "    \n",
    "    # 3. Traitement des données yeast\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TRAITEMENT DES DONNÉES YEAST\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    yeast_stats = map_compartment_data(\n",
    "        yeast_compartment_path,\n",
    "        yeast_mapping,\n",
    "        yeast_conversion,\n",
    "        output_dir / \"yeast_compartment_mapped.txt\",\n",
    "        'yeast'\n",
    "    )\n",
    "    \n",
    "    # 4. Affichage des résultats finaux\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"RÉSULTATS FINAUX\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"HUMAIN - Total: {human_stats['total']}, Mappés: {human_stats['mapped']} ({human_stats['mapped']/human_stats['total']:.1%})\")\n",
    "    print(f\"YEAST  - Total: {yeast_stats['total']}, Mappés: {yeast_stats['mapped']} ({yeast_stats['mapped']/yeast_stats['total']:.1%})\")\n",
    "    print(f\"\\nTemps total d'exécution: {time.time() - start_time:.2f} secondes\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERREUR: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Traitement des données levure ===\n",
      "Chargement du fichier de mapping: C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data\\raw data/autres/YEAST_559292_idmapping.dat\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 227\u001b[0m\n\u001b[0;32m    224\u001b[0m     process_dataset(base_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 227\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 224\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    221\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean data/autres\u001b[39m\u001b[38;5;124m\"\u001b[39m), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    223\u001b[0m \u001b[38;5;66;03m# Traitement des deux espèces\u001b[39;00m\n\u001b[1;32m--> 224\u001b[0m \u001b[43mprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlevure\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[16], line 185\u001b[0m, in \u001b[0;36mprocess_dataset\u001b[1;34m(base_dir, species)\u001b[0m\n\u001b[0;32m    171\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m: {\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmapping_file\u001b[39m\u001b[38;5;124m'\u001b[39m: os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw data/autres/HUMAN_9606_idmapping.dat\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m     }\n\u001b[0;32m    182\u001b[0m }[species]\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# Chargement des mappings\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m mappings \u001b[38;5;241m=\u001b[39m \u001b[43mload_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmapping_file\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mappings:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 55\u001b[0m, in \u001b[0;36mload_mapping\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m     45\u001b[0m mappings \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniprot_to_gene\u001b[39m\u001b[38;5;124m'\u001b[39m: defaultdict(\u001b[38;5;28mlist\u001b[39m),\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgene_to_uniprot\u001b[39m\u001b[38;5;124m'\u001b[39m: defaultdict(\u001b[38;5;28mlist\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstring_to_uniprot\u001b[39m\u001b[38;5;124m'\u001b[39m: defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m     52\u001b[0m }\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Remplissage des mappings\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43muniprot_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muniprot_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdb_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGene_OrderedLocusName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:1554\u001b[0m, in \u001b[0;36mDataFrame.iterrows\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1552\u001b[0m using_cow \u001b[38;5;241m=\u001b[39m using_copy_on_write()\n\u001b[0;32m   1553\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[1;32m-> 1554\u001b[0m     s \u001b[38;5;241m=\u001b[39m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1555\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m using_cow \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mis_single_block:\n\u001b[0;32m   1556\u001b[0m         s\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39madd_references(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\construction.py:606\u001b[0m, in \u001b[0;36msanitize_array\u001b[1;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[0;32m    604\u001b[0m subarr \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m--> 606\u001b[0m     subarr \u001b[38;5;241m=\u001b[39m \u001b[43mmaybe_infer_to_datetimelike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    608\u001b[0m         object_index\n\u001b[0;32m    609\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m using_pyarrow_string_dtype()\n\u001b[0;32m    610\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m is_string_dtype(subarr)\n\u001b[0;32m    611\u001b[0m     ):\n\u001b[0;32m    612\u001b[0m         \u001b[38;5;66;03m# Avoid inference when string option is set\u001b[39;00m\n\u001b[0;32m    613\u001b[0m         subarr \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[1;32mc:\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\cast.py:1189\u001b[0m, in \u001b[0;36mmaybe_infer_to_datetimelike\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# error: Incompatible return value type (got \"Union[ExtensionArray,\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# ndarray[Any, Any]]\", expected \"Union[ndarray[Any, Any], DatetimeArray,\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;66;03m# TimedeltaArray, PeriodArray, IntervalArray]\")\u001b[39;00m\n\u001b[1;32m-> 1189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaybe_convert_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[0;32m   1190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Here we do not convert numeric dtypes, as if we wanted that,\u001b[39;49;00m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#  numpy would have done it for us.\u001b[39;49;00m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconvert_non_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_if_all_nat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mM8[ns]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mlib.pyx:2543\u001b[0m, in \u001b[0;36mpandas._libs.lib.maybe_convert_objects\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\numeric.py:362\u001b[0m, in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order, device, like)\u001b[0m\n\u001b[0;32m    360\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    361\u001b[0m a \u001b[38;5;241m=\u001b[39m empty(shape, dtype, order, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m--> 362\u001b[0m \u001b[43mmultiarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyto\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43munsafe\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[1;32mc:\\Python\\Python312\\Lib\\site-packages\\numpy\\_core\\multiarray.py:1089\u001b[0m, in \u001b[0;36mcopyto\u001b[1;34m(dst, src, casting, where)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03m    unravel_index(indices, shape, order='C')\u001b[39;00m\n\u001b[0;32m   1043\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \n\u001b[0;32m   1085\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (indices,)\n\u001b[1;32m-> 1089\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mcopyto)\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopyto\u001b[39m(dst, src, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;124;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \n\u001b[0;32m   1136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.stats import pearsonr\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import numba\n",
    "\n",
    "# Optimisation Numba pour le calcul PCC\n",
    "@numba.jit(nopython=True)\n",
    "def calculate_pcc_numba(v, u):\n",
    "    \"\"\"Version compilée avec Numba du calcul PCC.\"\"\"\n",
    "    n = len(v)\n",
    "    if n != len(u) or n < 2:\n",
    "        return np.nan\n",
    "    \n",
    "    sum_v = np.sum(v)\n",
    "    sum_u = np.sum(u)\n",
    "    sum_v_sq = np.sum(v**2)\n",
    "    sum_u_sq = np.sum(u**2)\n",
    "    sum_vu = np.sum(v * u)\n",
    "    \n",
    "    numerator = sum_vu - (sum_v * sum_u) / n\n",
    "    denominator = np.sqrt((sum_v_sq - sum_v**2 / n) * (sum_u_sq - sum_u**2 / n))\n",
    "    \n",
    "    if denominator == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    pcc = numerator / denominator\n",
    "    return (pcc + 1) / 2  # Normalisation [0,1]\n",
    "\n",
    "def load_mapping(file_path):\n",
    "    \"\"\"Charge les mappings d'identifiants de manière optimisée.\"\"\"\n",
    "    print(f\"Chargement du fichier de mapping: {file_path}\")\n",
    "    try:\n",
    "        # Chargement avec pandas en spécifiant les types\n",
    "        dtype = {'uniprot_id': str, 'db': str, 'db_id': str}\n",
    "        df = pd.read_csv(file_path, sep='\\t', header=None, \n",
    "                        names=['uniprot_id', 'db', 'db_id'], \n",
    "                        dtype=dtype, usecols=[0,1,2])\n",
    "        \n",
    "        # Création des dictionnaires de mapping\n",
    "        mappings = {\n",
    "            'uniprot_to_gene': defaultdict(list),\n",
    "            'gene_to_uniprot': defaultdict(list),\n",
    "            'name_to_uniprot': defaultdict(list),\n",
    "            'orf_to_uniprot': defaultdict(list),\n",
    "            'ensembl_to_uniprot': defaultdict(list),\n",
    "            'string_to_uniprot': defaultdict(list)\n",
    "        }\n",
    "        \n",
    "        # Remplissage des mappings\n",
    "        for _, row in df.iterrows():\n",
    "            uniprot_id, db, db_id = row['uniprot_id'], row['db'], row['db_id']\n",
    "            \n",
    "            if db == 'Gene_OrderedLocusName':\n",
    "                mappings['uniprot_to_gene'][uniprot_id].append(db_id)\n",
    "                mappings['gene_to_uniprot'][db_id].append(uniprot_id)\n",
    "            elif db == 'Gene_Name':\n",
    "                mappings['name_to_uniprot'][db_id.upper()].append(uniprot_id)\n",
    "            elif db == 'ORF':\n",
    "                mappings['orf_to_uniprot'][db_id].append(uniprot_id)\n",
    "            elif db == 'Ensembl':\n",
    "                mappings['ensembl_to_uniprot'][db_id].append(uniprot_id)\n",
    "            elif db == 'STRING':\n",
    "                mappings['string_to_uniprot'][db_id].append(uniprot_id)\n",
    "        \n",
    "        # Simplification des mappings (garder le premier élément)\n",
    "        return {k: {k2: v2[0] for k2, v2 in v.items() if v2} \n",
    "               for k, v in mappings.items()}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement du mapping: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def load_expression_data(file_path, mappings, species):\n",
    "    \"\"\"Charge les données d'expression de manière optimisée.\"\"\"\n",
    "    print(f\"\\nChargement des données d'expression: {file_path}\")\n",
    "    \n",
    "    try:\n",
    "        # Trouver le début des données\n",
    "        skip_rows = 0\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                if line.startswith('!dataset_table_begin'):\n",
    "                    break\n",
    "                skip_rows += 1\n",
    "        \n",
    "        # Chargement avec pandas en spécifiant les types\n",
    "        dtype = {col: np.float32 for col in range(100)}  # Supposition: 100 colonnes max\n",
    "        df = pd.read_csv(file_path, sep='\\t', skiprows=skip_rows + 1, \n",
    "                        dtype=dtype, low_memory=False)\n",
    "        \n",
    "        # Filtrer les colonnes d'expression (GSM)\n",
    "        expr_cols = [col for col in df.columns if col.startswith('GSM')]\n",
    "        df = df[['IDENTIFIER'] + expr_cols].dropna()\n",
    "        \n",
    "        # Conversion numérique\n",
    "        df[expr_cols] = df[expr_cols].apply(pd.to_numeric, errors='coerce')\n",
    "        df = df.dropna(subset=expr_cols, how='all')\n",
    "        \n",
    "        # Mapping des identifiants\n",
    "        def map_identifier(x, species):\n",
    "            x = str(x).strip().upper()\n",
    "            if species == 'human':\n",
    "                return mappings['name_to_uniprot'].get(x, None)\n",
    "            else:  # levure\n",
    "                return (mappings['gene_to_uniprot'].get(x, None) or \n",
    "                       mappings['orf_to_uniprot'].get(x, None) or\n",
    "                       mappings['name_to_uniprot'].get(x, None))\n",
    "        \n",
    "        df['uniprot_id'] = df['IDENTIFIER'].apply(lambda x: map_identifier(x, species))\n",
    "        df = df.dropna(subset=['uniprot_id']).drop_duplicates('uniprot_id')\n",
    "        \n",
    "        # Préparation finale\n",
    "        df = df.set_index('uniprot_id')[expr_cols]\n",
    "        print(f\"Données d'expression chargées: {df.shape[0]} protéines\")\n",
    "        return df.astype(np.float32)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données d'expression: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_coexpression_parallel(ppi, expr_data, output_file):\n",
    "    \"\"\"Calcule la co-expression en parallèle.\"\"\"\n",
    "    print(\"\\nDébut du calcul de co-expression...\")\n",
    "    \n",
    "    # Filtrage des protéines communes\n",
    "    common_proteins = set(ppi['protein1']).union(set(ppi['protein2'])).intersection(set(expr_data.index))\n",
    "    ppi = ppi[ppi['protein1'].isin(common_proteins) & ppi['protein2'].isin(common_proteins)]\n",
    "    \n",
    "    if len(ppi) == 0:\n",
    "        print(\"Aucune paire valide après filtrage.\")\n",
    "        return\n",
    "    \n",
    "    # Préparation des données pour le calcul parallèle\n",
    "    expr_dict = {prot: expr_data.loc[prot].values for prot in common_proteins}\n",
    "    ppi_pairs = ppi[['protein1', 'protein2']].values\n",
    "    \n",
    "    # Calcul parallèle avec joblib\n",
    "    def process_pair(pair):\n",
    "        p1, p2 = pair\n",
    "        v = expr_dict.get(p1, None)\n",
    "        u = expr_dict.get(p2, None)\n",
    "        if v is not None and u is not None:\n",
    "            pcc = calculate_pcc_numba(v, u)\n",
    "            if not np.isnan(pcc):\n",
    "                return (p1, p2, pcc)\n",
    "        return None\n",
    "    \n",
    "    results = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(process_pair)(pair) for pair in ppi_pairs\n",
    "    )\n",
    "    \n",
    "    # Sauvegarde des résultats\n",
    "    valid_results = [r for r in results if r is not None]\n",
    "    if valid_results:\n",
    "        result_df = pd.DataFrame(valid_results, columns=['protein1', 'protein2', 'PCC'])\n",
    "        result_df.to_csv(output_file, sep='\\t', index=False)\n",
    "        print(f\"Résultats sauvegardés dans {output_file} ({len(result_df)} paires)\")\n",
    "    else:\n",
    "        print(\"Aucun résultat valide à sauvegarder.\")\n",
    "\n",
    "def process_dataset(base_dir, species):\n",
    "    \"\"\"Traite un ensemble de données complet.\"\"\"\n",
    "    print(f\"\\n=== Traitement des données {species} ===\")\n",
    "    \n",
    "    # Configuration des chemins\n",
    "    config = {\n",
    "        'human': {\n",
    "            'mapping_file': os.path.join(base_dir, \"raw data/autres/HUMAN_9606_idmapping.dat\"),\n",
    "            'expr_file': os.path.join(base_dir, \"raw data/autres/human_co-expression.soft\"),\n",
    "            'ppi_pattern': os.path.join(base_dir, \"clean data/interactions/*humain*\")\n",
    "        },\n",
    "        'levure': {\n",
    "            'mapping_file': os.path.join(base_dir, \"raw data/autres/YEAST_559292_idmapping.dat\"),\n",
    "            'expr_file': os.path.join(base_dir, \"raw data/autres/levure_co-expression.soft\"),\n",
    "            'ppi_pattern': os.path.join(base_dir, \"clean data/interactions/*levure*\")\n",
    "        }\n",
    "    }[species]\n",
    "    \n",
    "    # Chargement des mappings\n",
    "    mappings = load_mapping(config['mapping_file'])\n",
    "    if not mappings:\n",
    "        return\n",
    "    \n",
    "    # Chargement des données d'expression\n",
    "    expr_data = load_expression_data(config['expr_file'], mappings, species)\n",
    "    if expr_data.empty:\n",
    "        return\n",
    "    \n",
    "    # Traitement des fichiers PPI\n",
    "    ppi_files = glob.glob(config['ppi_pattern'])\n",
    "    for ppi_file in ppi_files:\n",
    "        print(f\"\\nTraitement de {os.path.basename(ppi_file)}\")\n",
    "        try:\n",
    "            ppi = pd.read_csv(ppi_file, sep='\\t', header=None, \n",
    "                             names=['protein1', 'protein2'], dtype=str)\n",
    "            \n",
    "            # Mapping des identifiants\n",
    "            ppi['protein1'] = ppi['protein1'].map(mappings['uniprot_to_uniprot'].get)\n",
    "            ppi['protein2'] = ppi['protein2'].map(mappings['uniprot_to_uniprot'].get)\n",
    "            ppi = ppi.dropna()\n",
    "            \n",
    "            if ppi.empty:\n",
    "                print(\"Aucune interaction valide après mapping.\")\n",
    "                continue\n",
    "            \n",
    "            # Calcul de co-expression\n",
    "            output_name = os.path.splitext(os.path.basename(ppi_file))[0]\n",
    "            output_file = os.path.join(base_dir, f\"clean data/autres/coexpression_{output_name}1.txt\")\n",
    "            \n",
    "            calculate_coexpression_parallel(ppi, expr_data, output_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    base_dir = \"C:/Users/PC/Documents/M2 HPC/PFE/PFE_CODE/Data\"\n",
    "    os.makedirs(os.path.join(base_dir, \"clean data/autres\"), exist_ok=True)\n",
    "    \n",
    "    # Traitement des deux espèces\n",
    "    process_dataset(base_dir, 'levure')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
